{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch2trt import torch2trt\n",
    "from onnx2trt import get_engine, allocate_buffers, do_inference\n",
    "\n",
    "from layers import disp_to_depth\n",
    "from utils import readlines\n",
    "import datasets\n",
    "import networks\n",
    "import time\n",
    "from thop import profile, clever_format\n",
    "\n",
    "import PIL.Image as pil\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, name=None, encoder=None, decoder=None, encoder_pth=None, decoder_pth=None, model=None):\n",
    "        self.name = name\n",
    "        self.height = encoder_pth[\"height\"] if encoder_pth is not None else None\n",
    "        self.width = encoder_pth[\"width\"] if encoder_pth is not None else None\n",
    "        self.encoder_pth = encoder_pth\n",
    "        self.decoder_pth = decoder_pth\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.model = model\n",
    "        \n",
    "    def set_net(self, name, encoder=None, decoder=None, encoder_pth=None, decoder_pth=None, model=None):\n",
    "        self.name = name\n",
    "        self.height = encoder_pth[\"height\"] if encoder_pth is not None else None\n",
    "        self.width = encoder_pth[\"width\"] if encoder_pth is not None else None\n",
    "        self.encoder_pth = encoder_pth\n",
    "        self.decoder_pth = decoder_pth\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.model = model\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        if self.encoder == None:\n",
    "            logging.warning('Must set encoder first')\n",
    "        return self.encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        if self.decoder == None:\n",
    "            logging.warning('Must set decoder first')\n",
    "        return self.decoder\n",
    "    \n",
    "    def get_height(self):\n",
    "        if self.height == None:\n",
    "            logging.warning('Must set height first')\n",
    "        return self.height\n",
    "    \n",
    "    def get_width(self):\n",
    "        if self.width == None:\n",
    "            logging.warning('Must set width first')\n",
    "        return self.width\n",
    "    \n",
    "    def get_name(self):\n",
    "        if self.name == None:\n",
    "            logging.warning('Must set name first')\n",
    "        return self.name\n",
    "    \n",
    "    def get_model(self):\n",
    "        if self.model == None:\n",
    "            logging.warning('Must set model first')\n",
    "        return Depth(self.encoder, self.decoder)\n",
    "    \n",
    "    def eval(self):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "    \n",
    "    def to_device(self, no_cuda=False):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        self.encoder.to(self.device)\n",
    "        self.decoder.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.encoder_dict = {}\n",
    "        self.decoder_dict = {}\n",
    "        self.is_set_Net = False\n",
    "        self.splits_dir = os.path.join(os.path.expanduser(\"~\"), \"depth\", \"monodepth2\", \"splits\")\n",
    "        self.data_path = \"/work/garin0115/datasets/kitti_data/\"\n",
    "        # Models which were trained with stereo supervision were trained with a nominal\n",
    "        # baseline of 0.1 units. The KITTI rig has a baseline of 54cm. Therefore,\n",
    "        # to convert our stereo predictions to real-world scale we multiply our depths by 5.4.\n",
    "        self.STEREO_SCALE_FACTOR = 5.4\n",
    "        self.eval_split = \"eigen\"\n",
    "        self.MIN_DEPTH = 1e-3\n",
    "        self.MAX_DEPTH = 80\n",
    "        self.disable_median_scaling = False\n",
    "        self.pred_depth_scale_factor = 1\n",
    "        self.CMAP = 'plasma'\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "        \n",
    "        self.no_cuda = False\n",
    "        self.ext = \"jpg\"\n",
    "        self.split_folder = os.path.join(os.path.expanduser(\"~\"), \"depth\", \"monodepth2\", \"splits\", self.eval_split)\n",
    "        if torch.cuda.is_available() and not self.no_cuda:\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "    \n",
    "    def get_model_dict(self, encoder_dict, decoder_dict):\n",
    "        self.encoder_dict = encoder_dict\n",
    "        self.decoder_dict = decoder_dict\n",
    "    \n",
    "\n",
    "            \n",
    "    \n",
    "    def load_model(self, name, is_torch2trt=False):\n",
    "        assert self.encoder_dict, \"Must load encoder dict first\"\n",
    "        assert self.decoder_dict, \"Must load decoder dict first\"\n",
    "        \n",
    "        is_finetune = True if name.split(\"_\")[-1] == \"finetune\" else False\n",
    "        if is_finetune:\n",
    "            load_weights_folder = self.get_modelPath(name[:-9], is_finetune)\n",
    "        else:\n",
    "            load_weights_folder = self.get_modelPath(name)\n",
    "        \n",
    "        encoder_path = os.path.join(load_weights_folder, \"encoder.pth\")\n",
    "        decoder_path = os.path.join(load_weights_folder, \"depth.pth\")\n",
    "        encoder_pth = torch.load(encoder_path)\n",
    "        decoder_pth = torch.load(decoder_path)\n",
    "        \n",
    "        encoder = self.encoder_dict[name.split(\"_\")[0]]\n",
    "        decoder = self.decoder_dict[name]\n",
    "        encoder.load_state_dict({k: v for k, v in encoder_pth.items() if k in encoder.state_dict()})\n",
    "        decoder.load_state_dict(decoder_pth)\n",
    "        if is_torch2trt == True:\n",
    "            x = torch.ones((1, 3, 256, 832)).to(self.device)\n",
    "            encoder = torch2trt(encoder.to(self.device), [x]).cpu()\n",
    "        \n",
    "        return encoder, decoder, encoder_pth, decoder_pth\n",
    "    \n",
    "        \n",
    "    \n",
    "    def get_modelPath(self, name, is_finetune=False):\n",
    "        load_weights_folder = os.path.join(\"/work\", \"garin0115\", \"models\", name+\"_256x832\", \"models\")\n",
    "        if not os.path.isdir(load_weights_folder):\n",
    "            load_weights_folder = os.path.join(os.path.expanduser(\"~\"), \n",
    "                                               \"depth\", \n",
    "                                               \"monodepth2\",\n",
    "                                               \"models\", \n",
    "                                               name+\"_256x832\", \n",
    "                                               \"models\")\n",
    "        \n",
    "        assert os.path.isdir(load_weights_folder), \"Cannot find a folder at {}\".format(load_weights_folder)\n",
    "\n",
    "        print(\"[info] Loading weights from {}\".format(load_weights_folder))\n",
    "            \n",
    "        if is_finetune:\n",
    "            load_weights_folder = os.path.join(load_weights_folder, \"weights_29\")\n",
    "        elif name == \"resnet18_oneLayer\":\n",
    "            load_weights_folder = os.path.join(load_weights_folder, \"weights_18\")\n",
    "        else:\n",
    "            load_weights_folder = os.path.join(load_weights_folder, \"weights_19\")\n",
    "        \n",
    "        return load_weights_folder\n",
    "    \n",
    "    def get_dataLoader(self, height, width):\n",
    "        filenames  = readlines(os.path.join(self.splits_dir, self.eval_split, \"test_files.txt\"))\n",
    "        dataset    = datasets.KITTIRAWDataset(data_path =self.data_path, \n",
    "                                           filenames =filenames,\n",
    "                                           height    =height, \n",
    "                                           width     =width,\n",
    "                                           frame_idxs=[0], \n",
    "                                           num_scales=4, \n",
    "                                           is_train  =False)\n",
    "        dataLoader = DataLoader(dataset    =dataset,\n",
    "                                batch_size =16,\n",
    "                                shuffle    =False,\n",
    "                                num_workers=16,\n",
    "                                pin_memory =True,\n",
    "                                drop_last  =False)\n",
    "        return dataLoader\n",
    "    \n",
    "    def batch_evaluate_depth(self, save_CSV=False, is_torch2trt=False):\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "            \n",
    "        results = []\n",
    "        \n",
    "        for net in self.nets:\n",
    "            disps, time_min, time_avg = self.evaluate_depth(net)\n",
    "            result = self.calculate_metric(net.get_name(), disps, time_min, time_avg)\n",
    "            results.append(result)\n",
    "            \n",
    "            disps, time_min, time_avg = self.evaluate_onnx_depth(net.get_name(), fp16_mode=False)\n",
    "            result = self.calculate_metric(net.get_name()+\"_trt32\", disps, time_min, time_avg)\n",
    "            results.append(result)\n",
    "            \n",
    "            disps, time_min, time_avg = self.evaluate_onnx_depth(net.get_name(), fp16_mode=True)\n",
    "            result = self.calculate_metric(net.get_name()+\"_trt16\", disps, time_min, time_avg)\n",
    "            results.append(result)\n",
    "        \n",
    "        if save_CSV:\n",
    "            import csv\n",
    "            # 開啟輸出的 CSV 檔案\n",
    "            with open('result.csv', 'w', newline='') as csvfile:\n",
    "                # 建立 CSV 檔寫入器\n",
    "                writer = csv.writer(csvfile)\n",
    "\n",
    "                # 寫入一列資料\n",
    "#                 writer.writerow(['Model', 'Height', 'Width', \"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\", \n",
    "#                               'Best FPS', 'Avg FPS', 'Parameters', 'params_enc', 'params_dec', 'FLOPs', 'fl_enc', 'fl_dec'])\n",
    "                writer.writerow(['Model', 'Height', 'Width', \"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\", \n",
    "                              'Best FPS', 'Avg FPS'])\n",
    "\n",
    "                # 寫入另外幾列資料\n",
    "                for res in results:\n",
    "                    writer.writerow(res)\n",
    "            \n",
    "    def calculate_metric(self, name, pred_disps, time_min, time_avg):\n",
    "        gt_path = os.path.join(self.splits_dir, self.eval_split, \"gt_depths.npz\")\n",
    "        gt_depths = np.load(gt_path, fix_imports=True, encoding='latin1', allow_pickle=True)[\"data\"]\n",
    "        \n",
    "        errors = []\n",
    "        ratios = []\n",
    "\n",
    "        for i in range(pred_disps.shape[0]):\n",
    "\n",
    "            gt_depth = gt_depths[i]\n",
    "            gt_height, gt_width = gt_depth.shape[:2]\n",
    "\n",
    "            pred_disp = pred_disps[i]\n",
    "            pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n",
    "            pred_depth = 1 / pred_disp\n",
    "\n",
    "            if self.eval_split == \"eigen\":\n",
    "                mask = np.logical_and(gt_depth > self.MIN_DEPTH, gt_depth < self.MAX_DEPTH)\n",
    "\n",
    "                crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n",
    "                                 0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n",
    "                crop_mask = np.zeros(mask.shape)\n",
    "                crop_mask[crop[0]:crop[1], crop[2]:crop[3]] = 1\n",
    "                mask = np.logical_and(mask, crop_mask)\n",
    "\n",
    "            else:\n",
    "                mask = gt_depth > 0\n",
    "\n",
    "            pred_depth = pred_depth[mask]\n",
    "            gt_depth = gt_depth[mask]\n",
    "\n",
    "            pred_depth *= self.pred_depth_scale_factor\n",
    "            if not self.disable_median_scaling:\n",
    "                ratio = np.median(gt_depth) / np.median(pred_depth)\n",
    "                ratios.append(ratio)\n",
    "                pred_depth *= ratio\n",
    "\n",
    "            pred_depth[pred_depth < self.MIN_DEPTH] = self.MIN_DEPTH\n",
    "            pred_depth[pred_depth > self.MAX_DEPTH] = self.MAX_DEPTH\n",
    "\n",
    "            errors.append(self.compute_errors(gt_depth, pred_depth))\n",
    "\n",
    "        if not self.disable_median_scaling:\n",
    "            ratios = np.array(ratios)\n",
    "            med = np.median(ratios)\n",
    "            print(\" Scaling ratios | med: {:0.3f} | std: {:0.3f}\".format(med, np.std(ratios / med)))\n",
    "\n",
    "        mean_errors = np.array(errors).mean(0)\n",
    "        print(\"[info] {}\".format(name))\n",
    "        print(\" best FPS: \", 1/time_min)\n",
    "        print(\" avg FPS: \", 1/time_avg)\n",
    "        print(\"\\n  \" + (\"{:>8} | \" * 7).format(\"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\"))\n",
    "        print((\"&{: 8.3f}  \" * 7).format(*mean_errors.tolist()) + \"\\\\\\\\\")\n",
    "        print(\"\\n-> Done!\")\n",
    "\n",
    "        \n",
    "#         flops_enc, params_enc = profile(encoder, inputs=(input_color, ))\n",
    "#         flops_dec, params_dec = profile(decoder, inputs=(*tuple(features), ))\n",
    "#         a, b, c, d, e, f = clever_format([params_enc+params_dec, \n",
    "#                                           params_enc, \n",
    "#                                           params_dec, \n",
    "#                                           flops_enc+flops_dec, \n",
    "#                                           flops_enc, \n",
    "#                                           flops_dec], \"%.3f\")\n",
    "\n",
    "        result = []\n",
    "        result.append(name)\n",
    "        result.append(256)\n",
    "        result.append(832)\n",
    "        for i in mean_errors:\n",
    "            result.append(i)\n",
    "        result.append(1/time_min)\n",
    "        result.append(1/time_avg)\n",
    "#         for i in [a, b, c, d, e, f]:\n",
    "#             result.append(i)\n",
    "    \n",
    "        return result\n",
    "        \n",
    "\n",
    "    \n",
    "    def evaluate_depth(self, net):\n",
    "        dataLoader = self.get_dataLoader(net.get_height(), net.get_width())\n",
    "        print(\"[info] Model {}\".format(net.get_name()))\n",
    "        \n",
    "        model = net.get_model()\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        pred_disps = []\n",
    "\n",
    "        print(\"[info] Computing predictions with size {}x{}\".format(\n",
    "            net.get_width(), net.get_height()))\n",
    "        \n",
    "        time_min = float('inf')\n",
    "        time_avg = 0\n",
    "        avg_FPS = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(5): #跑十次算FPS\n",
    "                for data in dataLoader:\n",
    "                    input_color = data[(\"color\", 0, 0)].cuda()\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    output = model(input_color)\n",
    "                    total_time = time.time() - start_time\n",
    "                    pred_disp, _ = disp_to_depth(output[(\"disp\", 0)], self.MIN_DEPTH, self.MAX_DEPTH)\n",
    "                    pred_disp = pred_disp[:, 0].cpu().numpy()\n",
    "            #                 pred_disp = pred_disp[:, 0].numpy()\n",
    "\n",
    "                    if i == 0:\n",
    "                        pred_disps.append(pred_disp)\n",
    "                    time_avg += total_time\n",
    "                    if total_time < time_min:\n",
    "                        time_min = total_time\n",
    "\n",
    "                time_avg /= len(dataLoader)\n",
    "                avg_FPS += time_avg\n",
    "            time_avg = avg_FPS / 10\n",
    "            \n",
    "        pred_disps = np.concatenate(pred_disps)\n",
    "        \n",
    "        return pred_disps, time_min, time_avg\n",
    "    \n",
    "    def evaluate_onnx_depth(self, name, fp16_mode=True):\n",
    "        dataLoader = self.get_dataLoader(256, 832)\n",
    "        print(\"[info] Model {}\".format(name))\n",
    "        \n",
    "\n",
    "        pred_disps = []\n",
    "\n",
    "        print(\"[info] Computing predictions with size {}x{}\".format(\n",
    "            256, 832))\n",
    "        \n",
    "        \n",
    "        \n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\", \n",
    "                                  name+\"_256x832\", \n",
    "                                  \"models\", \n",
    "                                  \"weights_19\", \n",
    "                                  name+\".onnx\")\n",
    "        if fp16_mode:\n",
    "            engine_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\",\n",
    "                                      \"trt16_models\",\n",
    "                                      name+\".trt\")\n",
    "        else:\n",
    "            engine_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\",\n",
    "                                      \"trt_models\",\n",
    "                                      name+\".trt\")\n",
    "        #engine\n",
    "        engine = get_engine(fp16_mode=False, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=False)\n",
    "        # Create the context for this engine\n",
    "        context = engine.create_execution_context()\n",
    "        # Allocate buffers for input and output\n",
    "        inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "        time_min = float('inf')\n",
    "        time_avg = 0\n",
    "        avg_FPS = 0\n",
    "        \n",
    "        for data in dataLoader:\n",
    "            input_images = data[(\"color\", 0, 0)].numpy()\n",
    "            batch_pred_disp = []\n",
    "            total_time = 0\n",
    "            for input_image in input_images:\n",
    "                input_image = np.expand_dims(input_image, axis=0).reshape(-1)\n",
    "                inputs[0].host = input_image\n",
    "                start_time = time.time()\n",
    "                trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "                end_time = time.time() - start_time\n",
    "                total_time += end_time\n",
    "                pred_disp, _ = disp_to_depth(trt_outputs[-1], self.MIN_DEPTH, self.MAX_DEPTH)\n",
    "                pred_disp = pred_disp.reshape(1, 256, 832)\n",
    "                batch_pred_disp.append(pred_disp)\n",
    "                \n",
    "                if end_time < time_min:\n",
    "                    time_min = end_time\n",
    "            total_time /= len(input_images)\n",
    "            time_avg += total_time\n",
    "                \n",
    "            pred_disps.append(np.concatenate(batch_pred_disp, axis=0))\n",
    "            \n",
    "        time_avg /= len(dataLoader)\n",
    "       \n",
    "        \n",
    "        \n",
    "        pred_disps = np.concatenate(pred_disps)\n",
    "        \n",
    "        return pred_disps, time_min, time_avg\n",
    "        \n",
    "\n",
    "    \n",
    "    def set_Net(self, is_torch2trt=False):\n",
    "        self.nets = []\n",
    "        for name in self.decoder_dict:\n",
    "            print(\"[Info] Deal with {} model\\n\".format(name))\n",
    "            net = Net()\n",
    "            encoder, decoder, encoder_pth, decoder_pth = self.load_model(name)\n",
    "            net.set_net(name, \n",
    "                        copy.deepcopy(encoder), \n",
    "                        copy.deepcopy(decoder), \n",
    "                        copy.deepcopy(encoder_pth), \n",
    "                        copy.deepcopy(decoder_pth))\n",
    "            self.nets.append(net)\n",
    "            print()\n",
    "            if is_torch2trt:\n",
    "                print(\"[Info] Deal with {} to trt model\\n\".format(name))\n",
    "                inputs = torch.ones((1, 3, net.get_height(), net.get_width()))#.to(self.device)\n",
    "                model = net.get_model()\n",
    "                #model.to(self.device)\n",
    "                \n",
    "                print(\"### Converting model to trt\")\n",
    "                model_trt = torch2trt(encoder, [inputs])\n",
    "                print(\"### Convert complete\")\n",
    "                \n",
    "                #model_trt.cpu()\n",
    "                \n",
    "        \n",
    "                self.nets.append(Net(name+\"_trt\", \n",
    "                                 encoder_pth=copy.deepcopy(encoder_pth), \n",
    "                                 decoder_pth=copy.deepcopy(decoder_pth), \n",
    "                                 model=model_trt))\n",
    "                print()\n",
    "    \n",
    "        \n",
    "    \n",
    "    def inference_depth(self, column=2, is_torch2trt=False, is_onnx=False):\n",
    "        lines = readlines(os.path.join(self.split_folder, \"test_files.txt\"))\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net(is_torch2trt)\n",
    "        with torch.no_grad():\n",
    "            for i in np.random.choice(len(lines), 10, replace=False):\n",
    "                folder, frame_id, side = lines[i].split()\n",
    "                frame_id = int(frame_id)  \n",
    "                image_path = os.path.join(self.data_path, folder, \n",
    "                                          \"image_0{}\".format(self.side_map[side]), \n",
    "                                          \"data\", \n",
    "                                          \"{:010d}.jpg\".format(frame_id))\n",
    "                input_image = pil.open(image_path).convert('RGB')\n",
    "                original_width, original_height = input_image.size\n",
    "                \n",
    "                \n",
    "                result = OrderedDict()\n",
    "                result[\"Input\"] = input_image\n",
    "#                 result[\"Mask\"] = self.seg_img(input_image)\n",
    "                \n",
    "                for net in self.nets:\n",
    "                    net.to_device()\n",
    "                    net.eval()\n",
    "                    input_image = pil.open(image_path).convert('RGB')\n",
    "                    input_image_resized = input_image.resize((net.get_width(), net.get_height()), pil.LANCZOS)\n",
    "                    input_image_torch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "                    input_image_torch = input_image_torch.to(self.device)\n",
    "                    \n",
    "                    features = net.get_encoder()(input_image_torch)\n",
    "                    outputs = net.get_decoder()(*tuple(features))\n",
    "                    \n",
    "                    disp = outputs[(\"disp\", 0)]\n",
    "                    disp_resized = torch.nn.functional.interpolate(\n",
    "                            disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "                    disp_resized_np = disp_resized.squeeze().cpu().detach().numpy()\n",
    "                    result[\"{}\".format(net.get_name())] = disp_resized_np\n",
    "                    \n",
    "                    if is_onnx:\n",
    "                        name = net.get_name()\n",
    "                        onnx_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\", \n",
    "                                      name+\"_256x832\", \n",
    "                                      \"models\", \n",
    "                                      \"weights_19\", \n",
    "                                      name+\".onnx\")\n",
    "                        engine16_path = os.path.join(\"/work\", \n",
    "                                                  \"garin0115\", \n",
    "                                                  \"models\",\n",
    "                                                  \"trt16_models\",\n",
    "                                                  name+\".trt\")\n",
    "\n",
    "                        engine_path = os.path.join(\"/work\", \n",
    "                                                  \"garin0115\", \n",
    "                                                  \"models\",\n",
    "                                                  \"trt_models\",\n",
    "                                                  name+\".trt\")\n",
    "                        \n",
    "                        input_image = input_image.resize((832, 256), pil.LANCZOS)\n",
    "                        input_image = np.array(input_image).transpose((2, 0, 1)).astype(np.float32) / 255.\n",
    "                        print(input_image.shape)\n",
    "                        input_image = np.expand_dims(input_image, axis=0).reshape(-1)\n",
    "\n",
    "\n",
    "                        #engine16\n",
    "                        engine16 = get_engine(fp16_mode=True, onnx_file_path=onnx_path, engine_file_path=engine16_path, save_engine=False)\n",
    "                        # Create the context for this engine\n",
    "                        context16 = engine16.create_execution_context()\n",
    "                        # Allocate buffers for input and output\n",
    "                        inputs16, outputs16, bindings16, stream16 = allocate_buffers(engine16)\n",
    "                        inputs16[0].host = input_image\n",
    "                        trt_outputs16 = do_inference(context16, bindings=bindings16, inputs=inputs16, outputs=outputs16, stream=stream16) # numpy data\n",
    "\n",
    "                        #engine\n",
    "                        engine = get_engine(fp16_mode=False, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=False)\n",
    "                        # Create the context for this engine\n",
    "                        context = engine.create_execution_context()\n",
    "                        # Allocate buffers for input and output\n",
    "                        inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "                        inputs[0].host = input_image\n",
    "                        trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) # numpy data\n",
    "\n",
    "                        result[\"{}\".format(net.get_name()+\"_trt32\")] = trt_outputs[-1].reshape((net.get_height(), net.get_width()))\n",
    "                        result[\"{}\".format(net.get_name()+\"_trt16\")] = trt_outputs16[-1].reshape((net.get_height(), net.get_width()))\n",
    "                \n",
    "                self.quick_show(result, column=column)\n",
    "    \n",
    "    def inference_segment_sky(self):\n",
    "        result = OrderedDict()\n",
    "        lines = readlines(os.path.join(self.split_folder, \"test_files.txt\"))\n",
    "        for i in np.random.choice(len(lines), 10, replace=False):\n",
    "            folder, frame_id, side = lines[i].split()\n",
    "            frame_id = int(frame_id)  \n",
    "            image_path = os.path.join(self.data_path, folder, \n",
    "                                      \"image_0{}\".format(self.side_map[side]), \n",
    "                                      \"data\", \n",
    "                                      \"{:010d}.jpg\".format(frame_id))\n",
    "            input_image = pil.open(image_path).convert('RGB')\n",
    "            \n",
    "            result[\"Input_{}\".format(i)] = input_image\n",
    "            result[\"Mask_{}\".format(i)] = self.seg_img(input_image)\n",
    "            \n",
    "        self.quick_show(result, column=4)\n",
    "            \n",
    "    \n",
    "    def seg_img(self, image):\n",
    "        image = cv2.cvtColor(np.asarray(image),cv2.COLOR_RGB2BGR) \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "        thresh_dilation = cv2.dilate(thresh, kernel, anchor=(-1,-1), iterations=8)\n",
    "\n",
    "        edges = cv2.Canny(gray, 1, 100)\n",
    "        edges_dilation = cv2.dilate(edges, kernel, anchor=(-1,-1), iterations=8)\n",
    "\n",
    "        mask = thresh_dilation | edges_dilation\n",
    "        mask_dilation = cv2.dilate(mask, kernel, anchor=(-1,-1), iterations=8)\n",
    "        segImg = 255 - mask_dilation \n",
    "        segImg[image.shape[0]//3:, :] = 0\n",
    "            \n",
    "        return segImg\n",
    "                \n",
    "    def evaluate_pose(self):\n",
    "        pass            \n",
    "    \n",
    "    def inference_pose(self):\n",
    "        pass\n",
    "    \n",
    "    def quick_show(self, result, column=2):\n",
    "        row = len(result) // column\n",
    "        if len(result) % column > 0:\n",
    "            row += 1\n",
    "        plt.figure(figsize=(column*3*3, row*1*3+1))\n",
    "        for idx, key in enumerate(result):\n",
    "            \n",
    "            if key.split(\"_\")[0] == \"Input\":\n",
    "                plt.subplot(row, column, idx+1)\n",
    "                plt.imshow(result[key])\n",
    "                plt.title(key, fontsize=22)\n",
    "                continue\n",
    "                \n",
    "            plt.subplot(row, column, idx+1)\n",
    "            if key.split(\"_\")[0] == \"Mask\":\n",
    "                plt.imshow(result[key], cmap=\"gray\")\n",
    "            else:\n",
    "                vmax = np.percentile(result[key], 95)\n",
    "                plt.imshow(result[key], cmap=self.CMAP, vmax=vmax)\n",
    "                \n",
    "            if key == \"resnet18_simplify2my3\":\n",
    "                plt.title(key, fontsize=22, color=\"red\")\n",
    "            elif key == \"resnet18_skip2Conv\":\n",
    "                plt.title(key, fontsize=22, color=\"blue\")\n",
    "            else:\n",
    "                plt.title(key, fontsize=22)\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0.5, w_pad=0.1, h_pad=0.1)\n",
    "        \n",
    "    def make_grid(self, result, column=2):\n",
    "        pass\n",
    "    \n",
    "    def make_vedio(self, file_name, video_output_folder, column=2):\n",
    "        # 取得資料夾中所有影像檔案路徑\n",
    "        kitti_depth_folder = '/work/garin0115/datasets/kitti_data/'+file_name+'/image_02'\n",
    "        filenames = glob.glob(kitti_depth_folder+'/*/*.jpg')\n",
    "\n",
    "        # 將檔案路徑排序\n",
    "        filenames.sort()\n",
    "        num_images = len(filenames)\n",
    "        print(\"Total images: {}\".format(num_images))\n",
    "        \n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "    \n",
    "        num_model = len(self.nets)\n",
    "        num_column = column\n",
    "        num_row = num_model // column if num_model % column == 0 else num_model // column + 1\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(video_output_folder+'/disp_{}.avi'.format(file_name.split('/')[-1]), \n",
    "                              fourcc, \n",
    "                              15.0, \n",
    "                              (original_width*num_column, original_height*num_row))\n",
    "        \n",
    "        \n",
    "        for idx in range(num_images):\n",
    "            res = []\n",
    "            input_image = pil.open(img).convert('RGB')\n",
    "            input_image = np.array(input_image)\n",
    "            cv2.putText(input_image, \"Input\", (10, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            res.append(input_image[:, :, ::-1])\n",
    "\n",
    "            for net in self.nets:\n",
    "                name = net.get_name()\n",
    "                disp = show[name][idx]\n",
    "                vmax = np.percentile(disp, 95)\n",
    "                normalizer = mpl.colors.Normalize(vmin=disp.min(), vmax=vmax)\n",
    "                mapper = cm.ScalarMappable(norm=normalizer, cmap=CMAP)\n",
    "                colormapped_im = (mapper.to_rgba(disp)[:, :, :3] * 255).astype(np.uint8)\n",
    "                cv2.putText(colormapped_im, name, (10, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(colormapped_im, \"FPS \"+show[\"{}_FPS\".format(name)][idx], (10, 100), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                im = pil.fromarray(colormapped_im[:, :, ::-1])\n",
    "                res.append(im)\n",
    "            \n",
    "            \n",
    "            result = []\n",
    "            for i in range(num_row):\n",
    "                result.append(np.hstack(res[num_row * num_column: (num_row+1) * num_column]))\n",
    "            result = np.vstack(result)\n",
    "\n",
    "\n",
    "            out.write(result)\n",
    "        out.release()    \n",
    "        \n",
    "        \n",
    "    \n",
    "    def calc_param(self, net):\n",
    "        net_params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "        weight_count = 0\n",
    "        for param in net_params:\n",
    "            weight_count += np.prod(param.size())\n",
    "        return weight_count\n",
    "    \n",
    "    def compute_errors(self, gt, pred):\n",
    "        \"\"\"Computation of error metrics between predicted and ground truth depths\n",
    "        \"\"\"\n",
    "        thresh = np.maximum((gt / pred), (pred / gt))\n",
    "        a1 = (thresh < 1.25     ).mean()\n",
    "        a2 = (thresh < 1.25 ** 2).mean()\n",
    "        a3 = (thresh < 1.25 ** 3).mean()\n",
    "\n",
    "        rmse = (gt - pred) ** 2\n",
    "        rmse = np.sqrt(rmse.mean())\n",
    "\n",
    "        rmse_log = (np.log(gt) - np.log(pred)) ** 2\n",
    "        rmse_log = np.sqrt(rmse_log.mean())\n",
    "\n",
    "        abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "\n",
    "        sq_rel = np.mean(((gt - pred) ** 2) / gt)\n",
    "\n",
    "        return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n",
    "    \n",
    "    def batch_post_process_disparity(self, l_disp, r_disp):\n",
    "        \"\"\"Apply the disparity post-processing method as introduced in Monodepthv1\n",
    "        \"\"\"\n",
    "        _, h, w = l_disp.shape\n",
    "        m_disp = 0.5 * (l_disp + r_disp)\n",
    "        l, _ = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
    "        l_mask = (1.0 - np.clip(20 * (l - 0.05), 0, 1))[None, ...]\n",
    "        r_mask = l_mask[:, :, ::-1]\n",
    "        return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n",
    "    \n",
    "    #save to ONNX model\n",
    "    def save_ONNX(self):\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "            \n",
    "        for j in range(len(self.nets)):\n",
    "            self.nets[j].to_device()\n",
    "            self.nets[j].eval()\n",
    "            x = torch.randn(1, 3, self.nets[j].get_height(), self.nets[j].get_width(), requires_grad=True).to(self.device)\n",
    "            path = self.get_modelPath(self.nets[j].get_name())\n",
    "            encoder = self.nets[j].get_encoder()\n",
    "            decoder = self.nets[j].get_decoder()\n",
    "            depth_model = Depth(encoder, decoder, output_list=True)\n",
    "            \n",
    "            # Export the model\n",
    "            torch.onnx.export(depth_model,               # model being run\n",
    "                              x,                         # model input (or a tuple for multiple inputs)\n",
    "                              path+\"/\"+self.nets[j].get_name()+\".onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                              export_params=True,        # store the trained parameter weights inside the model file\n",
    "                              opset_version=10,          # the ONNX version to export the model to\n",
    "                              verbose=True,\n",
    "                              do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                              input_names = ['input'],   # the model's input names\n",
    "                              output_names = ['output']) # the model's output names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX2TRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_mode = True\n",
    "print(\"Model Name           FPS\")\n",
    "for name in decoder_dict:\n",
    "    if name == \"resnet18_oneLayer\":\n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                          \"garin0115\", \n",
    "                          \"models\", \n",
    "                          name+\"_256x832\", \n",
    "                          \"models\", \n",
    "                          \"weights_18\", \n",
    "                          name+\".onnx\")\n",
    "    else:\n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\", \n",
    "                                  name+\"_256x832\", \n",
    "                                  \"models\", \n",
    "                                  \"weights_19\", \n",
    "                                  name+\".onnx\")\n",
    "    if fp16_mode:\n",
    "        engine_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\",\n",
    "                                  \"trt16_models\",\n",
    "                                  name+\".trt\")\n",
    "    else:\n",
    "        engine_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\",\n",
    "                                  \"trt_models\",\n",
    "                                  name+\".trt\")\n",
    "\n",
    "    engine = get_engine(fp16_mode=fp16_mode, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=True)\n",
    "\n",
    "    # Create the context for this engine\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Allocate buffers for input and output\n",
    "    inputs, outputs, bindings, stream = allocate_buffers(engine) # input, output: host # bindings\n",
    "\n",
    "\n",
    "    # Load data to the buffer\n",
    "    image_path = \"assets/test_image.jpg\"\n",
    "    input_image = pil.open(image_path).convert('RGB').resize((832, 256), pil.LANCZOS)\n",
    "    input_image = np.array(input_image).transpose((2, 0, 1)).astype(np.float32) / 255.\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    inputs[0].host = input_image.reshape(-1)\n",
    "\n",
    "    # inputs[1].host = ... for multiple input\n",
    "    t1 = time.time()\n",
    "    for i in range(100):\n",
    "        trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) # numpy data\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print(\"{}       {}\".format(name, 100/(t2-t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dict = {\n",
    "    \"resnet18\":networks.ResnetEncoder(18, False)\n",
    "}\n",
    "decoder_dict = {\n",
    "#     \"resnet18_my3\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#     \"resnet18_my3_smooth\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#     \"resnet18_my3_concatDepth\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, concatDepth=True),\n",
    "    \n",
    "\n",
    "#     \"resnet18_my3_skipSky\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#     \"resnet18_my3_firstConv\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True),\n",
    "    \n",
    "#     \"resnet18_my3_firstConv_skipSky\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True),\n",
    "#     \"resnet18_my3_firstConv_skipSky_conv11\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True, conv11=True),\n",
    "#     \"resnet18_my3_skyLoss\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "    \n",
    "#     \"resnet18_my3_skipSky_skyLoss\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),  \n",
    "#     \"resnet18_my3_nomask\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=35),\n",
    "#     \"resnet18_my3_skipSky_finetune\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#     \"resnet18_my3_finetune\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "    \n",
    "    \"resnet18\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#     \"resnet18_skyLoss\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#     \"resnet18_skipSky_skyLoss\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "    \n",
    "#     \"resnet18_pw\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, pw=True),\n",
    "    \n",
    "#     \"resnet18_skipFirstConv\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, skipFirstConv=True),\n",
    "#     \"resnet18_skipFirstConv_skipSky\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, skipFirstConv=True),\n",
    "    \n",
    "    \"resnet18_skip2Conv\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, skip2Conv=True),\n",
    "#     \"resnet18_skip2Conv_skipSky\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, skip2Conv=True),\n",
    "#     \"resnet18_skip2Conv_skyLoss\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, skip2Conv=True),\n",
    "    \n",
    "#     \"resnet18_oneLayer\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, oneLayer=True),\n",
    "    \n",
    "    \"resnet18_simplify2my3\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, pw=True, oneLayer=True),\n",
    "#     \"resnet18_simplify2my3_skyLoss\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, pw=True, oneLayer=True),\n",
    "#     \"resnet18_simplify2my3_skipSky\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, pw=True, oneLayer=True),\n",
    "#     \"resnet18_simplify2my3_skipSky_skyLoss\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, pw=True, oneLayer=True),\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble to one ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Depth(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_list=False):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output_list = output_list\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        feature = self.encoder(inputs)\n",
    "        output = self.decoder(*tuple(feature))\n",
    "        if self.output_list:\n",
    "            list_output = []\n",
    "            for key, value in output.items():\n",
    "                list_output.append(value)\n",
    "            output = list_output\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = Model()\n",
    "Models.get_model_dict(encoder_dict, decoder_dict)\n",
    "cv2.setNumThreads(0)  # This speeds up evaluation 5x on our unix systems (OpenCV 3.3.1)\n",
    "\n",
    "if False:\n",
    "    Models.save_ONNX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.batch_evaluate_depth(save_CSV=True, is_torch2trt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.inference_depth(column=2, is_torch2trt=False, is_onnx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.inference_segment_sky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"alexnet.proto\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "onnx.helper.printable_graph(model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt\n",
    "tensorrt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇要建立 video 的 data [TODO]\n",
    "# file_name = '2011_10_03/2011_10_03_drive_0047_sync' #837\n",
    "# file_name = '2011_09_30/2011_09_30_drive_0016_sync' #279\n",
    "# file_name = '2011_09_29/2011_09_29_drive_0026_sync' #158\n",
    "# file_name = '2011_09_28/2011_09_28_drive_0037_sync' #89\n",
    "file_name = '2011_09_26/2011_09_26_drive_0036_sync' #803\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0023_sync' #474\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0020_sync' #86\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0013_sync' #144\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0002_sync' #77\n",
    "\n",
    "# 選擇影片輸出資料夾 [TODO]\n",
    "video_output_folder = os.path.join(os.path.expanduser(\"~\"), \n",
    "                                                      \"depth\",\n",
    "                                                      \"monodepth2\",\n",
    "                                                      \"video_result\")\n",
    "\n",
    "# 取得資料夾中所有影像檔案路徑\n",
    "kitti_depth_folder = '/work/garin0115/datasets/kitti_data/'+file_name+'/image_02'\n",
    "filenames = glob.glob(kitti_depth_folder+'/*/*.jpg')\n",
    "\n",
    "# 將檔案路徑排序\n",
    "filenames.sort()\n",
    "num_images = len(filenames)\n",
    "print(\"Total images: {}\".format(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 128, 416])\n",
      "torch.Size([2, 64, 64, 208])\n",
      "torch.Size([2, 128, 32, 104])\n",
      "torch.Size([2, 256, 16, 52])\n",
      "torch.Size([2, 512, 8, 26])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 64, 128, 416]           9,408\n",
      "       BatchNorm2d-2         [16, 64, 128, 416]             128\n",
      "              ReLU-3         [16, 64, 128, 416]               0\n",
      "         MaxPool2d-4          [16, 64, 64, 208]               0\n",
      "            Conv2d-5          [16, 64, 64, 208]          36,864\n",
      "       BatchNorm2d-6          [16, 64, 64, 208]             128\n",
      "              ReLU-7          [16, 64, 64, 208]               0\n",
      "            Conv2d-8          [16, 64, 64, 208]          36,864\n",
      "       BatchNorm2d-9          [16, 64, 64, 208]             128\n",
      "             ReLU-10          [16, 64, 64, 208]               0\n",
      "       BasicBlock-11          [16, 64, 64, 208]               0\n",
      "           Conv2d-12          [16, 64, 64, 208]          36,864\n",
      "      BatchNorm2d-13          [16, 64, 64, 208]             128\n",
      "             ReLU-14          [16, 64, 64, 208]               0\n",
      "           Conv2d-15          [16, 64, 64, 208]          36,864\n",
      "      BatchNorm2d-16          [16, 64, 64, 208]             128\n",
      "             ReLU-17          [16, 64, 64, 208]               0\n",
      "       BasicBlock-18          [16, 64, 64, 208]               0\n",
      "           Conv2d-19         [16, 128, 32, 104]          73,728\n",
      "      BatchNorm2d-20         [16, 128, 32, 104]             256\n",
      "             ReLU-21         [16, 128, 32, 104]               0\n",
      "           Conv2d-22         [16, 128, 32, 104]         147,456\n",
      "      BatchNorm2d-23         [16, 128, 32, 104]             256\n",
      "           Conv2d-24         [16, 128, 32, 104]           8,192\n",
      "      BatchNorm2d-25         [16, 128, 32, 104]             256\n",
      "             ReLU-26         [16, 128, 32, 104]               0\n",
      "       BasicBlock-27         [16, 128, 32, 104]               0\n",
      "           Conv2d-28         [16, 128, 32, 104]         147,456\n",
      "      BatchNorm2d-29         [16, 128, 32, 104]             256\n",
      "             ReLU-30         [16, 128, 32, 104]               0\n",
      "           Conv2d-31         [16, 128, 32, 104]         147,456\n",
      "      BatchNorm2d-32         [16, 128, 32, 104]             256\n",
      "             ReLU-33         [16, 128, 32, 104]               0\n",
      "       BasicBlock-34         [16, 128, 32, 104]               0\n",
      "           Conv2d-35          [16, 256, 16, 52]         294,912\n",
      "      BatchNorm2d-36          [16, 256, 16, 52]             512\n",
      "             ReLU-37          [16, 256, 16, 52]               0\n",
      "           Conv2d-38          [16, 256, 16, 52]         589,824\n",
      "      BatchNorm2d-39          [16, 256, 16, 52]             512\n",
      "           Conv2d-40          [16, 256, 16, 52]          32,768\n",
      "      BatchNorm2d-41          [16, 256, 16, 52]             512\n",
      "             ReLU-42          [16, 256, 16, 52]               0\n",
      "       BasicBlock-43          [16, 256, 16, 52]               0\n",
      "           Conv2d-44          [16, 256, 16, 52]         589,824\n",
      "      BatchNorm2d-45          [16, 256, 16, 52]             512\n",
      "             ReLU-46          [16, 256, 16, 52]               0\n",
      "           Conv2d-47          [16, 256, 16, 52]         589,824\n",
      "      BatchNorm2d-48          [16, 256, 16, 52]             512\n",
      "             ReLU-49          [16, 256, 16, 52]               0\n",
      "       BasicBlock-50          [16, 256, 16, 52]               0\n",
      "           Conv2d-51           [16, 512, 8, 26]       1,179,648\n",
      "      BatchNorm2d-52           [16, 512, 8, 26]           1,024\n",
      "             ReLU-53           [16, 512, 8, 26]               0\n",
      "           Conv2d-54           [16, 512, 8, 26]       2,359,296\n",
      "      BatchNorm2d-55           [16, 512, 8, 26]           1,024\n",
      "           Conv2d-56           [16, 512, 8, 26]         131,072\n",
      "      BatchNorm2d-57           [16, 512, 8, 26]           1,024\n",
      "             ReLU-58           [16, 512, 8, 26]               0\n",
      "       BasicBlock-59           [16, 512, 8, 26]               0\n",
      "           Conv2d-60           [16, 512, 8, 26]       2,359,296\n",
      "      BatchNorm2d-61           [16, 512, 8, 26]           1,024\n",
      "             ReLU-62           [16, 512, 8, 26]               0\n",
      "           Conv2d-63           [16, 512, 8, 26]       2,359,296\n",
      "      BatchNorm2d-64           [16, 512, 8, 26]           1,024\n",
      "             ReLU-65           [16, 512, 8, 26]               0\n",
      "       BasicBlock-66           [16, 512, 8, 26]               0\n",
      "================================================================\n",
      "Total params: 11,176,512\n",
      "Trainable params: 11,176,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.00\n",
      "Forward/backward pass size (MB): 4264.00\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 4345.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet18 = networks.ResnetEncoder(18, False).cuda()\n",
    "summary(resnet18, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 128, 416])\n",
      "torch.Size([2, 256, 64, 208])\n",
      "torch.Size([2, 512, 32, 104])\n",
      "torch.Size([2, 1024, 16, 52])\n",
      "torch.Size([2, 2048, 8, 26])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 64, 128, 416]           9,408\n",
      "       BatchNorm2d-2         [16, 64, 128, 416]             128\n",
      "              ReLU-3         [16, 64, 128, 416]               0\n",
      "         MaxPool2d-4          [16, 64, 64, 208]               0\n",
      "            Conv2d-5          [16, 64, 64, 208]           4,096\n",
      "       BatchNorm2d-6          [16, 64, 64, 208]             128\n",
      "              ReLU-7          [16, 64, 64, 208]               0\n",
      "            Conv2d-8          [16, 64, 64, 208]          36,864\n",
      "       BatchNorm2d-9          [16, 64, 64, 208]             128\n",
      "             ReLU-10          [16, 64, 64, 208]               0\n",
      "           Conv2d-11         [16, 256, 64, 208]          16,384\n",
      "      BatchNorm2d-12         [16, 256, 64, 208]             512\n",
      "           Conv2d-13         [16, 256, 64, 208]          16,384\n",
      "      BatchNorm2d-14         [16, 256, 64, 208]             512\n",
      "             ReLU-15         [16, 256, 64, 208]               0\n",
      "       Bottleneck-16         [16, 256, 64, 208]               0\n",
      "           Conv2d-17          [16, 64, 64, 208]          16,384\n",
      "      BatchNorm2d-18          [16, 64, 64, 208]             128\n",
      "             ReLU-19          [16, 64, 64, 208]               0\n",
      "           Conv2d-20          [16, 64, 64, 208]          36,864\n",
      "      BatchNorm2d-21          [16, 64, 64, 208]             128\n",
      "             ReLU-22          [16, 64, 64, 208]               0\n",
      "           Conv2d-23         [16, 256, 64, 208]          16,384\n",
      "      BatchNorm2d-24         [16, 256, 64, 208]             512\n",
      "             ReLU-25         [16, 256, 64, 208]               0\n",
      "       Bottleneck-26         [16, 256, 64, 208]               0\n",
      "           Conv2d-27          [16, 64, 64, 208]          16,384\n",
      "      BatchNorm2d-28          [16, 64, 64, 208]             128\n",
      "             ReLU-29          [16, 64, 64, 208]               0\n",
      "           Conv2d-30          [16, 64, 64, 208]          36,864\n",
      "      BatchNorm2d-31          [16, 64, 64, 208]             128\n",
      "             ReLU-32          [16, 64, 64, 208]               0\n",
      "           Conv2d-33         [16, 256, 64, 208]          16,384\n",
      "      BatchNorm2d-34         [16, 256, 64, 208]             512\n",
      "             ReLU-35         [16, 256, 64, 208]               0\n",
      "       Bottleneck-36         [16, 256, 64, 208]               0\n",
      "           Conv2d-37         [16, 128, 64, 208]          32,768\n",
      "      BatchNorm2d-38         [16, 128, 64, 208]             256\n",
      "             ReLU-39         [16, 128, 64, 208]               0\n",
      "           Conv2d-40         [16, 128, 32, 104]         147,456\n",
      "      BatchNorm2d-41         [16, 128, 32, 104]             256\n",
      "             ReLU-42         [16, 128, 32, 104]               0\n",
      "           Conv2d-43         [16, 512, 32, 104]          65,536\n",
      "      BatchNorm2d-44         [16, 512, 32, 104]           1,024\n",
      "           Conv2d-45         [16, 512, 32, 104]         131,072\n",
      "      BatchNorm2d-46         [16, 512, 32, 104]           1,024\n",
      "             ReLU-47         [16, 512, 32, 104]               0\n",
      "       Bottleneck-48         [16, 512, 32, 104]               0\n",
      "           Conv2d-49         [16, 128, 32, 104]          65,536\n",
      "      BatchNorm2d-50         [16, 128, 32, 104]             256\n",
      "             ReLU-51         [16, 128, 32, 104]               0\n",
      "           Conv2d-52         [16, 128, 32, 104]         147,456\n",
      "      BatchNorm2d-53         [16, 128, 32, 104]             256\n",
      "             ReLU-54         [16, 128, 32, 104]               0\n",
      "           Conv2d-55         [16, 512, 32, 104]          65,536\n",
      "      BatchNorm2d-56         [16, 512, 32, 104]           1,024\n",
      "             ReLU-57         [16, 512, 32, 104]               0\n",
      "       Bottleneck-58         [16, 512, 32, 104]               0\n",
      "           Conv2d-59         [16, 128, 32, 104]          65,536\n",
      "      BatchNorm2d-60         [16, 128, 32, 104]             256\n",
      "             ReLU-61         [16, 128, 32, 104]               0\n",
      "           Conv2d-62         [16, 128, 32, 104]         147,456\n",
      "      BatchNorm2d-63         [16, 128, 32, 104]             256\n",
      "             ReLU-64         [16, 128, 32, 104]               0\n",
      "           Conv2d-65         [16, 512, 32, 104]          65,536\n",
      "      BatchNorm2d-66         [16, 512, 32, 104]           1,024\n",
      "             ReLU-67         [16, 512, 32, 104]               0\n",
      "       Bottleneck-68         [16, 512, 32, 104]               0\n",
      "           Conv2d-69         [16, 128, 32, 104]          65,536\n",
      "      BatchNorm2d-70         [16, 128, 32, 104]             256\n",
      "             ReLU-71         [16, 128, 32, 104]               0\n",
      "           Conv2d-72         [16, 128, 32, 104]         147,456\n",
      "      BatchNorm2d-73         [16, 128, 32, 104]             256\n",
      "             ReLU-74         [16, 128, 32, 104]               0\n",
      "           Conv2d-75         [16, 512, 32, 104]          65,536\n",
      "      BatchNorm2d-76         [16, 512, 32, 104]           1,024\n",
      "             ReLU-77         [16, 512, 32, 104]               0\n",
      "       Bottleneck-78         [16, 512, 32, 104]               0\n",
      "           Conv2d-79         [16, 256, 32, 104]         131,072\n",
      "      BatchNorm2d-80         [16, 256, 32, 104]             512\n",
      "             ReLU-81         [16, 256, 32, 104]               0\n",
      "           Conv2d-82          [16, 256, 16, 52]         589,824\n",
      "      BatchNorm2d-83          [16, 256, 16, 52]             512\n",
      "             ReLU-84          [16, 256, 16, 52]               0\n",
      "           Conv2d-85         [16, 1024, 16, 52]         262,144\n",
      "      BatchNorm2d-86         [16, 1024, 16, 52]           2,048\n",
      "           Conv2d-87         [16, 1024, 16, 52]         524,288\n",
      "      BatchNorm2d-88         [16, 1024, 16, 52]           2,048\n",
      "             ReLU-89         [16, 1024, 16, 52]               0\n",
      "       Bottleneck-90         [16, 1024, 16, 52]               0\n",
      "           Conv2d-91          [16, 256, 16, 52]         262,144\n",
      "      BatchNorm2d-92          [16, 256, 16, 52]             512\n",
      "             ReLU-93          [16, 256, 16, 52]               0\n",
      "           Conv2d-94          [16, 256, 16, 52]         589,824\n",
      "      BatchNorm2d-95          [16, 256, 16, 52]             512\n",
      "             ReLU-96          [16, 256, 16, 52]               0\n",
      "           Conv2d-97         [16, 1024, 16, 52]         262,144\n",
      "      BatchNorm2d-98         [16, 1024, 16, 52]           2,048\n",
      "             ReLU-99         [16, 1024, 16, 52]               0\n",
      "      Bottleneck-100         [16, 1024, 16, 52]               0\n",
      "          Conv2d-101          [16, 256, 16, 52]         262,144\n",
      "     BatchNorm2d-102          [16, 256, 16, 52]             512\n",
      "            ReLU-103          [16, 256, 16, 52]               0\n",
      "          Conv2d-104          [16, 256, 16, 52]         589,824\n",
      "     BatchNorm2d-105          [16, 256, 16, 52]             512\n",
      "            ReLU-106          [16, 256, 16, 52]               0\n",
      "          Conv2d-107         [16, 1024, 16, 52]         262,144\n",
      "     BatchNorm2d-108         [16, 1024, 16, 52]           2,048\n",
      "            ReLU-109         [16, 1024, 16, 52]               0\n",
      "      Bottleneck-110         [16, 1024, 16, 52]               0\n",
      "          Conv2d-111          [16, 256, 16, 52]         262,144\n",
      "     BatchNorm2d-112          [16, 256, 16, 52]             512\n",
      "            ReLU-113          [16, 256, 16, 52]               0\n",
      "          Conv2d-114          [16, 256, 16, 52]         589,824\n",
      "     BatchNorm2d-115          [16, 256, 16, 52]             512\n",
      "            ReLU-116          [16, 256, 16, 52]               0\n",
      "          Conv2d-117         [16, 1024, 16, 52]         262,144\n",
      "     BatchNorm2d-118         [16, 1024, 16, 52]           2,048\n",
      "            ReLU-119         [16, 1024, 16, 52]               0\n",
      "      Bottleneck-120         [16, 1024, 16, 52]               0\n",
      "          Conv2d-121          [16, 256, 16, 52]         262,144\n",
      "     BatchNorm2d-122          [16, 256, 16, 52]             512\n",
      "            ReLU-123          [16, 256, 16, 52]               0\n",
      "          Conv2d-124          [16, 256, 16, 52]         589,824\n",
      "     BatchNorm2d-125          [16, 256, 16, 52]             512\n",
      "            ReLU-126          [16, 256, 16, 52]               0\n",
      "          Conv2d-127         [16, 1024, 16, 52]         262,144\n",
      "     BatchNorm2d-128         [16, 1024, 16, 52]           2,048\n",
      "            ReLU-129         [16, 1024, 16, 52]               0\n",
      "      Bottleneck-130         [16, 1024, 16, 52]               0\n",
      "          Conv2d-131          [16, 256, 16, 52]         262,144\n",
      "     BatchNorm2d-132          [16, 256, 16, 52]             512\n",
      "            ReLU-133          [16, 256, 16, 52]               0\n",
      "          Conv2d-134          [16, 256, 16, 52]         589,824\n",
      "     BatchNorm2d-135          [16, 256, 16, 52]             512\n",
      "            ReLU-136          [16, 256, 16, 52]               0\n",
      "          Conv2d-137         [16, 1024, 16, 52]         262,144\n",
      "     BatchNorm2d-138         [16, 1024, 16, 52]           2,048\n",
      "            ReLU-139         [16, 1024, 16, 52]               0\n",
      "      Bottleneck-140         [16, 1024, 16, 52]               0\n",
      "          Conv2d-141          [16, 512, 16, 52]         524,288\n",
      "     BatchNorm2d-142          [16, 512, 16, 52]           1,024\n",
      "            ReLU-143          [16, 512, 16, 52]               0\n",
      "          Conv2d-144           [16, 512, 8, 26]       2,359,296\n",
      "     BatchNorm2d-145           [16, 512, 8, 26]           1,024\n",
      "            ReLU-146           [16, 512, 8, 26]               0\n",
      "          Conv2d-147          [16, 2048, 8, 26]       1,048,576\n",
      "     BatchNorm2d-148          [16, 2048, 8, 26]           4,096\n",
      "          Conv2d-149          [16, 2048, 8, 26]       2,097,152\n",
      "     BatchNorm2d-150          [16, 2048, 8, 26]           4,096\n",
      "            ReLU-151          [16, 2048, 8, 26]               0\n",
      "      Bottleneck-152          [16, 2048, 8, 26]               0\n",
      "          Conv2d-153           [16, 512, 8, 26]       1,048,576\n",
      "     BatchNorm2d-154           [16, 512, 8, 26]           1,024\n",
      "            ReLU-155           [16, 512, 8, 26]               0\n",
      "          Conv2d-156           [16, 512, 8, 26]       2,359,296\n",
      "     BatchNorm2d-157           [16, 512, 8, 26]           1,024\n",
      "            ReLU-158           [16, 512, 8, 26]               0\n",
      "          Conv2d-159          [16, 2048, 8, 26]       1,048,576\n",
      "     BatchNorm2d-160          [16, 2048, 8, 26]           4,096\n",
      "            ReLU-161          [16, 2048, 8, 26]               0\n",
      "      Bottleneck-162          [16, 2048, 8, 26]               0\n",
      "          Conv2d-163           [16, 512, 8, 26]       1,048,576\n",
      "     BatchNorm2d-164           [16, 512, 8, 26]           1,024\n",
      "            ReLU-165           [16, 512, 8, 26]               0\n",
      "          Conv2d-166           [16, 512, 8, 26]       2,359,296\n",
      "     BatchNorm2d-167           [16, 512, 8, 26]           1,024\n",
      "            ReLU-168           [16, 512, 8, 26]               0\n",
      "          Conv2d-169          [16, 2048, 8, 26]       1,048,576\n",
      "     BatchNorm2d-170          [16, 2048, 8, 26]           4,096\n",
      "            ReLU-171          [16, 2048, 8, 26]               0\n",
      "      Bottleneck-172          [16, 2048, 8, 26]               0\n",
      "================================================================\n",
      "Total params: 23,508,032\n",
      "Trainable params: 23,508,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.00\n",
      "Forward/backward pass size (MB): 19461.00\n",
      "Params size (MB): 89.68\n",
      "Estimated Total Size (MB): 19589.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet50 = networks.ResnetEncoder(50, False).cuda()\n",
    "summary(resnet50, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 128, 416])\n",
      "torch.Size([2, 128, 64, 208])\n",
      "torch.Size([2, 256, 32, 104])\n",
      "torch.Size([2, 512, 16, 52])\n",
      "torch.Size([2, 1024, 8, 26])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 32, 128, 416]             864\n",
      "       BatchNorm2d-2         [16, 32, 128, 416]              64\n",
      "              ReLU-3         [16, 32, 128, 416]               0\n",
      "         ConvBlock-4         [16, 32, 128, 416]               0\n",
      "            Conv2d-5         [16, 32, 128, 416]             288\n",
      "       BatchNorm2d-6         [16, 32, 128, 416]              64\n",
      "              ReLU-7         [16, 32, 128, 416]               0\n",
      "         ConvBlock-8         [16, 32, 128, 416]               0\n",
      "            Conv2d-9         [16, 64, 128, 416]           2,048\n",
      "      BatchNorm2d-10         [16, 64, 128, 416]             128\n",
      "             ReLU-11         [16, 64, 128, 416]               0\n",
      "        ConvBlock-12         [16, 64, 128, 416]               0\n",
      "     DwsConvBlock-13         [16, 64, 128, 416]               0\n",
      "           Conv2d-14          [16, 64, 64, 208]             576\n",
      "      BatchNorm2d-15          [16, 64, 64, 208]             128\n",
      "             ReLU-16          [16, 64, 64, 208]               0\n",
      "        ConvBlock-17          [16, 64, 64, 208]               0\n",
      "           Conv2d-18         [16, 128, 64, 208]           8,192\n",
      "      BatchNorm2d-19         [16, 128, 64, 208]             256\n",
      "             ReLU-20         [16, 128, 64, 208]               0\n",
      "        ConvBlock-21         [16, 128, 64, 208]               0\n",
      "     DwsConvBlock-22         [16, 128, 64, 208]               0\n",
      "           Conv2d-23         [16, 128, 64, 208]           1,152\n",
      "      BatchNorm2d-24         [16, 128, 64, 208]             256\n",
      "             ReLU-25         [16, 128, 64, 208]               0\n",
      "        ConvBlock-26         [16, 128, 64, 208]               0\n",
      "           Conv2d-27         [16, 128, 64, 208]          16,384\n",
      "      BatchNorm2d-28         [16, 128, 64, 208]             256\n",
      "             ReLU-29         [16, 128, 64, 208]               0\n",
      "        ConvBlock-30         [16, 128, 64, 208]               0\n",
      "     DwsConvBlock-31         [16, 128, 64, 208]               0\n",
      "           Conv2d-32         [16, 128, 32, 104]           1,152\n",
      "      BatchNorm2d-33         [16, 128, 32, 104]             256\n",
      "             ReLU-34         [16, 128, 32, 104]               0\n",
      "        ConvBlock-35         [16, 128, 32, 104]               0\n",
      "           Conv2d-36         [16, 256, 32, 104]          32,768\n",
      "      BatchNorm2d-37         [16, 256, 32, 104]             512\n",
      "             ReLU-38         [16, 256, 32, 104]               0\n",
      "        ConvBlock-39         [16, 256, 32, 104]               0\n",
      "     DwsConvBlock-40         [16, 256, 32, 104]               0\n",
      "           Conv2d-41         [16, 256, 32, 104]           2,304\n",
      "      BatchNorm2d-42         [16, 256, 32, 104]             512\n",
      "             ReLU-43         [16, 256, 32, 104]               0\n",
      "        ConvBlock-44         [16, 256, 32, 104]               0\n",
      "           Conv2d-45         [16, 256, 32, 104]          65,536\n",
      "      BatchNorm2d-46         [16, 256, 32, 104]             512\n",
      "             ReLU-47         [16, 256, 32, 104]               0\n",
      "        ConvBlock-48         [16, 256, 32, 104]               0\n",
      "     DwsConvBlock-49         [16, 256, 32, 104]               0\n",
      "           Conv2d-50          [16, 256, 16, 52]           2,304\n",
      "      BatchNorm2d-51          [16, 256, 16, 52]             512\n",
      "             ReLU-52          [16, 256, 16, 52]               0\n",
      "        ConvBlock-53          [16, 256, 16, 52]               0\n",
      "           Conv2d-54          [16, 512, 16, 52]         131,072\n",
      "      BatchNorm2d-55          [16, 512, 16, 52]           1,024\n",
      "             ReLU-56          [16, 512, 16, 52]               0\n",
      "        ConvBlock-57          [16, 512, 16, 52]               0\n",
      "     DwsConvBlock-58          [16, 512, 16, 52]               0\n",
      "           Conv2d-59          [16, 512, 16, 52]           4,608\n",
      "      BatchNorm2d-60          [16, 512, 16, 52]           1,024\n",
      "             ReLU-61          [16, 512, 16, 52]               0\n",
      "        ConvBlock-62          [16, 512, 16, 52]               0\n",
      "           Conv2d-63          [16, 512, 16, 52]         262,144\n",
      "      BatchNorm2d-64          [16, 512, 16, 52]           1,024\n",
      "             ReLU-65          [16, 512, 16, 52]               0\n",
      "        ConvBlock-66          [16, 512, 16, 52]               0\n",
      "     DwsConvBlock-67          [16, 512, 16, 52]               0\n",
      "           Conv2d-68          [16, 512, 16, 52]           4,608\n",
      "      BatchNorm2d-69          [16, 512, 16, 52]           1,024\n",
      "             ReLU-70          [16, 512, 16, 52]               0\n",
      "        ConvBlock-71          [16, 512, 16, 52]               0\n",
      "           Conv2d-72          [16, 512, 16, 52]         262,144\n",
      "      BatchNorm2d-73          [16, 512, 16, 52]           1,024\n",
      "             ReLU-74          [16, 512, 16, 52]               0\n",
      "        ConvBlock-75          [16, 512, 16, 52]               0\n",
      "     DwsConvBlock-76          [16, 512, 16, 52]               0\n",
      "           Conv2d-77          [16, 512, 16, 52]           4,608\n",
      "      BatchNorm2d-78          [16, 512, 16, 52]           1,024\n",
      "             ReLU-79          [16, 512, 16, 52]               0\n",
      "        ConvBlock-80          [16, 512, 16, 52]               0\n",
      "           Conv2d-81          [16, 512, 16, 52]         262,144\n",
      "      BatchNorm2d-82          [16, 512, 16, 52]           1,024\n",
      "             ReLU-83          [16, 512, 16, 52]               0\n",
      "        ConvBlock-84          [16, 512, 16, 52]               0\n",
      "     DwsConvBlock-85          [16, 512, 16, 52]               0\n",
      "           Conv2d-86          [16, 512, 16, 52]           4,608\n",
      "      BatchNorm2d-87          [16, 512, 16, 52]           1,024\n",
      "             ReLU-88          [16, 512, 16, 52]               0\n",
      "        ConvBlock-89          [16, 512, 16, 52]               0\n",
      "           Conv2d-90          [16, 512, 16, 52]         262,144\n",
      "      BatchNorm2d-91          [16, 512, 16, 52]           1,024\n",
      "             ReLU-92          [16, 512, 16, 52]               0\n",
      "        ConvBlock-93          [16, 512, 16, 52]               0\n",
      "     DwsConvBlock-94          [16, 512, 16, 52]               0\n",
      "           Conv2d-95          [16, 512, 16, 52]           4,608\n",
      "      BatchNorm2d-96          [16, 512, 16, 52]           1,024\n",
      "             ReLU-97          [16, 512, 16, 52]               0\n",
      "        ConvBlock-98          [16, 512, 16, 52]               0\n",
      "           Conv2d-99          [16, 512, 16, 52]         262,144\n",
      "     BatchNorm2d-100          [16, 512, 16, 52]           1,024\n",
      "            ReLU-101          [16, 512, 16, 52]               0\n",
      "       ConvBlock-102          [16, 512, 16, 52]               0\n",
      "    DwsConvBlock-103          [16, 512, 16, 52]               0\n",
      "          Conv2d-104           [16, 512, 8, 26]           4,608\n",
      "     BatchNorm2d-105           [16, 512, 8, 26]           1,024\n",
      "            ReLU-106           [16, 512, 8, 26]               0\n",
      "       ConvBlock-107           [16, 512, 8, 26]               0\n",
      "          Conv2d-108          [16, 1024, 8, 26]         524,288\n",
      "     BatchNorm2d-109          [16, 1024, 8, 26]           2,048\n",
      "            ReLU-110          [16, 1024, 8, 26]               0\n",
      "       ConvBlock-111          [16, 1024, 8, 26]               0\n",
      "    DwsConvBlock-112          [16, 1024, 8, 26]               0\n",
      "          Conv2d-113          [16, 1024, 8, 26]           9,216\n",
      "     BatchNorm2d-114          [16, 1024, 8, 26]           2,048\n",
      "            ReLU-115          [16, 1024, 8, 26]               0\n",
      "       ConvBlock-116          [16, 1024, 8, 26]               0\n",
      "          Conv2d-117          [16, 1024, 8, 26]       1,048,576\n",
      "     BatchNorm2d-118          [16, 1024, 8, 26]           2,048\n",
      "            ReLU-119          [16, 1024, 8, 26]               0\n",
      "       ConvBlock-120          [16, 1024, 8, 26]               0\n",
      "    DwsConvBlock-121          [16, 1024, 8, 26]               0\n",
      "================================================================\n",
      "Total params: 3,206,976\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.00\n",
      "Forward/backward pass size (MB): 11856.00\n",
      "Params size (MB): 12.23\n",
      "Estimated Total Size (MB): 11907.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mobilenet = networks.MobileNet().cuda()\n",
    "summary(mobilenet, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 128, 416])\n",
      "torch.Size([2, 24, 64, 208])\n",
      "torch.Size([2, 32, 32, 104])\n",
      "torch.Size([2, 96, 16, 52])\n",
      "torch.Size([2, 1280, 8, 26])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 32, 128, 416]             864\n",
      "       BatchNorm2d-2         [16, 32, 128, 416]              64\n",
      "             ReLU6-3         [16, 32, 128, 416]               0\n",
      "         ConvBlock-4         [16, 32, 128, 416]               0\n",
      "            Conv2d-5         [16, 32, 128, 416]           1,024\n",
      "       BatchNorm2d-6         [16, 32, 128, 416]              64\n",
      "             ReLU6-7         [16, 32, 128, 416]               0\n",
      "         ConvBlock-8         [16, 32, 128, 416]               0\n",
      "            Conv2d-9         [16, 32, 128, 416]             288\n",
      "      BatchNorm2d-10         [16, 32, 128, 416]              64\n",
      "            ReLU6-11         [16, 32, 128, 416]               0\n",
      "        ConvBlock-12         [16, 32, 128, 416]               0\n",
      "           Conv2d-13         [16, 16, 128, 416]             512\n",
      "      BatchNorm2d-14         [16, 16, 128, 416]              32\n",
      "        ConvBlock-15         [16, 16, 128, 416]               0\n",
      " LinearBottleneck-16         [16, 16, 128, 416]               0\n",
      "           Conv2d-17         [16, 96, 128, 416]           1,536\n",
      "      BatchNorm2d-18         [16, 96, 128, 416]             192\n",
      "            ReLU6-19         [16, 96, 128, 416]               0\n",
      "        ConvBlock-20         [16, 96, 128, 416]               0\n",
      "           Conv2d-21          [16, 96, 64, 208]             864\n",
      "      BatchNorm2d-22          [16, 96, 64, 208]             192\n",
      "            ReLU6-23          [16, 96, 64, 208]               0\n",
      "        ConvBlock-24          [16, 96, 64, 208]               0\n",
      "           Conv2d-25          [16, 24, 64, 208]           2,304\n",
      "      BatchNorm2d-26          [16, 24, 64, 208]              48\n",
      "        ConvBlock-27          [16, 24, 64, 208]               0\n",
      " LinearBottleneck-28          [16, 24, 64, 208]               0\n",
      "           Conv2d-29         [16, 144, 64, 208]           3,456\n",
      "      BatchNorm2d-30         [16, 144, 64, 208]             288\n",
      "            ReLU6-31         [16, 144, 64, 208]               0\n",
      "        ConvBlock-32         [16, 144, 64, 208]               0\n",
      "           Conv2d-33         [16, 144, 64, 208]           1,296\n",
      "      BatchNorm2d-34         [16, 144, 64, 208]             288\n",
      "            ReLU6-35         [16, 144, 64, 208]               0\n",
      "        ConvBlock-36         [16, 144, 64, 208]               0\n",
      "           Conv2d-37          [16, 24, 64, 208]           3,456\n",
      "      BatchNorm2d-38          [16, 24, 64, 208]              48\n",
      "        ConvBlock-39          [16, 24, 64, 208]               0\n",
      " LinearBottleneck-40          [16, 24, 64, 208]               0\n",
      "           Conv2d-41         [16, 144, 64, 208]           3,456\n",
      "      BatchNorm2d-42         [16, 144, 64, 208]             288\n",
      "            ReLU6-43         [16, 144, 64, 208]               0\n",
      "        ConvBlock-44         [16, 144, 64, 208]               0\n",
      "           Conv2d-45         [16, 144, 32, 104]           1,296\n",
      "      BatchNorm2d-46         [16, 144, 32, 104]             288\n",
      "            ReLU6-47         [16, 144, 32, 104]               0\n",
      "        ConvBlock-48         [16, 144, 32, 104]               0\n",
      "           Conv2d-49          [16, 32, 32, 104]           4,608\n",
      "      BatchNorm2d-50          [16, 32, 32, 104]              64\n",
      "        ConvBlock-51          [16, 32, 32, 104]               0\n",
      " LinearBottleneck-52          [16, 32, 32, 104]               0\n",
      "           Conv2d-53         [16, 192, 32, 104]           6,144\n",
      "      BatchNorm2d-54         [16, 192, 32, 104]             384\n",
      "            ReLU6-55         [16, 192, 32, 104]               0\n",
      "        ConvBlock-56         [16, 192, 32, 104]               0\n",
      "           Conv2d-57         [16, 192, 32, 104]           1,728\n",
      "      BatchNorm2d-58         [16, 192, 32, 104]             384\n",
      "            ReLU6-59         [16, 192, 32, 104]               0\n",
      "        ConvBlock-60         [16, 192, 32, 104]               0\n",
      "           Conv2d-61          [16, 32, 32, 104]           6,144\n",
      "      BatchNorm2d-62          [16, 32, 32, 104]              64\n",
      "        ConvBlock-63          [16, 32, 32, 104]               0\n",
      " LinearBottleneck-64          [16, 32, 32, 104]               0\n",
      "           Conv2d-65         [16, 192, 32, 104]           6,144\n",
      "      BatchNorm2d-66         [16, 192, 32, 104]             384\n",
      "            ReLU6-67         [16, 192, 32, 104]               0\n",
      "        ConvBlock-68         [16, 192, 32, 104]               0\n",
      "           Conv2d-69         [16, 192, 32, 104]           1,728\n",
      "      BatchNorm2d-70         [16, 192, 32, 104]             384\n",
      "            ReLU6-71         [16, 192, 32, 104]               0\n",
      "        ConvBlock-72         [16, 192, 32, 104]               0\n",
      "           Conv2d-73          [16, 32, 32, 104]           6,144\n",
      "      BatchNorm2d-74          [16, 32, 32, 104]              64\n",
      "        ConvBlock-75          [16, 32, 32, 104]               0\n",
      " LinearBottleneck-76          [16, 32, 32, 104]               0\n",
      "           Conv2d-77         [16, 192, 32, 104]           6,144\n",
      "      BatchNorm2d-78         [16, 192, 32, 104]             384\n",
      "            ReLU6-79         [16, 192, 32, 104]               0\n",
      "        ConvBlock-80         [16, 192, 32, 104]               0\n",
      "           Conv2d-81          [16, 192, 16, 52]           1,728\n",
      "      BatchNorm2d-82          [16, 192, 16, 52]             384\n",
      "            ReLU6-83          [16, 192, 16, 52]               0\n",
      "        ConvBlock-84          [16, 192, 16, 52]               0\n",
      "           Conv2d-85           [16, 64, 16, 52]          12,288\n",
      "      BatchNorm2d-86           [16, 64, 16, 52]             128\n",
      "        ConvBlock-87           [16, 64, 16, 52]               0\n",
      " LinearBottleneck-88           [16, 64, 16, 52]               0\n",
      "           Conv2d-89          [16, 384, 16, 52]          24,576\n",
      "      BatchNorm2d-90          [16, 384, 16, 52]             768\n",
      "            ReLU6-91          [16, 384, 16, 52]               0\n",
      "        ConvBlock-92          [16, 384, 16, 52]               0\n",
      "           Conv2d-93          [16, 384, 16, 52]           3,456\n",
      "      BatchNorm2d-94          [16, 384, 16, 52]             768\n",
      "            ReLU6-95          [16, 384, 16, 52]               0\n",
      "        ConvBlock-96          [16, 384, 16, 52]               0\n",
      "           Conv2d-97           [16, 64, 16, 52]          24,576\n",
      "      BatchNorm2d-98           [16, 64, 16, 52]             128\n",
      "        ConvBlock-99           [16, 64, 16, 52]               0\n",
      "LinearBottleneck-100           [16, 64, 16, 52]               0\n",
      "          Conv2d-101          [16, 384, 16, 52]          24,576\n",
      "     BatchNorm2d-102          [16, 384, 16, 52]             768\n",
      "           ReLU6-103          [16, 384, 16, 52]               0\n",
      "       ConvBlock-104          [16, 384, 16, 52]               0\n",
      "          Conv2d-105          [16, 384, 16, 52]           3,456\n",
      "     BatchNorm2d-106          [16, 384, 16, 52]             768\n",
      "           ReLU6-107          [16, 384, 16, 52]               0\n",
      "       ConvBlock-108          [16, 384, 16, 52]               0\n",
      "          Conv2d-109           [16, 64, 16, 52]          24,576\n",
      "     BatchNorm2d-110           [16, 64, 16, 52]             128\n",
      "       ConvBlock-111           [16, 64, 16, 52]               0\n",
      "LinearBottleneck-112           [16, 64, 16, 52]               0\n",
      "          Conv2d-113          [16, 384, 16, 52]          24,576\n",
      "     BatchNorm2d-114          [16, 384, 16, 52]             768\n",
      "           ReLU6-115          [16, 384, 16, 52]               0\n",
      "       ConvBlock-116          [16, 384, 16, 52]               0\n",
      "          Conv2d-117          [16, 384, 16, 52]           3,456\n",
      "     BatchNorm2d-118          [16, 384, 16, 52]             768\n",
      "           ReLU6-119          [16, 384, 16, 52]               0\n",
      "       ConvBlock-120          [16, 384, 16, 52]               0\n",
      "          Conv2d-121           [16, 64, 16, 52]          24,576\n",
      "     BatchNorm2d-122           [16, 64, 16, 52]             128\n",
      "       ConvBlock-123           [16, 64, 16, 52]               0\n",
      "LinearBottleneck-124           [16, 64, 16, 52]               0\n",
      "          Conv2d-125          [16, 384, 16, 52]          24,576\n",
      "     BatchNorm2d-126          [16, 384, 16, 52]             768\n",
      "           ReLU6-127          [16, 384, 16, 52]               0\n",
      "       ConvBlock-128          [16, 384, 16, 52]               0\n",
      "          Conv2d-129          [16, 384, 16, 52]           3,456\n",
      "     BatchNorm2d-130          [16, 384, 16, 52]             768\n",
      "           ReLU6-131          [16, 384, 16, 52]               0\n",
      "       ConvBlock-132          [16, 384, 16, 52]               0\n",
      "          Conv2d-133           [16, 96, 16, 52]          36,864\n",
      "     BatchNorm2d-134           [16, 96, 16, 52]             192\n",
      "       ConvBlock-135           [16, 96, 16, 52]               0\n",
      "LinearBottleneck-136           [16, 96, 16, 52]               0\n",
      "          Conv2d-137          [16, 576, 16, 52]          55,296\n",
      "     BatchNorm2d-138          [16, 576, 16, 52]           1,152\n",
      "           ReLU6-139          [16, 576, 16, 52]               0\n",
      "       ConvBlock-140          [16, 576, 16, 52]               0\n",
      "          Conv2d-141          [16, 576, 16, 52]           5,184\n",
      "     BatchNorm2d-142          [16, 576, 16, 52]           1,152\n",
      "           ReLU6-143          [16, 576, 16, 52]               0\n",
      "       ConvBlock-144          [16, 576, 16, 52]               0\n",
      "          Conv2d-145           [16, 96, 16, 52]          55,296\n",
      "     BatchNorm2d-146           [16, 96, 16, 52]             192\n",
      "       ConvBlock-147           [16, 96, 16, 52]               0\n",
      "LinearBottleneck-148           [16, 96, 16, 52]               0\n",
      "          Conv2d-149          [16, 576, 16, 52]          55,296\n",
      "     BatchNorm2d-150          [16, 576, 16, 52]           1,152\n",
      "           ReLU6-151          [16, 576, 16, 52]               0\n",
      "       ConvBlock-152          [16, 576, 16, 52]               0\n",
      "          Conv2d-153          [16, 576, 16, 52]           5,184\n",
      "     BatchNorm2d-154          [16, 576, 16, 52]           1,152\n",
      "           ReLU6-155          [16, 576, 16, 52]               0\n",
      "       ConvBlock-156          [16, 576, 16, 52]               0\n",
      "          Conv2d-157           [16, 96, 16, 52]          55,296\n",
      "     BatchNorm2d-158           [16, 96, 16, 52]             192\n",
      "       ConvBlock-159           [16, 96, 16, 52]               0\n",
      "LinearBottleneck-160           [16, 96, 16, 52]               0\n",
      "          Conv2d-161          [16, 576, 16, 52]          55,296\n",
      "     BatchNorm2d-162          [16, 576, 16, 52]           1,152\n",
      "           ReLU6-163          [16, 576, 16, 52]               0\n",
      "       ConvBlock-164          [16, 576, 16, 52]               0\n",
      "          Conv2d-165           [16, 576, 8, 26]           5,184\n",
      "     BatchNorm2d-166           [16, 576, 8, 26]           1,152\n",
      "           ReLU6-167           [16, 576, 8, 26]               0\n",
      "       ConvBlock-168           [16, 576, 8, 26]               0\n",
      "          Conv2d-169           [16, 160, 8, 26]          92,160\n",
      "     BatchNorm2d-170           [16, 160, 8, 26]             320\n",
      "       ConvBlock-171           [16, 160, 8, 26]               0\n",
      "LinearBottleneck-172           [16, 160, 8, 26]               0\n",
      "          Conv2d-173           [16, 960, 8, 26]         153,600\n",
      "     BatchNorm2d-174           [16, 960, 8, 26]           1,920\n",
      "           ReLU6-175           [16, 960, 8, 26]               0\n",
      "       ConvBlock-176           [16, 960, 8, 26]               0\n",
      "          Conv2d-177           [16, 960, 8, 26]           8,640\n",
      "     BatchNorm2d-178           [16, 960, 8, 26]           1,920\n",
      "           ReLU6-179           [16, 960, 8, 26]               0\n",
      "       ConvBlock-180           [16, 960, 8, 26]               0\n",
      "          Conv2d-181           [16, 160, 8, 26]         153,600\n",
      "     BatchNorm2d-182           [16, 160, 8, 26]             320\n",
      "       ConvBlock-183           [16, 160, 8, 26]               0\n",
      "LinearBottleneck-184           [16, 160, 8, 26]               0\n",
      "          Conv2d-185           [16, 960, 8, 26]         153,600\n",
      "     BatchNorm2d-186           [16, 960, 8, 26]           1,920\n",
      "           ReLU6-187           [16, 960, 8, 26]               0\n",
      "       ConvBlock-188           [16, 960, 8, 26]               0\n",
      "          Conv2d-189           [16, 960, 8, 26]           8,640\n",
      "     BatchNorm2d-190           [16, 960, 8, 26]           1,920\n",
      "           ReLU6-191           [16, 960, 8, 26]               0\n",
      "       ConvBlock-192           [16, 960, 8, 26]               0\n",
      "          Conv2d-193           [16, 160, 8, 26]         153,600\n",
      "     BatchNorm2d-194           [16, 160, 8, 26]             320\n",
      "       ConvBlock-195           [16, 160, 8, 26]               0\n",
      "LinearBottleneck-196           [16, 160, 8, 26]               0\n",
      "          Conv2d-197           [16, 960, 8, 26]         153,600\n",
      "     BatchNorm2d-198           [16, 960, 8, 26]           1,920\n",
      "           ReLU6-199           [16, 960, 8, 26]               0\n",
      "       ConvBlock-200           [16, 960, 8, 26]               0\n",
      "          Conv2d-201           [16, 960, 8, 26]           8,640\n",
      "     BatchNorm2d-202           [16, 960, 8, 26]           1,920\n",
      "           ReLU6-203           [16, 960, 8, 26]               0\n",
      "       ConvBlock-204           [16, 960, 8, 26]               0\n",
      "          Conv2d-205           [16, 320, 8, 26]         307,200\n",
      "     BatchNorm2d-206           [16, 320, 8, 26]             640\n",
      "       ConvBlock-207           [16, 320, 8, 26]               0\n",
      "LinearBottleneck-208           [16, 320, 8, 26]               0\n",
      "          Conv2d-209          [16, 1280, 8, 26]         409,600\n",
      "     BatchNorm2d-210          [16, 1280, 8, 26]           2,560\n",
      "           ReLU6-211          [16, 1280, 8, 26]               0\n",
      "       ConvBlock-212          [16, 1280, 8, 26]               0\n",
      "================================================================\n",
      "Total params: 2,224,960\n",
      "Trainable params: 2,224,960\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.00\n",
      "Forward/backward pass size (MB): 14673.75\n",
      "Params size (MB): 8.49\n",
      "Estimated Total Size (MB): 14721.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mobilenetv2 = networks.MobileNetV2().cuda()\n",
    "summary(mobilenetv2, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 128, 416])\n",
      "torch.Size([2, 24, 64, 208])\n",
      "torch.Size([2, 40, 32, 104])\n",
      "torch.Size([2, 112, 16, 52])\n",
      "torch.Size([2, 960, 8, 26])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 16, 128, 416]             432\n",
      "       BatchNorm2d-2         [16, 16, 128, 416]              32\n",
      "            HSwish-3         [16, 16, 128, 416]               0\n",
      "         ConvBlock-4         [16, 16, 128, 416]               0\n",
      "            Conv2d-5         [16, 16, 128, 416]             144\n",
      "       BatchNorm2d-6         [16, 16, 128, 416]              32\n",
      "              ReLU-7         [16, 16, 128, 416]               0\n",
      "         ConvBlock-8         [16, 16, 128, 416]               0\n",
      "            Conv2d-9         [16, 16, 128, 416]             256\n",
      "      BatchNorm2d-10         [16, 16, 128, 416]              32\n",
      "        ConvBlock-11         [16, 16, 128, 416]               0\n",
      "  MobileNetV3Unit-12         [16, 16, 128, 416]               0\n",
      "           Conv2d-13         [16, 64, 128, 416]           1,024\n",
      "      BatchNorm2d-14         [16, 64, 128, 416]             128\n",
      "             ReLU-15         [16, 64, 128, 416]               0\n",
      "        ConvBlock-16         [16, 64, 128, 416]               0\n",
      "           Conv2d-17          [16, 64, 64, 208]             576\n",
      "      BatchNorm2d-18          [16, 64, 64, 208]             128\n",
      "             ReLU-19          [16, 64, 64, 208]               0\n",
      "        ConvBlock-20          [16, 64, 64, 208]               0\n",
      "           Conv2d-21          [16, 24, 64, 208]           1,536\n",
      "      BatchNorm2d-22          [16, 24, 64, 208]              48\n",
      "        ConvBlock-23          [16, 24, 64, 208]               0\n",
      "  MobileNetV3Unit-24          [16, 24, 64, 208]               0\n",
      "           Conv2d-25          [16, 72, 64, 208]           1,728\n",
      "      BatchNorm2d-26          [16, 72, 64, 208]             144\n",
      "             ReLU-27          [16, 72, 64, 208]               0\n",
      "        ConvBlock-28          [16, 72, 64, 208]               0\n",
      "           Conv2d-29          [16, 72, 64, 208]             648\n",
      "      BatchNorm2d-30          [16, 72, 64, 208]             144\n",
      "             ReLU-31          [16, 72, 64, 208]               0\n",
      "        ConvBlock-32          [16, 72, 64, 208]               0\n",
      "           Conv2d-33          [16, 24, 64, 208]           1,728\n",
      "      BatchNorm2d-34          [16, 24, 64, 208]              48\n",
      "        ConvBlock-35          [16, 24, 64, 208]               0\n",
      "  MobileNetV3Unit-36          [16, 24, 64, 208]               0\n",
      "           Conv2d-37          [16, 72, 64, 208]           1,728\n",
      "      BatchNorm2d-38          [16, 72, 64, 208]             144\n",
      "             ReLU-39          [16, 72, 64, 208]               0\n",
      "        ConvBlock-40          [16, 72, 64, 208]               0\n",
      "           Conv2d-41          [16, 72, 32, 104]           1,800\n",
      "      BatchNorm2d-42          [16, 72, 32, 104]             144\n",
      "             ReLU-43          [16, 72, 32, 104]               0\n",
      "        ConvBlock-44          [16, 72, 32, 104]               0\n",
      "AdaptiveAvgPool2d-45             [16, 72, 1, 1]               0\n",
      "           Conv2d-46             [16, 24, 1, 1]           1,752\n",
      "             ReLU-47             [16, 24, 1, 1]               0\n",
      "           Conv2d-48             [16, 72, 1, 1]           1,800\n",
      "         HSigmoid-49             [16, 72, 1, 1]               0\n",
      "          SEBlock-50          [16, 72, 32, 104]               0\n",
      "           Conv2d-51          [16, 40, 32, 104]           2,880\n",
      "      BatchNorm2d-52          [16, 40, 32, 104]              80\n",
      "        ConvBlock-53          [16, 40, 32, 104]               0\n",
      "  MobileNetV3Unit-54          [16, 40, 32, 104]               0\n",
      "           Conv2d-55         [16, 120, 32, 104]           4,800\n",
      "      BatchNorm2d-56         [16, 120, 32, 104]             240\n",
      "             ReLU-57         [16, 120, 32, 104]               0\n",
      "        ConvBlock-58         [16, 120, 32, 104]               0\n",
      "           Conv2d-59         [16, 120, 32, 104]           3,000\n",
      "      BatchNorm2d-60         [16, 120, 32, 104]             240\n",
      "             ReLU-61         [16, 120, 32, 104]               0\n",
      "        ConvBlock-62         [16, 120, 32, 104]               0\n",
      "AdaptiveAvgPool2d-63            [16, 120, 1, 1]               0\n",
      "           Conv2d-64             [16, 32, 1, 1]           3,872\n",
      "             ReLU-65             [16, 32, 1, 1]               0\n",
      "           Conv2d-66            [16, 120, 1, 1]           3,960\n",
      "         HSigmoid-67            [16, 120, 1, 1]               0\n",
      "          SEBlock-68         [16, 120, 32, 104]               0\n",
      "           Conv2d-69          [16, 40, 32, 104]           4,800\n",
      "      BatchNorm2d-70          [16, 40, 32, 104]              80\n",
      "        ConvBlock-71          [16, 40, 32, 104]               0\n",
      "  MobileNetV3Unit-72          [16, 40, 32, 104]               0\n",
      "           Conv2d-73         [16, 120, 32, 104]           4,800\n",
      "      BatchNorm2d-74         [16, 120, 32, 104]             240\n",
      "             ReLU-75         [16, 120, 32, 104]               0\n",
      "        ConvBlock-76         [16, 120, 32, 104]               0\n",
      "           Conv2d-77         [16, 120, 32, 104]           3,000\n",
      "      BatchNorm2d-78         [16, 120, 32, 104]             240\n",
      "             ReLU-79         [16, 120, 32, 104]               0\n",
      "        ConvBlock-80         [16, 120, 32, 104]               0\n",
      "AdaptiveAvgPool2d-81            [16, 120, 1, 1]               0\n",
      "           Conv2d-82             [16, 32, 1, 1]           3,872\n",
      "             ReLU-83             [16, 32, 1, 1]               0\n",
      "           Conv2d-84            [16, 120, 1, 1]           3,960\n",
      "         HSigmoid-85            [16, 120, 1, 1]               0\n",
      "          SEBlock-86         [16, 120, 32, 104]               0\n",
      "           Conv2d-87          [16, 40, 32, 104]           4,800\n",
      "      BatchNorm2d-88          [16, 40, 32, 104]              80\n",
      "        ConvBlock-89          [16, 40, 32, 104]               0\n",
      "  MobileNetV3Unit-90          [16, 40, 32, 104]               0\n",
      "           Conv2d-91         [16, 240, 32, 104]           9,600\n",
      "      BatchNorm2d-92         [16, 240, 32, 104]             480\n",
      "           HSwish-93         [16, 240, 32, 104]               0\n",
      "        ConvBlock-94         [16, 240, 32, 104]               0\n",
      "           Conv2d-95          [16, 240, 16, 52]           2,160\n",
      "      BatchNorm2d-96          [16, 240, 16, 52]             480\n",
      "           HSwish-97          [16, 240, 16, 52]               0\n",
      "        ConvBlock-98          [16, 240, 16, 52]               0\n",
      "           Conv2d-99           [16, 80, 16, 52]          19,200\n",
      "     BatchNorm2d-100           [16, 80, 16, 52]             160\n",
      "       ConvBlock-101           [16, 80, 16, 52]               0\n",
      " MobileNetV3Unit-102           [16, 80, 16, 52]               0\n",
      "          Conv2d-103          [16, 200, 16, 52]          16,000\n",
      "     BatchNorm2d-104          [16, 200, 16, 52]             400\n",
      "          HSwish-105          [16, 200, 16, 52]               0\n",
      "       ConvBlock-106          [16, 200, 16, 52]               0\n",
      "          Conv2d-107          [16, 200, 16, 52]           1,800\n",
      "     BatchNorm2d-108          [16, 200, 16, 52]             400\n",
      "          HSwish-109          [16, 200, 16, 52]               0\n",
      "       ConvBlock-110          [16, 200, 16, 52]               0\n",
      "          Conv2d-111           [16, 80, 16, 52]          16,000\n",
      "     BatchNorm2d-112           [16, 80, 16, 52]             160\n",
      "       ConvBlock-113           [16, 80, 16, 52]               0\n",
      " MobileNetV3Unit-114           [16, 80, 16, 52]               0\n",
      "          Conv2d-115          [16, 184, 16, 52]          14,720\n",
      "     BatchNorm2d-116          [16, 184, 16, 52]             368\n",
      "          HSwish-117          [16, 184, 16, 52]               0\n",
      "       ConvBlock-118          [16, 184, 16, 52]               0\n",
      "          Conv2d-119          [16, 184, 16, 52]           1,656\n",
      "     BatchNorm2d-120          [16, 184, 16, 52]             368\n",
      "          HSwish-121          [16, 184, 16, 52]               0\n",
      "       ConvBlock-122          [16, 184, 16, 52]               0\n",
      "          Conv2d-123           [16, 80, 16, 52]          14,720\n",
      "     BatchNorm2d-124           [16, 80, 16, 52]             160\n",
      "       ConvBlock-125           [16, 80, 16, 52]               0\n",
      " MobileNetV3Unit-126           [16, 80, 16, 52]               0\n",
      "          Conv2d-127          [16, 184, 16, 52]          14,720\n",
      "     BatchNorm2d-128          [16, 184, 16, 52]             368\n",
      "          HSwish-129          [16, 184, 16, 52]               0\n",
      "       ConvBlock-130          [16, 184, 16, 52]               0\n",
      "          Conv2d-131          [16, 184, 16, 52]           1,656\n",
      "     BatchNorm2d-132          [16, 184, 16, 52]             368\n",
      "          HSwish-133          [16, 184, 16, 52]               0\n",
      "       ConvBlock-134          [16, 184, 16, 52]               0\n",
      "          Conv2d-135           [16, 80, 16, 52]          14,720\n",
      "     BatchNorm2d-136           [16, 80, 16, 52]             160\n",
      "       ConvBlock-137           [16, 80, 16, 52]               0\n",
      " MobileNetV3Unit-138           [16, 80, 16, 52]               0\n",
      "          Conv2d-139          [16, 480, 16, 52]          38,400\n",
      "     BatchNorm2d-140          [16, 480, 16, 52]             960\n",
      "          HSwish-141          [16, 480, 16, 52]               0\n",
      "       ConvBlock-142          [16, 480, 16, 52]               0\n",
      "          Conv2d-143          [16, 480, 16, 52]           4,320\n",
      "     BatchNorm2d-144          [16, 480, 16, 52]             960\n",
      "          HSwish-145          [16, 480, 16, 52]               0\n",
      "       ConvBlock-146          [16, 480, 16, 52]               0\n",
      "AdaptiveAvgPool2d-147            [16, 480, 1, 1]               0\n",
      "          Conv2d-148            [16, 120, 1, 1]          57,720\n",
      "            ReLU-149            [16, 120, 1, 1]               0\n",
      "          Conv2d-150            [16, 480, 1, 1]          58,080\n",
      "        HSigmoid-151            [16, 480, 1, 1]               0\n",
      "         SEBlock-152          [16, 480, 16, 52]               0\n",
      "          Conv2d-153          [16, 112, 16, 52]          53,760\n",
      "     BatchNorm2d-154          [16, 112, 16, 52]             224\n",
      "       ConvBlock-155          [16, 112, 16, 52]               0\n",
      " MobileNetV3Unit-156          [16, 112, 16, 52]               0\n",
      "          Conv2d-157          [16, 672, 16, 52]          75,264\n",
      "     BatchNorm2d-158          [16, 672, 16, 52]           1,344\n",
      "          HSwish-159          [16, 672, 16, 52]               0\n",
      "       ConvBlock-160          [16, 672, 16, 52]               0\n",
      "          Conv2d-161          [16, 672, 16, 52]           6,048\n",
      "     BatchNorm2d-162          [16, 672, 16, 52]           1,344\n",
      "          HSwish-163          [16, 672, 16, 52]               0\n",
      "       ConvBlock-164          [16, 672, 16, 52]               0\n",
      "AdaptiveAvgPool2d-165            [16, 672, 1, 1]               0\n",
      "          Conv2d-166            [16, 168, 1, 1]         113,064\n",
      "            ReLU-167            [16, 168, 1, 1]               0\n",
      "          Conv2d-168            [16, 672, 1, 1]         113,568\n",
      "        HSigmoid-169            [16, 672, 1, 1]               0\n",
      "         SEBlock-170          [16, 672, 16, 52]               0\n",
      "          Conv2d-171          [16, 112, 16, 52]          75,264\n",
      "     BatchNorm2d-172          [16, 112, 16, 52]             224\n",
      "       ConvBlock-173          [16, 112, 16, 52]               0\n",
      " MobileNetV3Unit-174          [16, 112, 16, 52]               0\n",
      "          Conv2d-175          [16, 672, 16, 52]          75,264\n",
      "     BatchNorm2d-176          [16, 672, 16, 52]           1,344\n",
      "          HSwish-177          [16, 672, 16, 52]               0\n",
      "       ConvBlock-178          [16, 672, 16, 52]               0\n",
      "          Conv2d-179           [16, 672, 8, 26]          16,800\n",
      "     BatchNorm2d-180           [16, 672, 8, 26]           1,344\n",
      "          HSwish-181           [16, 672, 8, 26]               0\n",
      "       ConvBlock-182           [16, 672, 8, 26]               0\n",
      "AdaptiveAvgPool2d-183            [16, 672, 1, 1]               0\n",
      "          Conv2d-184            [16, 168, 1, 1]         113,064\n",
      "            ReLU-185            [16, 168, 1, 1]               0\n",
      "          Conv2d-186            [16, 672, 1, 1]         113,568\n",
      "        HSigmoid-187            [16, 672, 1, 1]               0\n",
      "         SEBlock-188           [16, 672, 8, 26]               0\n",
      "          Conv2d-189           [16, 160, 8, 26]         107,520\n",
      "     BatchNorm2d-190           [16, 160, 8, 26]             320\n",
      "       ConvBlock-191           [16, 160, 8, 26]               0\n",
      " MobileNetV3Unit-192           [16, 160, 8, 26]               0\n",
      "          Conv2d-193           [16, 960, 8, 26]         153,600\n",
      "     BatchNorm2d-194           [16, 960, 8, 26]           1,920\n",
      "          HSwish-195           [16, 960, 8, 26]               0\n",
      "       ConvBlock-196           [16, 960, 8, 26]               0\n",
      "          Conv2d-197           [16, 960, 8, 26]          24,000\n",
      "     BatchNorm2d-198           [16, 960, 8, 26]           1,920\n",
      "          HSwish-199           [16, 960, 8, 26]               0\n",
      "       ConvBlock-200           [16, 960, 8, 26]               0\n",
      "AdaptiveAvgPool2d-201            [16, 960, 1, 1]               0\n",
      "          Conv2d-202            [16, 240, 1, 1]         230,640\n",
      "            ReLU-203            [16, 240, 1, 1]               0\n",
      "          Conv2d-204            [16, 960, 1, 1]         231,360\n",
      "        HSigmoid-205            [16, 960, 1, 1]               0\n",
      "         SEBlock-206           [16, 960, 8, 26]               0\n",
      "          Conv2d-207           [16, 160, 8, 26]         153,600\n",
      "     BatchNorm2d-208           [16, 160, 8, 26]             320\n",
      "       ConvBlock-209           [16, 160, 8, 26]               0\n",
      " MobileNetV3Unit-210           [16, 160, 8, 26]               0\n",
      "          Conv2d-211           [16, 960, 8, 26]         153,600\n",
      "     BatchNorm2d-212           [16, 960, 8, 26]           1,920\n",
      "          HSwish-213           [16, 960, 8, 26]               0\n",
      "       ConvBlock-214           [16, 960, 8, 26]               0\n",
      "          Conv2d-215           [16, 960, 8, 26]          24,000\n",
      "     BatchNorm2d-216           [16, 960, 8, 26]           1,920\n",
      "          HSwish-217           [16, 960, 8, 26]               0\n",
      "       ConvBlock-218           [16, 960, 8, 26]               0\n",
      "AdaptiveAvgPool2d-219            [16, 960, 1, 1]               0\n",
      "          Conv2d-220            [16, 240, 1, 1]         230,640\n",
      "            ReLU-221            [16, 240, 1, 1]               0\n",
      "          Conv2d-222            [16, 960, 1, 1]         231,360\n",
      "        HSigmoid-223            [16, 960, 1, 1]               0\n",
      "         SEBlock-224           [16, 960, 8, 26]               0\n",
      "          Conv2d-225           [16, 160, 8, 26]         153,600\n",
      "     BatchNorm2d-226           [16, 160, 8, 26]             320\n",
      "       ConvBlock-227           [16, 160, 8, 26]               0\n",
      " MobileNetV3Unit-228           [16, 160, 8, 26]               0\n",
      "          Conv2d-229           [16, 960, 8, 26]         153,600\n",
      "     BatchNorm2d-230           [16, 960, 8, 26]           1,920\n",
      "          HSwish-231           [16, 960, 8, 26]               0\n",
      "       ConvBlock-232           [16, 960, 8, 26]               0\n",
      "MobileNetV3FinalBlock-233           [16, 960, 8, 26]               0\n",
      "================================================================\n",
      "Total params: 2,971,952\n",
      "Trainable params: 2,971,952\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.00\n",
      "Forward/backward pass size (MB): 9455.17\n",
      "Params size (MB): 11.34\n",
      "Estimated Total Size (MB): 9505.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mobilenetv3 = networks.MobileNetV3().cuda()\n",
    "summary(mobilenetv3, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
