{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch2trt import torch2trt, TRTModule\n",
    "\n",
    "\n",
    "from onnx2trt import get_engine, allocate_buffers, do_inference\n",
    "\n",
    "from layers import disp_to_depth\n",
    "from utils import readlines\n",
    "import datasets\n",
    "import networks\n",
    "import time\n",
    "from thop import profile, clever_format\n",
    "\n",
    "import PIL.Image as pil\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, name, net=None, size=[256, 832]):\n",
    "        self.name = name\n",
    "        self.size = size #[height, width]\n",
    "        self.net = net\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.size\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_net(self):\n",
    "        if self.net == None:\n",
    "            logging.warning('Must set net first')\n",
    "        return self.net\n",
    "    \n",
    "class Depth(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output2list=True): #trt必須將結果轉成list(dict會錯誤)\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output2list = output2list\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        feature = self.encoder(inputs)\n",
    "        output = self.decoder(feature)\n",
    "        output_list = []\n",
    "        if self.output2list:\n",
    "            for i in range(4):\n",
    "                output_list.append(output[(\"disp\", 3-i)])\n",
    "            output = output_list\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.is_set_Net = False\n",
    "        \n",
    "        # Models which were trained with stereo supervision were trained with a nominal\n",
    "        # baseline of 0.1 units. The KITTI rig has a baseline of 54cm. Therefore,\n",
    "        # to convert our stereo predictions to real-world scale we multiply our depths by 5.4.\n",
    "        self.STEREO_SCALE_FACTOR = 5.4\n",
    "        self.MIN_DEPTH = 1e-3\n",
    "        self.MAX_DEPTH = 80\n",
    "        self.disable_median_scaling = False\n",
    "        self.pred_depth_scale_factor = 1\n",
    "        self.CMAP = 'plasma'\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "        \n",
    "        self.no_cuda = False\n",
    "        self.ext = \"jpg\"\n",
    "        self.data_path = \"/work/garin0115/datasets/kitti_data/\"\n",
    "        self.splits_dir = os.path.join(os.path.expanduser(\"~\"), \"depth\", \"monodepth2\", \"splits\")\n",
    "        self.eval_split = \"eigen\"\n",
    "        self.split_folder = os.path.join(self.splits_dir, self.eval_split)\n",
    "        if torch.cuda.is_available() and not self.no_cuda:\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        self.encoder_dict = {\n",
    "            \"resnet18\":networks.ResnetEncoder(18, False),\n",
    "            \"resnet50\":networks.ResnetEncoder(50, False),\n",
    "            \"mobilenet\":networks.MobileNet(),\n",
    "            \"mobilenetv2\":networks.MobileNetV2(),\n",
    "            \"mobilenetv3\":networks.MobileNetV3(),\n",
    "            \"shufflenetv2\":networks.ShuffleNetV2(),\n",
    "            \"peleenet\":networks.PeleeNet(),\n",
    "            \"mnasnet\":networks.MnasNet()\n",
    "        }\n",
    "        self.decoder_dict = {\n",
    "            \"mj\":networks.MJDecoder,\n",
    "            \"ys\":networks.YSDecoder,\n",
    "            \"mono\":networks.MonoDecoder,\n",
    "            \"ours\":networks.OursDecoder\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def set_Net(self, size=[256, 832]):\n",
    "        self.nets = []\n",
    "        for name in self.model_name:\n",
    "            print(\"[Info] Deal with {} model\\n\".format(name))\n",
    "            if \"trt16\" in name.split(\"_\"):\n",
    "                trt = 16 \n",
    "            elif  \"trt32\" in name.split(\"_\"):\n",
    "                trt = 32 \n",
    "            else:\n",
    "                trt = None\n",
    "            \n",
    "            net = self.load_model(name, epoch=19, size=size, trt=trt)\n",
    "            if not trt:\n",
    "                net = copy.deepcopy(net)\n",
    "            self.nets.append(Net(name, net, size))\n",
    "            \n",
    "    \n",
    "    def load_model(self, name, epoch=19, size=[256, 832], trt=None):\n",
    "        load_weights_folder = self.get_modelPath(name, epoch=epoch, trt=trt)\n",
    "        if trt:\n",
    "            model_pth = torch.load(os.path.join(load_weights_folder, f\"{name}.pth\"))\n",
    "            net = TRTModule()\n",
    "            net.load_state_dict(model_pth)\n",
    "            \n",
    "        else:\n",
    "            encoder_pth = torch.load(os.path.join(load_weights_folder, \"encoder.pth\"))\n",
    "            decoder_pth = torch.load(os.path.join(load_weights_folder, \"depth.pth\"))\n",
    "            assert size==[encoder_pth[\"height\"], encoder_pth[\"width\"]]\n",
    "            # 分析 name\n",
    "            name_split = name.split(\"_\")     \n",
    "            encoder = self.encoder_dict[name_split[0]]\n",
    "            if \"ours\" in name_split:\n",
    "                ablation = {\"bn\":False, \n",
    "                            \"oneLayer\":False, \n",
    "                            \"dw\":False, \n",
    "                            \"pw\":False,\n",
    "                            \"skipAdd\":False, \n",
    "                            \"skipInput\":False}\n",
    "                for key, abl in ablation.items():\n",
    "                    if key in name_split:\n",
    "                        ablation[key] = True\n",
    "                decoder = self.decoder_dict[name_split[1]](num_ch_enc=encoder.num_ch_enc,\n",
    "                                                           bn=ablation[\"bn\"], \n",
    "                                                           dw=ablation[\"dw\"], \n",
    "                                                           pw=ablation[\"pw\"], \n",
    "                                                           oneLayer=ablation[\"oneLayer\"], \n",
    "                                                           skipAdd=ablation[\"skipAdd\"], \n",
    "                                                           skipInput=ablation[\"skipInput\"])\n",
    "            else:\n",
    "                decoder = self.decoder_dict[name_split[1]](num_ch_enc=encoder.num_ch_enc)\n",
    "\n",
    "            encoder.load_state_dict({k: v for k, v in encoder_pth.items() if k in encoder.state_dict()})\n",
    "            decoder.load_state_dict(decoder_pth)\n",
    "            net = Depth(encoder, decoder, True)\n",
    "        \n",
    "        return net\n",
    "    \n",
    "        \n",
    "    \n",
    "    def get_modelPath(self, name, epoch=19, size=[256, 832], trt=None):\n",
    "        if trt:\n",
    "            load_weights_folder = os.path.join(\"/work\", \"garin0115\", \"models\", f\"trt{trt}_models\")\n",
    "        else:\n",
    "            load_weights_folder = os.path.join(\"/work\", \"garin0115\", \"models\", f\"{name}_{size[0]}x{size[1]}\", \"models\")\n",
    "            if not os.path.isdir(load_weights_folder):\n",
    "                load_weights_folder = os.path.join(os.path.expanduser(\"~\"), \n",
    "                                                   \"depth\", \n",
    "                                                   \"monodepth2\",\n",
    "                                                   \"models\", \n",
    "                                                   f\"{name}_{size[0]}x{size[1]}\", \n",
    "                                                   \"models\")\n",
    "\n",
    "            assert os.path.isdir(load_weights_folder), \"Cannot find a folder at {}\".format(load_weights_folder)\n",
    "\n",
    "            print(\"[info] Loading weights from {}\".format(load_weights_folder))\n",
    "\n",
    "            load_weights_folder = os.path.join(load_weights_folder, f\"weights_{epoch}\")\n",
    "        \n",
    "        return load_weights_folder\n",
    "    \n",
    "    def get_dataLoader(self, height, width, batch_size=12):\n",
    "        filenames  = readlines(os.path.join(self.split_folder, \"test_files.txt\"))\n",
    "        dataset    = datasets.KITTIRAWDataset(data_path =self.data_path, \n",
    "                                              filenames =filenames,\n",
    "                                               height    =height, \n",
    "                                               width     =width,\n",
    "                                               frame_idxs=[0], \n",
    "                                               num_scales=4, \n",
    "                                               is_train  =False)\n",
    "        dataLoader = DataLoader(dataset    =dataset,\n",
    "                                batch_size =batch_size,\n",
    "                                shuffle    =False,\n",
    "                                num_workers=12,\n",
    "                                pin_memory =True,\n",
    "                                drop_last  =False)\n",
    "        return dataLoader\n",
    "    \n",
    "    def batch_evaluate_depth(self, save_CSV=False, is_torch2trt=False):\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "            \n",
    "        results = []\n",
    "        \n",
    "        for net in self.nets:\n",
    "            disps, time_min, time_avg = self.evaluate_depth(net)\n",
    "            result = self.calculate_metric(net.get_name(), disps, time_min, time_avg)\n",
    "            results.append(result)\n",
    "            \n",
    "#             disps, time_min, time_avg = self.evaluate_onnx_depth(net.get_name(), fp16_mode=False)\n",
    "#             result = self.calculate_metric(net.get_name()+\"_trt32\", disps, time_min, time_avg)\n",
    "#             results.append(result)\n",
    "            \n",
    "#             disps, time_min, time_avg = self.evaluate_onnx_depth(net.get_name(), fp16_mode=True)\n",
    "#             result = self.calculate_metric(net.get_name()+\"_trt16\", disps, time_min, time_avg)\n",
    "#             results.append(result)\n",
    "        \n",
    "        if save_CSV:\n",
    "            import csv\n",
    "            # 開啟輸出的 CSV 檔案\n",
    "            with open('result.csv', 'w', newline='') as csvfile:\n",
    "                # 建立 CSV 檔寫入器\n",
    "                writer = csv.writer(csvfile)\n",
    "\n",
    "                # 寫入一列資料\n",
    "#                 writer.writerow(['Model', 'Height', 'Width', \"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\", \n",
    "#                               'Best FPS', 'Avg FPS', 'Parameters', 'params_enc', 'params_dec', 'FLOPs', 'fl_enc', 'fl_dec'])\n",
    "                writer.writerow(['Model', 'Height', 'Width', \"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\", \n",
    "                              'Best FPS', 'Avg FPS'])\n",
    "\n",
    "                # 寫入另外幾列資料\n",
    "                for res in results:\n",
    "                    writer.writerow(res)\n",
    "            \n",
    "    def calculate_metric(self, name, pred_disps, time_min, time_avg):\n",
    "        gt_path = os.path.join(self.split_folder, \"gt_depths.npz\")\n",
    "        gt_depths = np.load(gt_path, fix_imports=True, encoding='latin1', allow_pickle=True)[\"data\"]\n",
    "        \n",
    "        errors = []\n",
    "        ratios = []\n",
    "\n",
    "        for i in range(pred_disps.shape[0]):\n",
    "\n",
    "            gt_depth = gt_depths[i]\n",
    "            gt_height, gt_width = gt_depth.shape[:2]\n",
    "\n",
    "            pred_disp = pred_disps[i]\n",
    "            pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n",
    "            pred_depth = 1 / pred_disp\n",
    "\n",
    "            if self.eval_split == \"eigen\":\n",
    "                mask = np.logical_and(gt_depth > self.MIN_DEPTH, gt_depth < self.MAX_DEPTH)\n",
    "\n",
    "                crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n",
    "                                 0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n",
    "                crop_mask = np.zeros(mask.shape)\n",
    "                crop_mask[crop[0]:crop[1], crop[2]:crop[3]] = 1\n",
    "                mask = np.logical_and(mask, crop_mask)\n",
    "\n",
    "            else:\n",
    "                mask = gt_depth > 0\n",
    "\n",
    "            pred_depth = pred_depth[mask]\n",
    "            gt_depth = gt_depth[mask]\n",
    "\n",
    "            pred_depth *= self.pred_depth_scale_factor\n",
    "            if not self.disable_median_scaling:\n",
    "                ratio = np.median(gt_depth) / np.median(pred_depth)\n",
    "                ratios.append(ratio)\n",
    "                pred_depth *= ratio\n",
    "\n",
    "            pred_depth[pred_depth < self.MIN_DEPTH] = self.MIN_DEPTH\n",
    "            pred_depth[pred_depth > self.MAX_DEPTH] = self.MAX_DEPTH\n",
    "\n",
    "            errors.append(self.compute_errors(gt_depth, pred_depth))\n",
    "\n",
    "        if not self.disable_median_scaling:\n",
    "            ratios = np.array(ratios)\n",
    "            med = np.median(ratios)\n",
    "            print(\" Scaling ratios | med: {:0.3f} | std: {:0.3f}\".format(med, np.std(ratios / med)))\n",
    "\n",
    "        mean_errors = np.array(errors).mean(0)\n",
    "        print(\"[info] {}\".format(name))\n",
    "        print(\" best FPS: \", 1/time_min)\n",
    "        print(\" avg FPS: \", 1/time_avg)\n",
    "        print(\"\\n  \" + (\"{:>8} | \" * 7).format(\"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\"))\n",
    "        print((\"&{: 8.3f}  \" * 7).format(*mean_errors.tolist()) + \"\\\\\\\\\")\n",
    "        print(\"\\n-> Done!\")\n",
    "\n",
    "        \n",
    "#         flops_enc, params_enc = profile(encoder, inputs=(input_color, ))\n",
    "#         flops_dec, params_dec = profile(decoder, inputs=(*tuple(features), ))\n",
    "#         a, b, c, d, e, f = clever_format([params_enc+params_dec, \n",
    "#                                           params_enc, \n",
    "#                                           params_dec, \n",
    "#                                           flops_enc+flops_dec, \n",
    "#                                           flops_enc, \n",
    "#                                           flops_dec], \"%.3f\")\n",
    "\n",
    "        result = []\n",
    "        result.append(name)\n",
    "        result.append(256)\n",
    "        result.append(832)\n",
    "        for i in mean_errors:\n",
    "            result.append(i)\n",
    "        result.append(1/time_min)\n",
    "        result.append(1/time_avg)\n",
    "#         for i in [a, b, c, d, e, f]:\n",
    "#             result.append(i)\n",
    "    \n",
    "        return result\n",
    "        \n",
    "\n",
    "    \n",
    "    def evaluate_depth(self, net):\n",
    "        name = net.get_name()\n",
    "        size = net.get_size()\n",
    "        dataLoader = self.get_dataLoader(size[0], size[1], batch_size=1)\n",
    "        print(\"[info] Model {}\".format(name))\n",
    "        if \"trt16\" in name.split(\"_\"):\n",
    "            trt = 16 \n",
    "        elif  \"trt32\" in name.split(\"_\"):\n",
    "            trt = 32 \n",
    "        else:\n",
    "            trt = None\n",
    "\n",
    "        if trt:\n",
    "            model = net.get_net().to(self.device)\n",
    "#             x = torch.ones((1, 3, 256, 832)).cuda()\n",
    "#             model = torch2trt(model, [x], keep_network=True)\n",
    "        else:\n",
    "            model = net.get_net().eval().to(self.device)\n",
    "\n",
    "        \n",
    "        \n",
    "        pred_disps = []\n",
    "\n",
    "        print(\"[info] Computing predictions with size {}x{}\".format(\n",
    "            size[0], size[1]))\n",
    "        \n",
    "        time_min = float('inf')\n",
    "        time_avg = 0\n",
    "        avg_FPS = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(10): #跑十次算FPS\n",
    "                for data in dataLoader:\n",
    "                \n",
    "\n",
    "                    input_color = data[(\"color\", 0, 0)].cuda()\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "                    output = model(input_color)\n",
    "                    total_time = time.time() - start_time\n",
    "#                     pred_disp, _ = disp_to_depth(output[(\"disp\", 0)], self.MIN_DEPTH, self.MAX_DEPTH)\n",
    "                    pred_disp, _ = disp_to_depth(output[0], self.MIN_DEPTH, self.MAX_DEPTH)\n",
    "                    pred_disp = pred_disp[:, 0].cpu().numpy()\n",
    "#                     pred_disp = pred_disp[:, 0].numpy()\n",
    "                    if i == 0:\n",
    "                        pred_disps.append(pred_disp)\n",
    "                    time_avg += total_time\n",
    "                    if total_time < time_min:\n",
    "                        time_min = total_time\n",
    "                \n",
    "                time_avg /= len(dataLoader)\n",
    "                avg_FPS += time_avg\n",
    "            time_avg = avg_FPS / 10\n",
    "            \n",
    "        pred_disps = np.concatenate(pred_disps)\n",
    "        \n",
    "        return pred_disps, time_min, time_avg\n",
    "    \n",
    "    def evaluate_onnx_depth(self, name, fp16_mode=True):\n",
    "        dataLoader = self.get_dataLoader(256, 832)\n",
    "        print(\"[info] Model {}\".format(name))\n",
    "        \n",
    "\n",
    "        pred_disps = []\n",
    "\n",
    "        print(\"[info] Computing predictions with size {}x{}\".format(\n",
    "            256, 832))\n",
    "        \n",
    "        \n",
    "        \n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\", \n",
    "                                  name+\"_256x832\", \n",
    "                                  \"models\", \n",
    "                                  \"weights_19\", \n",
    "                                  name+\".onnx\")\n",
    "        if fp16_mode:\n",
    "            engine_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\",\n",
    "                                      \"trt16_models\",\n",
    "                                      name+\".trt\")\n",
    "        else:\n",
    "            engine_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\",\n",
    "                                      \"trt_models\",\n",
    "                                      name+\".trt\")\n",
    "        #engine\n",
    "        engine = get_engine(fp16_mode=False, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=False)\n",
    "        # Create the context for this engine\n",
    "        context = engine.create_execution_context()\n",
    "        # Allocate buffers for input and output\n",
    "        inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "        time_min = float('inf')\n",
    "        time_avg = 0\n",
    "        avg_FPS = 0\n",
    "        \n",
    "        for data in dataLoader:\n",
    "            input_images = data[(\"color\", 0, 0)].numpy()\n",
    "            batch_pred_disp = []\n",
    "            total_time = 0\n",
    "            for input_image in input_images:\n",
    "                input_image = np.expand_dims(input_image, axis=0).reshape(-1)\n",
    "                inputs[0].host = input_image\n",
    "                start_time = time.time()\n",
    "                trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "                end_time = time.time() - start_time\n",
    "                total_time += end_time\n",
    "                pred_disp, _ = disp_to_depth(trt_outputs[-1], self.MIN_DEPTH, self.MAX_DEPTH)\n",
    "                pred_disp = pred_disp.reshape(1, 256, 832)\n",
    "                batch_pred_disp.append(pred_disp)\n",
    "                \n",
    "                if end_time < time_min:\n",
    "                    time_min = end_time\n",
    "            total_time /= len(input_images)\n",
    "            time_avg += total_time\n",
    "                \n",
    "            pred_disps.append(np.concatenate(batch_pred_disp, axis=0))\n",
    "            \n",
    "        time_avg /= len(dataLoader)\n",
    "       \n",
    "        \n",
    "        \n",
    "        pred_disps = np.concatenate(pred_disps)\n",
    "        \n",
    "        return pred_disps, time_min, time_avg\n",
    "        \n",
    "\n",
    "    \n",
    " \n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "    def inference_depth(self, column=2, is_torch2trt=False, is_onnx=False):\n",
    "        lines = readlines(os.path.join(self.split_folder, \"test_files.txt\"))\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net(is_torch2trt)\n",
    "        with torch.no_grad():\n",
    "            for i in np.random.choice(len(lines), 10, replace=False):\n",
    "                folder, frame_id, side = lines[i].split()\n",
    "                frame_id = int(frame_id)  \n",
    "                image_path = os.path.join(self.data_path, folder, \n",
    "                                          \"image_0{}\".format(self.side_map[side]), \n",
    "                                          \"data\", \n",
    "                                          \"{:010d}.jpg\".format(frame_id))\n",
    "                input_image = pil.open(image_path).convert('RGB')\n",
    "                original_width, original_height = input_image.size\n",
    "                \n",
    "                \n",
    "                result = OrderedDict()\n",
    "                result[\"Input\"] = input_image\n",
    "#                 result[\"Mask\"] = self.seg_img(input_image)\n",
    "                \n",
    "                for net in self.nets:\n",
    "                    size = net.get_size()\n",
    "                    model = net.get_net()\n",
    "                    model.eval()\n",
    "                    model.to(self.device)\n",
    "                    \n",
    "                    input_image = pil.open(image_path).convert('RGB')\n",
    "                    input_image_resized = input_image.resize(net.get_size(), pil.LANCZOS)\n",
    "                    input_image_torch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "                    input_image_torch = input_image_torch.to(self.device)\n",
    "                    \n",
    "                    outputs = model(input_image_torch)\n",
    "                    \n",
    "                    \n",
    "                    disp = outputs[(\"disp\", 0)]\n",
    "                    disp_resized = torch.nn.functional.interpolate(\n",
    "                            disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "                    disp_resized_np = disp_resized.squeeze().cpu().detach().numpy()\n",
    "                    result[\"{}\".format(net.get_name())] = disp_resized_np\n",
    "                    \n",
    "                    if is_onnx:\n",
    "                        name = net.get_name()\n",
    "                        onnx_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\", \n",
    "                                      name+\"_256x832\", \n",
    "                                      \"models\", \n",
    "                                      \"weights_19\", \n",
    "                                      name+\".onnx\")\n",
    "                        engine16_path = os.path.join(\"/work\", \n",
    "                                                  \"garin0115\", \n",
    "                                                  \"models\",\n",
    "                                                  \"trt16_models\",\n",
    "                                                  name+\".trt\")\n",
    "\n",
    "                        engine_path = os.path.join(\"/work\", \n",
    "                                                  \"garin0115\", \n",
    "                                                  \"models\",\n",
    "                                                  \"trt_models\",\n",
    "                                                  name+\".trt\")\n",
    "                        \n",
    "                        input_image = input_image.resize((832, 256), pil.LANCZOS)\n",
    "                        input_image = np.array(input_image).transpose((2, 0, 1)).astype(np.float32) / 255.\n",
    "                        print(input_image.shape)\n",
    "                        input_image = np.expand_dims(input_image, axis=0).reshape(-1)\n",
    "\n",
    "\n",
    "                        #engine16\n",
    "                        engine16 = get_engine(fp16_mode=True, onnx_file_path=onnx_path, engine_file_path=engine16_path, save_engine=False)\n",
    "                        # Create the context for this engine\n",
    "                        context16 = engine16.create_execution_context()\n",
    "                        # Allocate buffers for input and output\n",
    "                        inputs16, outputs16, bindings16, stream16 = allocate_buffers(engine16)\n",
    "                        inputs16[0].host = input_image\n",
    "                        trt_outputs16 = do_inference(context16, bindings=bindings16, inputs=inputs16, outputs=outputs16, stream=stream16) # numpy data\n",
    "\n",
    "                        #engine\n",
    "                        engine = get_engine(fp16_mode=False, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=False)\n",
    "                        # Create the context for this engine\n",
    "                        context = engine.create_execution_context()\n",
    "                        # Allocate buffers for input and output\n",
    "                        inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "                        inputs[0].host = input_image\n",
    "                        trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) # numpy data\n",
    "\n",
    "                        result[\"{}\".format(name+\"_trt32\")] = trt_outputs[-1].reshape(size)\n",
    "                        result[\"{}\".format(name+\"_trt16\")] = trt_outputs16[-1].reshape(size)\n",
    "                \n",
    "                self.quick_show(result, column=column)\n",
    "    \n",
    "    def inference_segment_sky(self):\n",
    "        result = OrderedDict()\n",
    "        lines = readlines(os.path.join(self.split_folder, \"test_files.txt\"))\n",
    "        for i in np.random.choice(len(lines), 10, replace=False):\n",
    "            folder, frame_id, side = lines[i].split()\n",
    "            frame_id = int(frame_id)  \n",
    "            image_path = os.path.join(self.data_path, folder, \n",
    "                                      \"image_0{}\".format(self.side_map[side]), \n",
    "                                      \"data\", \n",
    "                                      \"{:010d}.jpg\".format(frame_id))\n",
    "            input_image = pil.open(image_path).convert('RGB')\n",
    "            \n",
    "            result[\"Input_{}\".format(i)] = input_image\n",
    "            result[\"Mask_{}\".format(i)] = self.seg_img(input_image)\n",
    "            \n",
    "        self.quick_show(result, column=4)\n",
    "            \n",
    "    \n",
    "    def seg_img(self, image):\n",
    "        image = cv2.cvtColor(np.asarray(image),cv2.COLOR_RGB2BGR) \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "        thresh_dilation = cv2.dilate(thresh, kernel, anchor=(-1,-1), iterations=8)\n",
    "\n",
    "        edges = cv2.Canny(gray, 1, 100)\n",
    "        edges_dilation = cv2.dilate(edges, kernel, anchor=(-1,-1), iterations=8)\n",
    "\n",
    "        mask = thresh_dilation | edges_dilation\n",
    "        mask_dilation = cv2.dilate(mask, kernel, anchor=(-1,-1), iterations=8)\n",
    "        segImg = 255 - mask_dilation \n",
    "        segImg[image.shape[0]//3:, :] = 0\n",
    "            \n",
    "        return segImg\n",
    "                \n",
    "    def evaluate_pose(self):\n",
    "        pass            \n",
    "    \n",
    "    def inference_pose(self):\n",
    "        pass\n",
    "    \n",
    "    def quick_show(self, result, column=2):\n",
    "        row = len(result) // column\n",
    "        if len(result) % column > 0:\n",
    "            row += 1\n",
    "        plt.figure(figsize=(column*3*3, row*1*3+1))\n",
    "        for idx, key in enumerate(result):\n",
    "            \n",
    "            if key.split(\"_\")[0] == \"Input\":\n",
    "                plt.subplot(row, column, idx+1)\n",
    "                plt.imshow(result[key])\n",
    "                plt.title(key, fontsize=22)\n",
    "                continue\n",
    "                \n",
    "            plt.subplot(row, column, idx+1)\n",
    "            if key.split(\"_\")[0] == \"Mask\":\n",
    "                plt.imshow(result[key], cmap=\"gray\")\n",
    "            else:\n",
    "                vmax = np.percentile(result[key], 95)\n",
    "                plt.imshow(result[key], cmap=self.CMAP, vmax=vmax)\n",
    "                \n",
    "            if key == \"resnet18_simplify2my3\":\n",
    "                plt.title(key, fontsize=22, color=\"red\")\n",
    "            elif key == \"resnet18_skip2Conv\":\n",
    "                plt.title(key, fontsize=22, color=\"blue\")\n",
    "            else:\n",
    "                plt.title(key, fontsize=22)\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0.5, w_pad=0.1, h_pad=0.1)\n",
    "        \n",
    "    def make_grid(self, result, column=2):\n",
    "        pass\n",
    "    \n",
    "    def make_vedio(self, file_name, video_output_folder, column=2):\n",
    "        # 取得資料夾中所有影像檔案路徑\n",
    "        kitti_depth_folder = '/work/garin0115/datasets/kitti_data/'+file_name+'/image_02'\n",
    "        filenames = glob.glob(kitti_depth_folder+'/*/*.jpg')\n",
    "\n",
    "        # 將檔案路徑排序\n",
    "        filenames.sort()\n",
    "        num_images = len(filenames)\n",
    "        print(\"Total images: {}\".format(num_images))\n",
    "        \n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "    \n",
    "        num_model = len(self.nets)\n",
    "        num_column = column\n",
    "        num_row = num_model // column if num_model % column == 0 else num_model // column + 1\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(video_output_folder+\"/disp_{}.avi\".format(file_name.split('/')[-1]), fourcc, 15.0, (original_width*num_column, original_height*num_row))\n",
    "        \n",
    "        \n",
    "        for idx in range(num_images):\n",
    "            res = []\n",
    "            input_image = pil.open(img).convert('RGB')\n",
    "            input_image = np.array(input_image)\n",
    "            cv2.putText(input_image, \"Input\", (10, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            res.append(input_image[:, :, ::-1])\n",
    "\n",
    "            for net in self.nets:\n",
    "                name = net.get_name()\n",
    "                disp = show[name][idx]\n",
    "                vmax = np.percentile(disp, 95)\n",
    "                normalizer = mpl.colors.Normalize(vmin=disp.min(), vmax=vmax)\n",
    "                mapper = cm.ScalarMappable(norm=normalizer, cmap=CMAP)\n",
    "                colormapped_im = (mapper.to_rgba(disp)[:, :, :3] * 255).astype(np.uint8)\n",
    "                cv2.putText(colormapped_im, name, (10, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(colormapped_im, \"FPS \"+show[\"{}_FPS\".format(name)][idx], (10, 100), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                im = pil.fromarray(colormapped_im[:, :, ::-1])\n",
    "                res.append(im)\n",
    "            \n",
    "            \n",
    "            result = []\n",
    "            for i in range(num_row):\n",
    "                result.append(np.hstack(res[num_row * num_column: (num_row+1) * num_column]))\n",
    "            result = np.vstack(result)\n",
    "\n",
    "\n",
    "            out.write(result)\n",
    "        out.release()    \n",
    "        \n",
    "        \n",
    "    \n",
    "    def calc_param(self, net):\n",
    "        net_params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "        weight_count = 0\n",
    "        for param in net_params:\n",
    "            weight_count += np.prod(param.size())\n",
    "        return weight_count\n",
    "    \n",
    "    def compute_errors(self, gt, pred):\n",
    "        \"\"\"Computation of error metrics between predicted and ground truth depths\n",
    "        \"\"\"\n",
    "        thresh = np.maximum((gt / pred), (pred / gt))\n",
    "        a1 = (thresh < 1.25     ).mean()\n",
    "        a2 = (thresh < 1.25 ** 2).mean()\n",
    "        a3 = (thresh < 1.25 ** 3).mean()\n",
    "\n",
    "        rmse = (gt - pred) ** 2\n",
    "        rmse = np.sqrt(rmse.mean())\n",
    "\n",
    "        rmse_log = (np.log(gt) - np.log(pred)) ** 2\n",
    "        rmse_log = np.sqrt(rmse_log.mean())\n",
    "\n",
    "        abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "\n",
    "        sq_rel = np.mean(((gt - pred) ** 2) / gt)\n",
    "\n",
    "        return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n",
    "    \n",
    "    def batch_post_process_disparity(self, l_disp, r_disp):\n",
    "        \"\"\"Apply the disparity post-processing method as introduced in Monodepthv1\n",
    "        \"\"\"\n",
    "        _, h, w = l_disp.shape\n",
    "        m_disp = 0.5 * (l_disp + r_disp)\n",
    "        l, _ = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
    "        l_mask = (1.0 - np.clip(20 * (l - 0.05), 0, 1))[None, ...]\n",
    "        r_mask = l_mask[:, :, ::-1]\n",
    "        return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n",
    "    \n",
    "    #save to ONNX model\n",
    "    def save_ONNX(self):\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "            \n",
    "        for net in self.nets:\n",
    "            net_name = net.get_name()\n",
    "            net_height = net.get_height()\n",
    "            net_width = net.get_width()\n",
    "            path = self.get_modelPath(net_name)\n",
    "            \n",
    "            if not os.path.isfile(path+\"/\"+net_name+\".onnx\"):\n",
    "                encoder = net.get_encoder()\n",
    "                decoder = net.get_decoder()\n",
    "                depth_model = Depth(encoder, decoder, output_list=True)\n",
    "                depth_model.to(self.device)\n",
    "                depth_model.eval()\n",
    "                x = torch.randn(1, 3, net_height, net_width, requires_grad=True).to(self.device)\n",
    "                \n",
    "                # Export the model\n",
    "                torch.onnx.export(depth_model,               # model being run\n",
    "                                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                                  path+\"/\"+net.get_name()+\".onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                                  opset_version=11,          # the ONNX version to export the model to\n",
    "                                  verbose=True,\n",
    "                                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                                  input_names = ['input'],   # the model's input names\n",
    "                                  output_names = ['output']) # the model's output names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model name list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = [\n",
    "    #3種 model\n",
    "    \"resnet18_mono\", # have trained\n",
    "    \"resnet18_mono_trt32\", # have trained\n",
    "    \"resnet18_mono_trt16\", # have trained\n",
    "    \n",
    "    \"resnet50_ys\", # have trained\n",
    "    \"resnet50_ys_trt32\", # have trained\n",
    "    \"resnet50_ys_trt16\", # have trained\n",
    "    \n",
    "    \"mobilenetv2_mj\", # have trained\n",
    "    \"mobilenetv2_mj_trt32\", # have trained\n",
    "    \"mobilenetv2_mj_trt16\", # have trained\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #判斷 encoder 好壞\n",
    "    \"resnet18_ys\", # have trained\n",
    "    \"resnet18_ys_trt32\", # have trained\n",
    "    \"resnet18_ys_trt16\", # have trained\n",
    "    \"mobilenetv2_ys\", # have trained\n",
    "    \"mobilenetv2_ys_trt32\", # have trained\n",
    "    \"mobilenetv2_ys_trt16\", # have trained\n",
    "    \n",
    "    #判斷 decoder 好壞\n",
    "    \"resnet18_mj\", # have trained\n",
    "    \"resnet18_mj_trt32\", # have trained\n",
    "    \"resnet18_mj_trt16\", # have trained\n",
    "    \n",
    "    #改進天空訓練\n",
    "    \"resnet18_ours_skipSky\", #have trained\n",
    "    \"resnet18_ours_skipSky_trt32\", #have trained\n",
    "    \"resnet18_ours_skipSky_trt16\", #have trained\n",
    "    \"resnet18_ours_skyLoss\",\n",
    "    \"resnet18_ours_skyLoss_trt32\",\n",
    "    \"resnet18_ours_skyLoss_trt16\",\n",
    "    \"resnet18_ours_skipSky_skyLoss\",\n",
    "    \"resnet18_ours_skipSky_skyLoss_trt32\",\n",
    "    \"resnet18_ours_skipSky_skyLoss_trt16\",\n",
    "    \n",
    "    #簡化 ours decoder\n",
    "    \"resnet18_ours\", #應該跟resnet18_momo差不多 have trained\n",
    "    \"resnet18_ours_trt32\", #應該跟resnet18_momo差不多 have trained\n",
    "    \"resnet18_ours_trt16\", #應該跟resnet18_momo差不多 have trained\n",
    "    \"resnet18_ours_bn\", # have trained\n",
    "    \"resnet18_ours_bn_trt32\", # have trained\n",
    "    \"resnet18_ours_bn_trt16\", # have trained\n",
    "    \"resnet18_ours_oneLayer\", # have trained\n",
    "    \"resnet18_ours_oneLayer_trt32\", # have trained\n",
    "    \"resnet18_ours_oneLayer_trt16\", # have trained\n",
    "    \"resnet18_ours_dw\", # have trained\n",
    "    \"resnet18_ours_dw_trt32\", # have trained\n",
    "    \"resnet18_ours_dw_trt16\", # have trained\n",
    "    \"resnet18_ours_pw\", # have trained\n",
    "    \"resnet18_ours_pw_trt32\", # have trained\n",
    "    \"resnet18_ours_pw_trt16\", # have trained\n",
    "    \"resnet18_ours_skipAdd\", # have trained\n",
    "    \"resnet18_ours_skipAdd_trt32\", # have trained\n",
    "    \"resnet18_ours_skipAdd_trt16\", # have trained\n",
    "    \n",
    "\n",
    "    #其他訓練\n",
    "    \"resnet18_ours_oneLayer_pw\", #have trained\n",
    "    \"resnet18_ours_oneLayer_pw_trt32\", #have trained\n",
    "    \"resnet18_ours_oneLayer_pw_trt16\", #have trained\n",
    "    \"resnet18_ours_oneLayer_dw_pw\", #have trained\n",
    "    \"resnet18_ours_oneLayer_dw_pw_trt32\", #have trained\n",
    "    \"resnet18_ours_oneLayer_dw_pw_trt16\", #have trained\n",
    "    \"resnet18_ours_bn_dw\", #看dw有沒有做bn的影響 #have trained\n",
    "    \"resnet18_ours_bn_dw_trt32\", #看dw有沒有做bn的影響 #have trained\n",
    "    \"resnet18_ours_bn_dw_trt16\", #看dw有沒有做bn的影響 #have trained\n",
    "    \"mobilenet_ours\", #have trained\n",
    "    \"mobilenet_ours_trt32\", #have trained\n",
    "    \"mobilenet_ours_trt16\", #have trained\n",
    "    \"mobilenetv3_ours\", #have trained\n",
    "    \"mobilenetv3_ours_trt32\", #have trained\n",
    "    \"mobilenetv3_ours_trt16\", #have trained\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = Model(model_name)\n",
    "cv2.setNumThreads(0)  # This speeds up evaluation 5x on our unix systems (OpenCV 3.3.1)\n",
    "\n",
    "\n",
    "# Models.save_ONNX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Deal with resnet18_mono model\n",
      "\n",
      "[info] Loading weights from /work/garin0115/models/resnet18_mono_256x832/models\n",
      "[Info] Deal with resnet18_mono_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_mono_trt16 model\n",
      "\n",
      "[Info] Deal with resnet50_ys model\n",
      "\n",
      "[info] Loading weights from /work/garin0115/models/resnet50_ys_256x832/models\n",
      "[Info] Deal with resnet50_ys_trt32 model\n",
      "\n",
      "[Info] Deal with resnet50_ys_trt16 model\n",
      "\n",
      "[Info] Deal with mobilenetv2_mj model\n",
      "\n",
      "[info] Loading weights from /work/garin0115/models/mobilenetv2_mj_256x832/models\n",
      "[Info] Deal with mobilenetv2_mj_trt32 model\n",
      "\n",
      "[Info] Deal with mobilenetv2_mj_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ys model\n",
      "\n",
      "[info] Loading weights from /work/garin0115/models/resnet18_ys_256x832/models\n",
      "[Info] Deal with resnet18_ys_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ys_trt16 model\n",
      "\n",
      "[Info] Deal with mobilenetv2_ys model\n",
      "\n",
      "[info] Loading weights from /work/garin0115/models/mobilenetv2_ys_256x832/models\n",
      "[Info] Deal with mobilenetv2_ys_trt32 model\n",
      "\n",
      "[Info] Deal with mobilenetv2_ys_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_mj model\n",
      "\n",
      "[info] Loading weights from /work/garin0115/models/resnet18_mj_256x832/models\n",
      "[Info] Deal with resnet18_mj_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_mj_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_skipSky model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_skipSky_256x832/models\n",
      "[Info] Deal with resnet18_ours_skipSky_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_skipSky_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_skyLoss model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_skyLoss_256x832/models\n",
      "[Info] Deal with resnet18_ours_skyLoss_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_skyLoss_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_skipSky_skyLoss model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_skipSky_skyLoss_256x832/models\n",
      "[Info] Deal with resnet18_ours_skipSky_skyLoss_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_skipSky_skyLoss_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_256x832/models\n",
      "[Info] Deal with resnet18_ours_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_bn model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_bn_256x832/models\n",
      "[Info] Deal with resnet18_ours_bn_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_bn_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_oneLayer model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_oneLayer_256x832/models\n",
      "[Info] Deal with resnet18_ours_oneLayer_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_oneLayer_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_dw model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_dw_256x832/models\n",
      "[Info] Deal with resnet18_ours_dw_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_dw_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_pw model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_pw_256x832/models\n",
      "[Info] Deal with resnet18_ours_pw_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_pw_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_skipAdd model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_skipAdd_256x832/models\n",
      "[Info] Deal with resnet18_ours_skipAdd_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_skipAdd_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_oneLayer_pw model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_oneLayer_pw_256x832/models\n",
      "[Info] Deal with resnet18_ours_oneLayer_pw_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_oneLayer_pw_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_oneLayer_dw_pw model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_oneLayer_dw_pw_256x832/models\n",
      "[Info] Deal with resnet18_ours_oneLayer_dw_pw_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_oneLayer_dw_pw_trt16 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_bn_dw model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/resnet18_ours_bn_dw_256x832/models\n",
      "[Info] Deal with resnet18_ours_bn_dw_trt32 model\n",
      "\n",
      "[Info] Deal with resnet18_ours_bn_dw_trt16 model\n",
      "\n",
      "[Info] Deal with mobilenet_ours model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/mobilenet_ours_256x832/models\n",
      "[Info] Deal with mobilenet_ours_trt32 model\n",
      "\n",
      "[Info] Deal with mobilenet_ours_trt16 model\n",
      "\n",
      "[Info] Deal with mobilenetv3_ours model\n",
      "\n",
      "[info] Loading weights from /home/garin0115/depth/monodepth2/models/mobilenetv3_ours_256x832/models\n",
      "[Info] Deal with mobilenetv3_ours_trt32 model\n",
      "\n",
      "[Info] Deal with mobilenetv3_ours_trt16 model\n",
      "\n",
      "[info] Model resnet18_mono\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3030.673 | std: 0.083\n",
      "[info] resnet18_mono\n",
      " best FPS:  253.98474022041904\n",
      " avg FPS:  156.2906691972314\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.905  &   4.806  &   0.192  &   0.880  &   0.961  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_mono_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3030.673 | std: 0.083\n",
      "[info] resnet18_mono_trt32\n",
      " best FPS:  1933.7501152604887\n",
      " avg FPS:  1050.7313970038044\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.905  &   4.806  &   0.192  &   0.880  &   0.961  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_mono_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3031.451 | std: 0.083\n",
      "[info] resnet18_mono_trt16\n",
      " best FPS:  1830.7743343518114\n",
      " avg FPS:  1039.7873903607683\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.904  &   4.804  &   0.192  &   0.880  &   0.961  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet50_ys\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2831.921 | std: 0.078\n",
      "[info] resnet50_ys\n",
      " best FPS:  144.02032757614256\n",
      " avg FPS:  92.13732604938818\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.111  &   0.914  &   4.772  &   0.188  &   0.888  &   0.962  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet50_ys_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2831.921 | std: 0.078\n",
      "[info] resnet50_ys_trt32\n",
      " best FPS:  1450.8142511241786\n",
      " avg FPS:  845.5431010644827\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.111  &   0.914  &   4.772  &   0.188  &   0.888  &   0.962  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet50_ys_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2832.499 | std: 0.078\n",
      "[info] resnet50_ys_trt16\n",
      " best FPS:  1287.3861264579496\n",
      " avg FPS:  816.606341824253\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.111  &   0.915  &   4.774  &   0.188  &   0.888  &   0.962  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv2_mj\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 5160.142 | std: 0.175\n",
      "[info] mobilenetv2_mj\n",
      " best FPS:  127.3664328444323\n",
      " avg FPS:  85.12935034377283\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.455  &   5.025  &  12.324  &   0.616  &   0.293  &   0.548  &   0.748  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv2_mj_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 5160.142 | std: 0.175\n",
      "[info] mobilenetv2_mj_trt32\n",
      " best FPS:  1543.158204562178\n",
      " avg FPS:  900.6270227390107\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.455  &   5.025  &  12.324  &   0.616  &   0.293  &   0.548  &   0.748  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv2_mj_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 5160.094 | std: 0.175\n",
      "[info] mobilenetv2_mj_trt16\n",
      " best FPS:  1716.163666121113\n",
      " avg FPS:  1006.6594129999977\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.455  &   5.025  &  12.324  &   0.616  &   0.293  &   0.548  &   0.748  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ys\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2997.763 | std: 0.083\n",
      "[info] resnet18_ys\n",
      " best FPS:  238.58384527872582\n",
      " avg FPS:  150.18574069438503\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.947  &   4.843  &   0.194  &   0.879  &   0.960  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ys_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2997.763 | std: 0.083\n",
      "[info] resnet18_ys_trt32\n",
      " best FPS:  1711.2623419012648\n",
      " avg FPS:  984.0173440320133\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.947  &   4.843  &   0.194  &   0.879  &   0.960  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ys_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2998.046 | std: 0.083\n",
      "[info] resnet18_ys_trt16\n",
      " best FPS:  1577.992475545523\n",
      " avg FPS:  943.2726449666113\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.948  &   4.844  &   0.194  &   0.879  &   0.960  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv2_ys\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3731.796 | std: 0.099\n",
      "[info] mobilenetv2_ys\n",
      " best FPS:  151.85199666920096\n",
      " avg FPS:  94.26689614565257\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.146  &   1.248  &   5.508  &   0.224  &   0.824  &   0.939  &   0.973  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv2_ys_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3731.795 | std: 0.099\n",
      "[info] mobilenetv2_ys_trt32\n",
      " best FPS:  1271.7719830200122\n",
      " avg FPS:  797.4961078043367\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.146  &   1.248  &   5.508  &   0.224  &   0.824  &   0.939  &   0.973  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv2_ys_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3731.535 | std: 0.099\n",
      "[info] mobilenetv2_ys_trt16\n",
      " best FPS:  1374.7309078990495\n",
      " avg FPS:  847.8192281921928\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.146  &   1.248  &   5.506  &   0.224  &   0.824  &   0.939  &   0.973  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_mj\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4529.043 | std: 0.167\n",
      "[info] resnet18_mj\n",
      " best FPS:  181.9575723395948\n",
      " avg FPS:  110.11824521552815\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.441  &   4.788  &  12.147  &   0.593  &   0.303  &   0.561  &   0.762  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_mj_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4529.043 | std: 0.167\n",
      "[info] resnet18_mj_trt32\n",
      " best FPS:  2143.231476750128\n",
      " avg FPS:  1142.6894740812404\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.441  &   4.788  &  12.147  &   0.593  &   0.303  &   0.561  &   0.762  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_mj_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4529.258 | std: 0.167\n",
      "[info] resnet18_mj_trt16\n",
      " best FPS:  2021.3513253012047\n",
      " avg FPS:  1141.768126903834\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.441  &   4.788  &  12.147  &   0.593  &   0.303  &   0.561  &   0.762  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipSky\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3119.956 | std: 0.086\n",
      "[info] resnet18_ours_skipSky\n",
      " best FPS:  313.3351262513073\n",
      " avg FPS:  183.67836142157353\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.844  &   4.796  &   0.190  &   0.875  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipSky_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3119.956 | std: 0.086\n",
      "[info] resnet18_ours_skipSky_trt32\n",
      " best FPS:  1940.9088385006942\n",
      " avg FPS:  1071.9649557394416\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.844  &   4.796  &   0.190  &   0.875  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipSky_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3119.488 | std: 0.087\n",
      "[info] resnet18_ours_skipSky_trt16\n",
      " best FPS:  1856.7082779991147\n",
      " avg FPS:  1074.2451928329447\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.844  &   4.796  &   0.190  &   0.875  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skyLoss\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3040.428 | std: 0.083\n",
      "[info] resnet18_ours_skyLoss\n",
      " best FPS:  313.14797670598773\n",
      " avg FPS:  185.25055248214292\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.877  &   4.854  &   0.191  &   0.871  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skyLoss_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3040.429 | std: 0.083\n",
      "[info] resnet18_ours_skyLoss_trt32\n",
      " best FPS:  1883.3875168387965\n",
      " avg FPS:  1035.986336295006\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.877  &   4.854  &   0.191  &   0.871  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skyLoss_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3040.962 | std: 0.083\n",
      "[info] resnet18_ours_skyLoss_trt16\n",
      " best FPS:  1855.8867256637168\n",
      " avg FPS:  1074.8925256640882\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.876  &   4.853  &   0.191  &   0.871  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipSky_skyLoss\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3110.854 | std: 0.082\n",
      "[info] resnet18_ours_skipSky_skyLoss\n",
      " best FPS:  315.31378740039094\n",
      " avg FPS:  183.24752053596043\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.821  &   4.743  &   0.189  &   0.876  &   0.960  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipSky_skyLoss_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3110.853 | std: 0.082\n",
      "[info] resnet18_ours_skipSky_skyLoss_trt32\n",
      " best FPS:  1922.2291475710358\n",
      " avg FPS:  1078.0682597821622\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.821  &   4.743  &   0.189  &   0.876  &   0.960  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipSky_skyLoss_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3109.980 | std: 0.082\n",
      "[info] resnet18_ours_skipSky_skyLoss_trt16\n",
      " best FPS:  1845.2723273207216\n",
      " avg FPS:  1070.079936531757\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.820  &   4.740  &   0.189  &   0.876  &   0.960  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3021.449 | std: 0.084\n",
      "[info] resnet18_ours\n",
      " best FPS:  320.1758778625954\n",
      " avg FPS:  186.7495864646251\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.827  &   4.777  &   0.189  &   0.874  &   0.960  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3021.449 | std: 0.084\n",
      "[info] resnet18_ours_trt32\n",
      " best FPS:  1909.9744990892532\n",
      " avg FPS:  1063.9583800965097\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.827  &   4.777  &   0.189  &   0.874  &   0.960  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3021.953 | std: 0.084\n",
      "[info] resnet18_ours_trt16\n",
      " best FPS:  1856.7082779991147\n",
      " avg FPS:  1075.7823032142169\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.826  &   4.775  &   0.189  &   0.874  &   0.960  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_bn\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4176.083 | std: 0.086\n",
      "[info] resnet18_ours_bn\n",
      " best FPS:  280.5742190113051\n",
      " avg FPS:  171.30898211832726\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.835  &   4.791  &   0.188  &   0.871  &   0.959  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_bn_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4176.082 | std: 0.086\n",
      "[info] resnet18_ours_bn_trt32\n",
      " best FPS:  1930.1905200184078\n",
      " avg FPS:  1062.444990633682\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.835  &   4.791  &   0.188  &   0.871  &   0.959  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_bn_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4176.227 | std: 0.086\n",
      "[info] resnet18_ours_bn_trt16\n",
      " best FPS:  1884.2336028751124\n",
      " avg FPS:  1081.9566396962384\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.114  &   0.835  &   4.790  &   0.188  &   0.871  &   0.959  &   0.983  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3069.431 | std: 0.083\n",
      "[info] resnet18_ours_oneLayer\n",
      " best FPS:  343.6827269747624\n",
      " avg FPS:  200.9507416495797\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.116  &   0.856  &   4.831  &   0.192  &   0.872  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3069.430 | std: 0.083\n",
      "[info] resnet18_ours_oneLayer_trt32\n",
      " best FPS:  2063.110673880964\n",
      " avg FPS:  1108.0081983812925\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.116  &   0.856  &   4.831  &   0.192  &   0.872  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3069.422 | std: 0.083\n",
      "[info] resnet18_ours_oneLayer_trt16\n",
      " best FPS:  1874.968261063925\n",
      " avg FPS:  1087.37196678243\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.116  &   0.856  &   4.831  &   0.192  &   0.872  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_dw\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3101.755 | std: 0.084\n",
      "[info] resnet18_ours_dw\n",
      " best FPS:  279.37813894624657\n",
      " avg FPS:  166.3582381593111\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.116  &   0.854  &   4.856  &   0.191  &   0.872  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_dw_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3101.754 | std: 0.084\n",
      "[info] resnet18_ours_dw_trt32\n",
      " best FPS:  1238.3537053439622\n",
      " avg FPS:  786.7737322407579\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.116  &   0.854  &   4.856  &   0.191  &   0.872  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_dw_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3102.296 | std: 0.084\n",
      "[info] resnet18_ours_dw_trt16\n",
      " best FPS:  1568.5504861630516\n",
      " avg FPS:  946.7563819867422\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.116  &   0.855  &   4.856  &   0.191  &   0.872  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_pw\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2823.617 | std: 0.082\n",
      "[info] resnet18_ours_pw\n",
      " best FPS:  314.9349752215047\n",
      " avg FPS:  188.4072598014477\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.868  &   4.868  &   0.191  &   0.874  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_pw_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2823.617 | std: 0.082\n",
      "[info] resnet18_ours_pw_trt32\n",
      " best FPS:  1796.2758029978586\n",
      " avg FPS:  1010.2065793718558\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.868  &   4.868  &   0.191  &   0.874  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_pw_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2824.113 | std: 0.082\n",
      "[info] resnet18_ours_pw_trt16\n",
      " best FPS:  1793.9709153122326\n",
      " avg FPS:  1035.6995978077416\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.115  &   0.869  &   4.870  &   0.191  &   0.873  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipAdd\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3223.874 | std: 0.081\n",
      "[info] resnet18_ours_skipAdd\n",
      " best FPS:  307.5228389178092\n",
      " avg FPS:  177.8904192351736\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.909  &   4.888  &   0.191  &   0.873  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipAdd_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3223.873 | std: 0.081\n",
      "[info] resnet18_ours_skipAdd_trt32\n",
      " best FPS:  2009.7287973167226\n",
      " avg FPS:  1097.5759685994815\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.909  &   4.888  &   0.191  &   0.873  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_skipAdd_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3223.730 | std: 0.081\n",
      "[info] resnet18_ours_skipAdd_trt16\n",
      " best FPS:  1896.1591320072332\n",
      " avg FPS:  1052.9452583243442\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.909  &   4.888  &   0.191  &   0.873  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer_pw\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2713.047 | std: 0.081\n",
      "[info] resnet18_ours_oneLayer_pw\n",
      " best FPS:  336.48648215002004\n",
      " avg FPS:  196.16757527307982\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.906  &   4.916  &   0.193  &   0.872  &   0.958  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer_pw_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2713.048 | std: 0.081\n",
      "[info] resnet18_ours_oneLayer_pw_trt32\n",
      " best FPS:  1929.3026678932843\n",
      " avg FPS:  1062.6660435739445\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.906  &   4.916  &   0.193  &   0.872  &   0.958  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer_pw_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2713.388 | std: 0.081\n",
      "[info] resnet18_ours_oneLayer_pw_trt16\n",
      " best FPS:  1896.1591320072332\n",
      " avg FPS:  1098.3084053532164\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.906  &   4.916  &   0.193  &   0.872  &   0.958  &   0.981  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer_dw_pw\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2855.471 | std: 0.081\n",
      "[info] resnet18_ours_oneLayer_dw_pw\n",
      " best FPS:  310.36732277638004\n",
      " avg FPS:  182.6268491875992\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.885  &   4.861  &   0.190  &   0.873  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer_dw_pw_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2855.471 | std: 0.081\n",
      "[info] resnet18_ours_oneLayer_dw_pw_trt32\n",
      " best FPS:  1695.3532740501212\n",
      " avg FPS:  992.9131124357394\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.885  &   4.861  &   0.190  &   0.873  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_oneLayer_dw_pw_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 2855.404 | std: 0.081\n",
      "[info] resnet18_ours_oneLayer_dw_pw_trt16\n",
      " best FPS:  1676.3804956035171\n",
      " avg FPS:  977.6354895048311\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.117  &   0.885  &   4.861  &   0.190  &   0.873  &   0.959  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_bn_dw\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4179.706 | std: 0.085\n",
      "[info] resnet18_ours_bn_dw\n",
      " best FPS:  232.33279787292972\n",
      " avg FPS:  145.80456014046777\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.118  &   0.878  &   4.930  &   0.192  &   0.867  &   0.958  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_bn_dw_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4179.707 | std: 0.085\n",
      "[info] resnet18_ours_bn_dw_trt32\n",
      " best FPS:  1274.4770586447887\n",
      " avg FPS:  791.6690152785922\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.118  &   0.878  &   4.930  &   0.192  &   0.867  &   0.958  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model resnet18_ours_bn_dw_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 4179.828 | std: 0.085\n",
      "[info] resnet18_ours_bn_dw_trt16\n",
      " best FPS:  1563.8717375093213\n",
      " avg FPS:  956.0629349513081\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.118  &   0.878  &   4.930  &   0.192  &   0.867  &   0.958  &   0.982  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenet_ours\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3672.065 | std: 0.097\n",
      "[info] mobilenet_ours\n",
      " best FPS:  293.6159607980399\n",
      " avg FPS:  171.3982966024024\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.147  &   1.158  &   5.465  &   0.223  &   0.815  &   0.936  &   0.974  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenet_ours_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3672.065 | std: 0.097\n",
      "[info] mobilenet_ours_trt32\n",
      " best FPS:  1971.0075187969924\n",
      " avg FPS:  1148.5316269644343\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.147  &   1.158  &   5.465  &   0.223  &   0.815  &   0.936  &   0.974  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenet_ours_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3671.723 | std: 0.097\n",
      "[info] mobilenet_ours_trt16\n",
      " best FPS:  1672.3700159489633\n",
      " avg FPS:  997.4445872441447\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.147  &   1.159  &   5.465  &   0.223  &   0.815  &   0.936  &   0.974  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv3_ours\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3823.413 | std: 0.096\n",
      "[info] mobilenetv3_ours\n",
      " best FPS:  125.90970220941402\n",
      " avg FPS:  82.09180725401507\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.150  &   1.232  &   5.529  &   0.224  &   0.813  &   0.936  &   0.973  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv3_ours_trt32\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3823.412 | std: 0.096\n",
      "[info] mobilenetv3_ours_trt32\n",
      " best FPS:  1153.5489548954895\n",
      " avg FPS:  743.0676107034134\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.150  &   1.232  &   5.529  &   0.224  &   0.813  &   0.936  &   0.973  \\\\\n",
      "\n",
      "-> Done!\n",
      "[info] Model mobilenetv3_ours_trt16\n",
      "[info] Computing predictions with size 256x832\n",
      " Scaling ratios | med: 3822.152 | std: 0.096\n",
      "[info] mobilenetv3_ours_trt16\n",
      " best FPS:  1177.513756316676\n",
      " avg FPS:  738.3309704877283\n",
      "\n",
      "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
      "&   0.150  &   1.231  &   5.528  &   0.224  &   0.813  &   0.936  &   0.973  \\\\\n",
      "\n",
      "-> Done!\n"
     ]
    }
   ],
   "source": [
    "Models.batch_evaluate_depth(save_CSV=True, is_torch2trt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.inference_depth(column=2, is_torch2trt=False, is_onnx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.inference_segment_sky()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX2TRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_mode = True\n",
    "print(\"Model Name           FPS\")\n",
    "for name in model_name:\n",
    "    if name == \"resnet18_oneLayer\":\n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                          \"garin0115\", \n",
    "                          \"models\", \n",
    "                          name+\"_256x832\", \n",
    "                          \"models\", \n",
    "                          \"weights_18\", \n",
    "                          name+\".onnx\")\n",
    "    else:\n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\", \n",
    "                                  name+\"_256x832\", \n",
    "                                  \"models\", \n",
    "                                  \"weights_19\", \n",
    "                                  name+\".onnx\")\n",
    "    if fp16_mode:\n",
    "        engine_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\",\n",
    "                                  \"trt16_models\",\n",
    "                                  name+\".trt\")\n",
    "    else:\n",
    "        engine_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\",\n",
    "                                  \"trt_models\",\n",
    "                                  name+\".trt\")\n",
    "\n",
    "    engine = get_engine(fp16_mode=fp16_mode, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=True)\n",
    "\n",
    "    # Create the context for this engine\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Allocate buffers for input and output\n",
    "    inputs, outputs, bindings, stream = allocate_buffers(engine) # input, output: host # bindings\n",
    "\n",
    "\n",
    "    # Load data to the buffer\n",
    "    image_path = \"assets/test_image.jpg\"\n",
    "    input_image = pil.open(image_path).convert('RGB').resize((832, 256), pil.LANCZOS)\n",
    "    input_image = np.array(input_image).transpose((2, 0, 1)).astype(np.float32) / 255.\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    inputs[0].host = input_image.reshape(-1)\n",
    "\n",
    "    # inputs[1].host = ... for multiple input\n",
    "    t1 = time.time()\n",
    "    for i in range(100):\n",
    "        trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) # numpy data\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print(\"{}       {}\".format(name, 100/(t2-t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇要建立 video 的 data [TODO]\n",
    "# file_name = '2011_10_03/2011_10_03_drive_0047_sync' #837\n",
    "# file_name = '2011_09_30/2011_09_30_drive_0016_sync' #279\n",
    "# file_name = '2011_09_29/2011_09_29_drive_0026_sync' #158\n",
    "# file_name = '2011_09_28/2011_09_28_drive_0037_sync' #89\n",
    "file_name = '2011_09_26/2011_09_26_drive_0036_sync' #803\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0023_sync' #474\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0020_sync' #86\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0013_sync' #144\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0002_sync' #77\n",
    "\n",
    "# 選擇影片輸出資料夾 [TODO]\n",
    "video_output_folder = os.path.join(os.path.expanduser(\"~\"), \n",
    "                                                      \"depth\",\n",
    "                                                      \"monodepth2\",\n",
    "                                                      \"video_result\")\n",
    "\n",
    "# 取得資料夾中所有影像檔案路徑\n",
    "kitti_depth_folder = '/work/garin0115/datasets/kitti_data/'+file_name+'/image_02'\n",
    "filenames = glob.glob(kitti_depth_folder+'/*/*.jpg')\n",
    "\n",
    "# 將檔案路徑排序\n",
    "filenames.sort()\n",
    "num_images = len(filenames)\n",
    "print(\"Total images: {}\".format(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 思考看看要不要實作Analyze的class去產生圖示化的結果\n",
    "2. 或是直接實作在Depth上面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyze():\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = networks.ResnetEncoder(18, False).cuda()\n",
    "summary(resnet18, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = networks.ResnetEncoder(50, False).cuda()\n",
    "summary(resnet50, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = networks.MobileNet().cuda()\n",
    "summary(mobilenet, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv2 = networks.MobileNetV2().cuda()\n",
    "summary(mobilenetv2, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv3 = networks.MobileNetV3().cuda()\n",
    "summary(mobilenetv3, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 轉 tensorRT 並儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch2trt import torch2trt\n",
    "import torch.nn as nn\n",
    "class Depth(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output2list=True): #trt必須將結果轉成list(dict會錯誤)\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output2list = output2list\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        feature = self.encoder(inputs)\n",
    "        output = self.decoder(feature)\n",
    "        output_list = []\n",
    "        if self.output2list:\n",
    "            for i in range(4):\n",
    "                output_list.append(output[(\"disp\", 3-i)])\n",
    "            output = output_list\n",
    "        return output\n",
    "    \n",
    "encoder_dict = {\n",
    "    \"resnet18\":networks.ResnetEncoder(18, False),\n",
    "    \"resnet50\":networks.ResnetEncoder(50, False),\n",
    "    \"mobilenet\":networks.MobileNet(),\n",
    "    \"mobilenetv2\":networks.MobileNetV2(),\n",
    "    \"mobilenetv3\":networks.MobileNetV3(),\n",
    "    \"shufflenetv2\":networks.ShuffleNetV2(),\n",
    "    \"peleenet\":networks.PeleeNet(),\n",
    "    \"mnasnet\":networks.MnasNet()\n",
    "}\n",
    "decoder_dict = {\n",
    "    \"mj\":networks.MJDecoder,\n",
    "    \"ys\":networks.YSDecoder,\n",
    "    \"mono\":networks.MonoDecoder,\n",
    "    \"ours\":networks.OursDecoder\n",
    "}\n",
    "\n",
    "for name in model_name:\n",
    "    print(f\"[info] Deal with {name}\")\n",
    "    name_split = name.split(\"_\")     \n",
    "    encoder = encoder_dict[name_split[0]]\n",
    "    if \"ours\" in name_split:\n",
    "        ablation = {\"bn\":False, \n",
    "                    \"oneLayer\":False, \n",
    "                    \"dw\":False, \n",
    "                    \"pw\":False,\n",
    "                    \"skipAdd\":False, \n",
    "                    \"skipInput\":False}\n",
    "        for key, abl in ablation.items():\n",
    "            if key in name_split:\n",
    "                ablation[key] = True\n",
    "        decoder = decoder_dict[name_split[1]](num_ch_enc=encoder.num_ch_enc,\n",
    "                                                   bn=ablation[\"bn\"], \n",
    "                                                   dw=ablation[\"dw\"], \n",
    "                                                   pw=ablation[\"pw\"], \n",
    "                                                   oneLayer=ablation[\"oneLayer\"], \n",
    "                                                   skipAdd=ablation[\"skipAdd\"], \n",
    "                                                   skipInput=ablation[\"skipInput\"])\n",
    "    else:\n",
    "        decoder = decoder_dict[name_split[1]](num_ch_enc=encoder.num_ch_enc)\n",
    "    \n",
    "\n",
    "    path = os.path.join(os.path.expanduser(\"~\"), \"depth\", \"monodepth2\",\"models\", name+\"_256x832\", \"models\", \"weights_19\")\n",
    "    if not os.path.isdir(path):\n",
    "        path = os.path.join(\"/work\", \"garin0115\", \"models\", f\"{name}_256x832\", \"models\", \"weights_19\")\n",
    "    encoder_pth = torch.load(path+\"/encoder.pth\")\n",
    "    decoder_pth = torch.load(path+\"/depth.pth\")\n",
    "    encoder.load_state_dict({k: v for k, v in encoder_pth.items() if k in encoder.state_dict()})\n",
    "    decoder.load_state_dict(decoder_pth)\n",
    "    model = Depth(encoder, decoder, True).eval().cuda()\n",
    "    # create example data\n",
    "    x = torch.ones((1, 3, 256, 832)).cuda()\n",
    "\n",
    "    # convert to TensorRT feeding sample data as input\n",
    "    print(\"  Convert trt32...\")\n",
    "    model_trt32 = torch2trt(model, [x])\n",
    "    print(\"  Convert trt16...\")\n",
    "    model_trt16 = torch2trt(model, [x], fp16_mode=True)\n",
    "    \n",
    "    path32 = \"/work/garin0115/models/trt32_models/\"\n",
    "    path16 = \"/work/garin0115/models/trt16_models/\"\n",
    "    torch.save(model_trt32.state_dict(), path32+f'{name}_trt32.pth')\n",
    "    torch.save(model_trt16.state_dict(), path16+f'{name}_trt16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
