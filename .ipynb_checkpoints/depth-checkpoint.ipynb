{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch2trt import torch2trt, TRTModule\n",
    "\n",
    "\n",
    "from onnx2trt import get_engine, allocate_buffers, do_inference\n",
    "\n",
    "from layers import disp_to_depth\n",
    "from utils import readlines\n",
    "import datasets\n",
    "import networks\n",
    "import time\n",
    "from thop import profile, clever_format\n",
    "\n",
    "import PIL.Image as pil\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, name, net=None, size=[256, 832]):\n",
    "        self.name = name\n",
    "        self.size = size #[height, width]\n",
    "        self.net = net\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.size\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_net(self):\n",
    "        if self.net == None:\n",
    "            logging.warning('Must set net first')\n",
    "        return self.net\n",
    "    \n",
    "class Depth(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output2list=True): #trt必須將結果轉成list(dict會錯誤)\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output2list = output2list\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        feature = self.encoder(inputs)\n",
    "        output = self.decoder(feature)\n",
    "        output_list = []\n",
    "        if self.output2list:\n",
    "            for i in range(4):\n",
    "                output_list.append(output[(\"disp\", i)]) # 0 最大\n",
    "            output = output_list\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.is_set_Net = False\n",
    "        \n",
    "        # Models which were trained with stereo supervision were trained with a nominal\n",
    "        # baseline of 0.1 units. The KITTI rig has a baseline of 54cm. Therefore,\n",
    "        # to convert our stereo predictions to real-world scale we multiply our depths by 5.4.\n",
    "        self.STEREO_SCALE_FACTOR = 5.4\n",
    "        self.MIN_DEPTH = 1e-3\n",
    "        self.MAX_DEPTH = 80\n",
    "        self.disable_median_scaling = False\n",
    "        self.pred_depth_scale_factor = 1\n",
    "        self.CMAP = 'plasma'\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "        \n",
    "        self.no_cuda = False\n",
    "        self.ext = \"jpg\"\n",
    "        self.data_path = \"/work/garin0115/datasets/kitti_data/\"\n",
    "        self.splits_dir = os.path.join(os.path.expanduser(\"~\"), \"depth\", \"monodepth2\", \"splits\")\n",
    "        self.eval_split = \"eigen\"\n",
    "        self.split_folder = os.path.join(self.splits_dir, self.eval_split)\n",
    "        if torch.cuda.is_available() and not self.no_cuda:\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        self.encoder_dict = {\n",
    "            \"resnet18\":networks.ResnetEncoder(18, False),\n",
    "            \"resnet50\":networks.ResnetEncoder(50, False),\n",
    "            \"mobilenet\":networks.MobileNet(),\n",
    "            \"mobilenetv2\":networks.MobileNetV2(),\n",
    "            \"mobilenetv3\":networks.MobileNetV3(),\n",
    "            \"shufflenetv2\":networks.ShuffleNetV2(),\n",
    "            \"peleenet\":networks.PeleeNet(),\n",
    "            \"mnasnet\":networks.MnasNet()\n",
    "        }\n",
    "        self.decoder_dict = {\n",
    "            \"mj\":networks.MJDecoder,\n",
    "            \"ys\":networks.YSDecoder,\n",
    "            \"mono\":networks.MonoDecoder,\n",
    "            \"ours\":networks.OursDecoder\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def set_Net(self, size=[256, 832]):\n",
    "        self.nets = []\n",
    "        for name in self.model_name:\n",
    "            print(\"[Info] Deal with {} model\\n\".format(name))\n",
    "            if \"trt16\" in name.split(\"_\"):\n",
    "                trt = 16 \n",
    "            elif  \"trt32\" in name.split(\"_\"):\n",
    "                trt = 32 \n",
    "            else:\n",
    "                trt = None\n",
    "            \n",
    "            net = self.load_model(name, epoch=19, size=size, trt=trt)\n",
    "            if not trt:\n",
    "                net = copy.deepcopy(net)\n",
    "            self.nets.append(Net(name, net, size))\n",
    "            \n",
    "    \n",
    "    def load_model(self, name, epoch=19, size=[256, 832], trt=None):\n",
    "        load_weights_folder = self.get_modelPath(name, epoch=epoch, trt=trt)\n",
    "        if trt:\n",
    "            model_pth = torch.load(os.path.join(load_weights_folder, f\"{name}.pth\"))\n",
    "            net = TRTModule()\n",
    "            net.load_state_dict(model_pth)\n",
    "            \n",
    "        else:\n",
    "            pathModel = \"/work/garin0115/models/models/\"\n",
    "            if os.path.isfile(pathModel+f'{name}.pth'):\n",
    "                model_pth = torch.load(pathModel+f'{name}.pth')\n",
    "            else:\n",
    "                encoder_pth = torch.load(os.path.join(load_weights_folder, \"encoder.pth\"))\n",
    "                decoder_pth = torch.load(os.path.join(load_weights_folder, \"depth.pth\"))\n",
    "                assert size==[encoder_pth[\"height\"], encoder_pth[\"width\"]]\n",
    "            # 分析 name\n",
    "            name_split = name.split(\"_\")     \n",
    "            encoder = self.encoder_dict[name_split[0]]\n",
    "            if \"ours\" in name_split:\n",
    "                ablation = {\"bn\":False, \n",
    "                            \"oneLayer\":False, \n",
    "                            \"dw\":False, \n",
    "                            \"pw\":False,\n",
    "                            \"skipAdd\":False, \n",
    "                            \"skipInput\":False,\n",
    "                            \"fastdw\":False, \n",
    "                            \"relu\":False}\n",
    "                for key, abl in ablation.items():\n",
    "                    if key in name_split:\n",
    "                        ablation[key] = True\n",
    "                decoder = self.decoder_dict[name_split[1]](num_ch_enc=encoder.num_ch_enc,\n",
    "                                                           bn=ablation[\"bn\"], \n",
    "                                                           dw=ablation[\"dw\"], \n",
    "                                                           pw=ablation[\"pw\"], \n",
    "                                                           oneLayer=ablation[\"oneLayer\"], \n",
    "                                                           skipAdd=ablation[\"skipAdd\"],\n",
    "                                                           fastdw=ablation[\"fastdw\"],\n",
    "                                                           relu=ablation[\"relu\"])\n",
    "            else:\n",
    "                decoder = self.decoder_dict[name_split[1]](num_ch_enc=encoder.num_ch_enc)\n",
    "            \n",
    "            \n",
    "            if os.path.isfile(pathModel+f'{name}.pth'):\n",
    "                net = Depth(encoder, decoder, True)\n",
    "                net.load_state_dict(model_pth)\n",
    "            else:\n",
    "                encoder.load_state_dict({k: v for k, v in encoder_pth.items() if k in encoder.state_dict()})\n",
    "                decoder.load_state_dict(decoder_pth)\n",
    "                net = Depth(encoder, decoder, True)\n",
    "        return net\n",
    "    \n",
    "        \n",
    "    \n",
    "    def get_modelPath(self, name, epoch=19, size=[256, 832], trt=None):\n",
    "        if trt:\n",
    "            load_weights_folder = os.path.join(\"/work\", \"garin0115\", \"models\", f\"trt{trt}_models\")\n",
    "        else:\n",
    "            \n",
    "            load_weights_folder = os.path.join(\"/work\", \"garin0115\", \"models\", f\"{name}_{size[0]}x{size[1]}\", \"models\")\n",
    "            if not os.path.isdir(load_weights_folder):\n",
    "                load_weights_folder = os.path.join(os.path.expanduser(\"~\"), \n",
    "                                                   \"depth\", \n",
    "                                                   \"monodepth2\",\n",
    "                                                   \"models\", \n",
    "                                                   f\"{name}_{size[0]}x{size[1]}\", \n",
    "                                                   \"models\")\n",
    "\n",
    "            assert os.path.isdir(load_weights_folder), \"Cannot find a folder at {}\".format(load_weights_folder)\n",
    "\n",
    "            print(\"[info] Loading weights from {}\".format(load_weights_folder))\n",
    "\n",
    "            load_weights_folder = os.path.join(load_weights_folder, f\"weights_{epoch}\")\n",
    "        \n",
    "        return load_weights_folder\n",
    "    \n",
    "    def get_dataLoader(self, height, width, batch_size=12):\n",
    "        filenames  = readlines(os.path.join(self.split_folder, \"test_files.txt\"))\n",
    "        dataset    = datasets.KITTIRAWDataset(data_path =self.data_path, \n",
    "                                              filenames =filenames,\n",
    "                                               height    =height, \n",
    "                                               width     =width,\n",
    "                                               frame_idxs=[0], \n",
    "                                               num_scales=4, \n",
    "                                               is_train  =False)\n",
    "        dataLoader = DataLoader(dataset    =dataset,\n",
    "                                batch_size =batch_size,\n",
    "                                shuffle    =False,\n",
    "                                num_workers=12,\n",
    "                                pin_memory =True,\n",
    "                                drop_last  =False)\n",
    "        return dataLoader\n",
    "    \n",
    "    def batch_evaluate_depth(self, save_CSV=False, is_torch2trt=False):\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "            \n",
    "        results = []\n",
    "        \n",
    "        for net in self.nets:\n",
    "            disps, time_min, time_avg = self.evaluate_depth(net, quick_show=True)\n",
    "            result = self.calculate_metric(net.get_name(), disps, time_min, time_avg)\n",
    "            results.append(result)\n",
    "            \n",
    "#             disps, time_min, time_avg = self.evaluate_onnx_depth(net.get_name(), fp16_mode=False)\n",
    "#             result = self.calculate_metric(net.get_name()+\"_trt32\", disps, time_min, time_avg)\n",
    "#             results.append(result)\n",
    "            \n",
    "#             disps, time_min, time_avg = self.evaluate_onnx_depth(net.get_name(), fp16_mode=True)\n",
    "#             result = self.calculate_metric(net.get_name()+\"_trt16\", disps, time_min, time_avg)\n",
    "#             results.append(result)\n",
    "        \n",
    "        if save_CSV:\n",
    "            import csv\n",
    "            # 開啟輸出的 CSV 檔案\n",
    "            with open('result.csv', 'w', newline='') as csvfile:\n",
    "                # 建立 CSV 檔寫入器\n",
    "                writer = csv.writer(csvfile)\n",
    "\n",
    "                # 寫入一列資料\n",
    "#                 writer.writerow(['Model', 'Height', 'Width', \"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\", \n",
    "#                               'Best FPS', 'Avg FPS', 'Parameters', 'params_enc', 'params_dec', 'FLOPs', 'fl_enc', 'fl_dec'])\n",
    "                writer.writerow(['Model', 'Height', 'Width', \"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\", \n",
    "                              'Best FPS', 'Avg FPS'])\n",
    "\n",
    "                # 寫入另外幾列資料\n",
    "                for res in results:\n",
    "                    writer.writerow(res)\n",
    "            \n",
    "    def calculate_metric(self, name, pred_disps, time_min, time_avg):\n",
    "        gt_path = os.path.join(self.split_folder, \"gt_depths.npz\")\n",
    "        gt_depths = np.load(gt_path, fix_imports=True, encoding='latin1', allow_pickle=True)[\"data\"]\n",
    "        \n",
    "        errors = []\n",
    "        ratios = []\n",
    "\n",
    "        for i in range(pred_disps.shape[0]):\n",
    "\n",
    "            gt_depth = gt_depths[i]\n",
    "            gt_height, gt_width = gt_depth.shape[:2]\n",
    "\n",
    "            pred_disp = pred_disps[i]\n",
    "            pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n",
    "            pred_depth = 1 / pred_disp\n",
    "\n",
    "            if self.eval_split == \"eigen\":\n",
    "                mask = np.logical_and(gt_depth > self.MIN_DEPTH, gt_depth < self.MAX_DEPTH)\n",
    "\n",
    "                crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n",
    "                                 0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n",
    "                crop_mask = np.zeros(mask.shape)\n",
    "                crop_mask[crop[0]:crop[1], crop[2]:crop[3]] = 1\n",
    "                mask = np.logical_and(mask, crop_mask)\n",
    "\n",
    "            else:\n",
    "                mask = gt_depth > 0\n",
    "\n",
    "            pred_depth = pred_depth[mask]\n",
    "            gt_depth = gt_depth[mask]\n",
    "\n",
    "            pred_depth *= self.pred_depth_scale_factor\n",
    "            if not self.disable_median_scaling:\n",
    "                ratio = np.median(gt_depth) / np.median(pred_depth)\n",
    "                ratios.append(ratio)\n",
    "                pred_depth *= ratio\n",
    "\n",
    "            pred_depth[pred_depth < self.MIN_DEPTH] = self.MIN_DEPTH\n",
    "            pred_depth[pred_depth > self.MAX_DEPTH] = self.MAX_DEPTH\n",
    "\n",
    "            errors.append(self.compute_errors(gt_depth, pred_depth))\n",
    "\n",
    "        if not self.disable_median_scaling:\n",
    "            ratios = np.array(ratios)\n",
    "            med = np.median(ratios)\n",
    "            print(\" Scaling ratios | med: {:0.3f} | std: {:0.3f}\".format(med, np.std(ratios / med)))\n",
    "\n",
    "        mean_errors = np.array(errors).mean(0)\n",
    "        print(\"[info] {}\".format(name))\n",
    "        print(\" best FPS: \", 1/time_min)\n",
    "        print(\" avg FPS: \", 1/time_avg)\n",
    "        print(\"\\n  \" + (\"{:>8} | \" * 7).format(\"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\"))\n",
    "        print((\"&{: 8.3f}  \" * 7).format(*mean_errors.tolist()) + \"\\\\\\\\\")\n",
    "        print(\"\\n-> Done!\")\n",
    "\n",
    "        \n",
    "#         flops_enc, params_enc = profile(encoder, inputs=(input_color, ))\n",
    "#         flops_dec, params_dec = profile(decoder, inputs=(*tuple(features), ))\n",
    "#         a, b, c, d, e, f = clever_format([params_enc+params_dec, \n",
    "#                                           params_enc, \n",
    "#                                           params_dec, \n",
    "#                                           flops_enc+flops_dec, \n",
    "#                                           flops_enc, \n",
    "#                                           flops_dec], \"%.3f\")\n",
    "\n",
    "        result = []\n",
    "        result.append(name)\n",
    "        result.append(256)\n",
    "        result.append(832)\n",
    "        for i in mean_errors:\n",
    "            result.append(i)\n",
    "        result.append(1/time_min)\n",
    "        result.append(1/time_avg)\n",
    "#         for i in [a, b, c, d, e, f]:\n",
    "#             result.append(i)\n",
    "    \n",
    "        return result\n",
    "        \n",
    "\n",
    "    \n",
    "    def evaluate_depth(self, net, quick_show=False):\n",
    "        name = net.get_name()\n",
    "        size = net.get_size()\n",
    "        dataLoader = self.get_dataLoader(size[0], size[1], batch_size=1)\n",
    "        print(\"[info] Model {}\".format(name))\n",
    "        if \"trt16\" in name.split(\"_\"):\n",
    "            trt = 16 \n",
    "        elif  \"trt32\" in name.split(\"_\"):\n",
    "            trt = 32 \n",
    "        else:\n",
    "            trt = None\n",
    "\n",
    "        if trt:\n",
    "            model = net.get_net().to(self.device)\n",
    "#             x = torch.ones((1, 3, 256, 832)).cuda()\n",
    "#             model = torch2trt(model, [x], keep_network=True)\n",
    "        else:\n",
    "            model = net.get_net().eval().to(self.device)\n",
    "\n",
    "        \n",
    "        \n",
    "        pred_disps = []\n",
    "\n",
    "        print(\"[info] Computing predictions with size {}x{}\".format(\n",
    "            size[0], size[1]))\n",
    "        \n",
    "        time_min = float('inf')\n",
    "        all_data_time = 0\n",
    "        avg_FPS = 0\n",
    "        run_times = 1\n",
    "        with torch.no_grad():\n",
    "            for i in range(run_times): #跑十次算FPS\n",
    "                for idx, data in enumerate(dataLoader):\n",
    "                    input_color = data[(\"color\", 0, 0)].cuda()\n",
    "                    start_time = time.time()\n",
    "                    output = model(input_color)\n",
    "                    end_time = time.time()\n",
    "#                     pred_disp, _ = disp_to_depth(output[(\"disp\", 0)], self.MIN_DEPTH, self.MAX_DEPTH)\n",
    "                    pred_disp, _ = disp_to_depth(output[0], self.MIN_DEPTH, self.MAX_DEPTH)\n",
    "                    pred_disp = pred_disp[:, 0].cpu().numpy()\n",
    "#                     pred_disp = pred_disp[:, 0].numpy()\n",
    "                    if i == 0: #只有跑第一次時把結果存起來\n",
    "                        pred_disps.append(pred_disp)\n",
    "                        if quick_show and idx == 0:\n",
    "                            show_input = data[(\"color\", 0, 0)]\n",
    "                            show_disp = output[0]\n",
    "                    batch_time = end_time - start_time\n",
    "                    all_data_time += batch_time\n",
    "                    if batch_time < time_min:\n",
    "                        time_min = batch_time\n",
    "                    \n",
    "\n",
    "                \n",
    "                FPS = all_data_time / len(dataLoader)\n",
    "                avg_FPS += FPS\n",
    "            avg_FPS /= run_times\n",
    "            \n",
    "        pred_disps = np.concatenate(pred_disps)\n",
    "        \n",
    "        if quick_show:\n",
    "            disp_output = show_disp.squeeze().cpu().detach().numpy()\n",
    "            print(disp_output.shape)\n",
    "            disp_resized = cv2.resize(disp_output, dsize=(1226, 370))\n",
    "\n",
    "            plt.figure(figsize=(2*3*3, 1*3+1))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(show_input.squeeze().numpy().transpose(1, 2, 0))\n",
    "            plt.title(\"Input\")\n",
    "            vmax = np.percentile(disp_resized, 95)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(disp_resized, cmap=self.CMAP, vmax=vmax)\n",
    "            plt.title(net.get_name()+f\"{avg_FPS}\")\n",
    "            plt.show()\n",
    "        \n",
    "        return pred_disps, time_min, avg_FPS\n",
    "    \n",
    "    def evaluate_onnx_depth(self, name, fp16_mode=True):\n",
    "        dataLoader = self.get_dataLoader(256, 832)\n",
    "        print(\"[info] Model {}\".format(name))\n",
    "        \n",
    "\n",
    "        pred_disps = []\n",
    "\n",
    "        print(\"[info] Computing predictions with size {}x{}\".format(\n",
    "            256, 832))\n",
    "        \n",
    "        \n",
    "        \n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\", \n",
    "                                  name+\"_256x832\", \n",
    "                                  \"models\", \n",
    "                                  \"weights_19\", \n",
    "                                  name+\".onnx\")\n",
    "        if fp16_mode:\n",
    "            engine_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\",\n",
    "                                      \"trt16_models\",\n",
    "                                      name+\".trt\")\n",
    "        else:\n",
    "            engine_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\",\n",
    "                                      \"trt_models\",\n",
    "                                      name+\".trt\")\n",
    "        #engine\n",
    "        engine = get_engine(fp16_mode=False, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=False)\n",
    "        # Create the context for this engine\n",
    "        context = engine.create_execution_context()\n",
    "        # Allocate buffers for input and output\n",
    "        inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "        time_min = float('inf')\n",
    "        time_avg = 0\n",
    "        avg_FPS = 0\n",
    "        \n",
    "        for data in dataLoader:\n",
    "            input_images = data[(\"color\", 0, 0)].numpy()\n",
    "            batch_pred_disp = []\n",
    "            total_time = 0\n",
    "            for input_image in input_images:\n",
    "                input_image = np.expand_dims(input_image, axis=0).reshape(-1)\n",
    "                inputs[0].host = input_image\n",
    "                start_time = time.time()\n",
    "                trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "                end_time = time.time() - start_time\n",
    "                total_time += end_time\n",
    "                pred_disp, _ = disp_to_depth(trt_outputs[-1], self.MIN_DEPTH, self.MAX_DEPTH)\n",
    "                pred_disp = pred_disp.reshape(1, 256, 832)\n",
    "                batch_pred_disp.append(pred_disp)\n",
    "                \n",
    "                if end_time < time_min:\n",
    "                    time_min = end_time\n",
    "            total_time /= len(input_images)\n",
    "            time_avg += total_time\n",
    "                \n",
    "            pred_disps.append(np.concatenate(batch_pred_disp, axis=0))\n",
    "            \n",
    "        time_avg /= len(dataLoader)\n",
    "       \n",
    "        \n",
    "        \n",
    "        pred_disps = np.concatenate(pred_disps)\n",
    "        \n",
    "        return pred_disps, time_min, time_avg\n",
    "        \n",
    "\n",
    "    def batch_inference_depth(self, column=2, output_video=False):\n",
    "        lines = readlines(os.path.join(self.split_folder, \"test_files.txt\"))\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net(is_torch2trt)\n",
    "        \n",
    "        input_images = []\n",
    "        original_shape = []\n",
    "        if output_video:\n",
    "            pass\n",
    "        else:\n",
    "            for i in np.random.choice(len(lines), 10, replace=False):\n",
    "                folder, frame_id, side = lines[i].split()\n",
    "                frame_id = int(frame_id)  \n",
    "                image_path = os.path.join(self.data_path, folder, \n",
    "                                          \"image_0{}\".format(self.side_map[side]), \n",
    "                                          \"data\", \n",
    "                                          \"{:010d}.jpg\".format(frame_id))\n",
    "                input_image = pil.open(image_path).convert('RGB')\n",
    "                original_width, original_height = input_image.size\n",
    "                input_images.append(input_image)\n",
    "                original_shape.append(input_image.size)\n",
    "        \n",
    "            \n",
    "            \n",
    "        for net in self.nets:\n",
    "            disp = self.inference_depth(net, input_image)\n",
    "\n",
    "    \n",
    "    def inference_depth(self, net, input_image):\n",
    "        with torch.no_grad():\n",
    "            for i in np.random.choice(len(lines), 10, replace=False):\n",
    "                folder, frame_id, side = lines[i].split()\n",
    "                frame_id = int(frame_id)  \n",
    "                image_path = os.path.join(self.data_path, folder, \n",
    "                                          \"image_0{}\".format(self.side_map[side]), \n",
    "                                          \"data\", \n",
    "                                          \"{:010d}.jpg\".format(frame_id))\n",
    "                input_image = pil.open(image_path).convert('RGB')\n",
    "                original_width, original_height = input_image.size\n",
    "                \n",
    "                \n",
    "                result = OrderedDict()\n",
    "                result[\"Input\"] = input_image\n",
    "#                 result[\"Mask\"] = self.seg_img(input_image)\n",
    "                \n",
    "                for net in self.nets:\n",
    "                    size = net.get_size()\n",
    "                    model = net.get_net()\n",
    "                    model.eval()\n",
    "                    model.to(self.device)\n",
    "                    \n",
    "                    input_image = pil.open(image_path).convert('RGB')\n",
    "                    input_image_resized = input_image.resize(net.get_size(), pil.LANCZOS)\n",
    "                    input_image_torch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "                    input_image_torch = input_image_torch.to(self.device)\n",
    "                    \n",
    "                    outputs = model(input_image_torch)\n",
    "                    \n",
    "                    \n",
    "                    disp = outputs[(\"disp\", 0)]\n",
    "                    disp_resized = torch.nn.functional.interpolate(\n",
    "                            disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "                    disp_resized_np = disp_resized.squeeze().cpu().detach().numpy()\n",
    "                    result[\"{}\".format(net.get_name())] = disp_resized_np\n",
    "                    \n",
    "                    if is_onnx:\n",
    "                        name = net.get_name()\n",
    "                        onnx_path = os.path.join(\"/work\", \n",
    "                                      \"garin0115\", \n",
    "                                      \"models\", \n",
    "                                      name+\"_256x832\", \n",
    "                                      \"models\", \n",
    "                                      \"weights_19\", \n",
    "                                      name+\".onnx\")\n",
    "                        engine16_path = os.path.join(\"/work\", \n",
    "                                                  \"garin0115\", \n",
    "                                                  \"models\",\n",
    "                                                  \"trt16_models\",\n",
    "                                                  name+\".trt\")\n",
    "\n",
    "                        engine_path = os.path.join(\"/work\", \n",
    "                                                  \"garin0115\", \n",
    "                                                  \"models\",\n",
    "                                                  \"trt_models\",\n",
    "                                                  name+\".trt\")\n",
    "                        \n",
    "                        input_image = input_image.resize((832, 256), pil.LANCZOS)\n",
    "                        input_image = np.array(input_image).transpose((2, 0, 1)).astype(np.float32) / 255.\n",
    "                        print(input_image.shape)\n",
    "                        input_image = np.expand_dims(input_image, axis=0).reshape(-1)\n",
    "\n",
    "\n",
    "                        #engine16\n",
    "                        engine16 = get_engine(fp16_mode=True, onnx_file_path=onnx_path, engine_file_path=engine16_path, save_engine=False)\n",
    "                        # Create the context for this engine\n",
    "                        context16 = engine16.create_execution_context()\n",
    "                        # Allocate buffers for input and output\n",
    "                        inputs16, outputs16, bindings16, stream16 = allocate_buffers(engine16)\n",
    "                        inputs16[0].host = input_image\n",
    "                        trt_outputs16 = do_inference(context16, bindings=bindings16, inputs=inputs16, outputs=outputs16, stream=stream16) # numpy data\n",
    "\n",
    "                        #engine\n",
    "                        engine = get_engine(fp16_mode=False, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=False)\n",
    "                        # Create the context for this engine\n",
    "                        context = engine.create_execution_context()\n",
    "                        # Allocate buffers for input and output\n",
    "                        inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "                        inputs[0].host = input_image\n",
    "                        trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) # numpy data\n",
    "\n",
    "                        result[\"{}\".format(name+\"_trt32\")] = trt_outputs[-1].reshape(size)\n",
    "                        result[\"{}\".format(name+\"_trt16\")] = trt_outputs16[-1].reshape(size)\n",
    "                \n",
    "                self.quick_show(result, column=column)\n",
    "    \n",
    "    def inference_segment_sky(self):\n",
    "        result = OrderedDict()\n",
    "        lines = readlines(os.path.join(self.split_folder, \"test_files.txt\"))\n",
    "        for i in np.random.choice(len(lines), 10, replace=False):\n",
    "            folder, frame_id, side = lines[i].split()\n",
    "            frame_id = int(frame_id)  \n",
    "            image_path = os.path.join(self.data_path, folder, \n",
    "                                      \"image_0{}\".format(self.side_map[side]), \n",
    "                                      \"data\", \n",
    "                                      \"{:010d}.jpg\".format(frame_id))\n",
    "            input_image = pil.open(image_path).convert('RGB')\n",
    "            \n",
    "            result[\"Input_{}\".format(i)] = input_image\n",
    "            result[\"Mask_{}\".format(i)] = self.seg_img(input_image)\n",
    "            \n",
    "        self.quick_show(result, column=4)\n",
    "            \n",
    "    \n",
    "    def seg_img(self, image):\n",
    "        image = cv2.cvtColor(np.asarray(image),cv2.COLOR_RGB2BGR) \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "        thresh_dilation = cv2.dilate(thresh, kernel, anchor=(-1,-1), iterations=8)\n",
    "\n",
    "        edges = cv2.Canny(gray, 1, 100)\n",
    "        edges_dilation = cv2.dilate(edges, kernel, anchor=(-1,-1), iterations=8)\n",
    "\n",
    "        mask = thresh_dilation | edges_dilation\n",
    "        mask_dilation = cv2.dilate(mask, kernel, anchor=(-1,-1), iterations=8)\n",
    "        segImg = 255 - mask_dilation \n",
    "        segImg[image.shape[0]//3:, :] = 0\n",
    "            \n",
    "        return segImg\n",
    "                \n",
    "    def evaluate_pose(self):\n",
    "        pass            \n",
    "    \n",
    "    def inference_pose(self):\n",
    "        pass\n",
    "    \n",
    "    def quick_show(self, result, column=2):\n",
    "        row = len(result) // column\n",
    "        if len(result) % column > 0:\n",
    "            row += 1\n",
    "        plt.figure(figsize=(column*3*3, row*1*3+1))\n",
    "        for idx, key in enumerate(result):\n",
    "            \n",
    "            if key.split(\"_\")[0] == \"Input\":\n",
    "                plt.subplot(row, column, idx+1)\n",
    "                plt.imshow(result[key])\n",
    "                plt.title(key, fontsize=22)\n",
    "                continue\n",
    "                \n",
    "            plt.subplot(row, column, idx+1)\n",
    "            if key.split(\"_\")[0] == \"Mask\":\n",
    "                plt.imshow(result[key], cmap=\"gray\")\n",
    "            else:\n",
    "                vmax = np.percentile(result[key], 95)\n",
    "                plt.imshow(result[key], cmap=self.CMAP, vmax=vmax)\n",
    "                \n",
    "            if key == \"resnet18_simplify2my3\":\n",
    "                plt.title(key, fontsize=22, color=\"red\")\n",
    "            elif key == \"resnet18_skip2Conv\":\n",
    "                plt.title(key, fontsize=22, color=\"blue\")\n",
    "            else:\n",
    "                plt.title(key, fontsize=22)\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0.5, w_pad=0.1, h_pad=0.1)\n",
    "        \n",
    "    def make_grid(self, result, column=2):\n",
    "        pass\n",
    "    \n",
    "    def make_vedio(self, file_name, video_output_folder, column=2):\n",
    "        # 取得資料夾中所有影像檔案路徑\n",
    "        kitti_depth_folder = '/work/garin0115/datasets/kitti_data/'+file_name+'/image_02'\n",
    "        filenames = glob.glob(kitti_depth_folder+'/*/*.jpg')\n",
    "\n",
    "        # 將檔案路徑排序\n",
    "        filenames.sort()\n",
    "        num_images = len(filenames)\n",
    "        print(\"Total images: {}\".format(num_images))\n",
    "        \n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "    \n",
    "        num_model = len(self.nets)\n",
    "        num_column = column\n",
    "        num_row = num_model // column if num_model % column == 0 else num_model // column + 1\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(video_output_folder+\"/disp_{}.avi\".format(file_name.split('/')[-1]), fourcc, 15.0, (original_width*num_column, original_height*num_row))\n",
    "        \n",
    "        \n",
    "        for idx in range(num_images):\n",
    "            res = []\n",
    "            input_image = pil.open(img).convert('RGB')\n",
    "            input_image = np.array(input_image)\n",
    "            cv2.putText(input_image, \"Input\", (10, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            res.append(input_image[:, :, ::-1])\n",
    "\n",
    "            for net in self.nets:\n",
    "                name = net.get_name()\n",
    "                disp = show[name][idx]\n",
    "                vmax = np.percentile(disp, 95)\n",
    "                normalizer = mpl.colors.Normalize(vmin=disp.min(), vmax=vmax)\n",
    "                mapper = cm.ScalarMappable(norm=normalizer, cmap=CMAP)\n",
    "                colormapped_im = (mapper.to_rgba(disp)[:, :, :3] * 255).astype(np.uint8)\n",
    "                cv2.putText(colormapped_im, name, (10, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(colormapped_im, \"FPS \"+show[\"{}_FPS\".format(name)][idx], (10, 100), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                im = pil.fromarray(colormapped_im[:, :, ::-1])\n",
    "                res.append(im)\n",
    "            \n",
    "            \n",
    "            result = []\n",
    "            for i in range(num_row):\n",
    "                result.append(np.hstack(res[num_row * num_column: (num_row+1) * num_column]))\n",
    "            result = np.vstack(result)\n",
    "\n",
    "\n",
    "            out.write(result)\n",
    "        out.release()    \n",
    "        \n",
    "        \n",
    "    \n",
    "    def calc_param(self, net):\n",
    "        net_params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "        weight_count = 0\n",
    "        for param in net_params:\n",
    "            weight_count += np.prod(param.size())\n",
    "        return weight_count\n",
    "    \n",
    "    def compute_errors(self, gt, pred):\n",
    "        \"\"\"Computation of error metrics between predicted and ground truth depths\n",
    "        \"\"\"\n",
    "        thresh = np.maximum((gt / pred), (pred / gt))\n",
    "        a1 = (thresh < 1.25     ).mean()\n",
    "        a2 = (thresh < 1.25 ** 2).mean()\n",
    "        a3 = (thresh < 1.25 ** 3).mean()\n",
    "\n",
    "        rmse = (gt - pred) ** 2\n",
    "        rmse = np.sqrt(rmse.mean())\n",
    "\n",
    "        rmse_log = (np.log(gt) - np.log(pred)) ** 2\n",
    "        rmse_log = np.sqrt(rmse_log.mean())\n",
    "\n",
    "        abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "\n",
    "        sq_rel = np.mean(((gt - pred) ** 2) / gt)\n",
    "\n",
    "        return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n",
    "    \n",
    "    def batch_post_process_disparity(self, l_disp, r_disp):\n",
    "        \"\"\"Apply the disparity post-processing method as introduced in Monodepthv1\n",
    "        \"\"\"\n",
    "        _, h, w = l_disp.shape\n",
    "        m_disp = 0.5 * (l_disp + r_disp)\n",
    "        l, _ = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
    "        l_mask = (1.0 - np.clip(20 * (l - 0.05), 0, 1))[None, ...]\n",
    "        r_mask = l_mask[:, :, ::-1]\n",
    "        return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n",
    "    \n",
    "    #save to ONNX model\n",
    "    def save_ONNX(self):\n",
    "        if self.is_set_Net == False:\n",
    "            self.is_set_Net = True\n",
    "            self.set_Net()\n",
    "            \n",
    "        for net in self.nets:\n",
    "            net_name = net.get_name()\n",
    "            net_height = net.get_height()\n",
    "            net_width = net.get_width()\n",
    "            path = self.get_modelPath(net_name)\n",
    "            \n",
    "            if not os.path.isfile(path+\"/\"+net_name+\".onnx\"):\n",
    "                encoder = net.get_encoder()\n",
    "                decoder = net.get_decoder()\n",
    "                depth_model = Depth(encoder, decoder, output_list=True)\n",
    "                depth_model.to(self.device)\n",
    "                depth_model.eval()\n",
    "                x = torch.randn(1, 3, net_height, net_width, requires_grad=True).to(self.device)\n",
    "                \n",
    "                # Export the model\n",
    "                torch.onnx.export(depth_model,               # model being run\n",
    "                                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                                  path+\"/\"+net.get_name()+\".onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                                  opset_version=11,          # the ONNX version to export the model to\n",
    "                                  verbose=True,\n",
    "                                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                                  input_names = ['input'],   # the model's input names\n",
    "                                  output_names = ['output']) # the model's output names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model name list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = [\n",
    "    #3種 model\n",
    "    \"resnet18_mono\", # have trained\n",
    "    \"resnet50_ys\", # have trained\n",
    "    \"mobilenetv2_mj\", # have trained\n",
    "    \n",
    "    #判斷 encoder 好壞\n",
    "    \"resnet18_ys\", # have trained\n",
    "    \"mobilenetv2_ys\", # have trained\n",
    "    \n",
    "    #判斷 decoder 好壞\n",
    "    \"resnet18_mj\", # have trained\n",
    "    \n",
    "    #改進天空訓練\n",
    "    \"resnet18_ours_skipSky\", #have trained\n",
    "    \"resnet18_ours_skyLoss\",\n",
    "    \"resnet18_ours_skipSky_skyLoss\",\n",
    "    \n",
    "    #簡化 ours decoder\n",
    "    \"resnet18_ours\", #應該跟resnet18_momo差不多 have trained\n",
    "    \"resnet18_ours_bn\", # have trained\n",
    "    \"resnet18_ours_oneLayer\", # have trained\n",
    "    \"resnet18_ours_dw\", # have trained\n",
    "    \"resnet18_ours_pw\", # have trained\n",
    "    \"resnet18_ours_skipAdd\", # have trained\n",
    "    \n",
    "\n",
    "    #其他訓練\n",
    "    \"resnet18_ours_oneLayer_pw\", #have trained\n",
    "    \"resnet18_ours_oneLayer_dw_pw\", #have trained\n",
    "    \"resnet18_ours_bn_dw\", #看dw有沒有做bn的影響 #have trained\n",
    "    \"mobilenet_ours\", #have trained\n",
    "    \"mobilenetv3_ours\", #have trained\n",
    "    \n",
    "    ##針對 ours 改進\n",
    "    \"resnet18_ours_oneLayer_bn_dw_pw\",\n",
    "    \"resnet18_ours_oneLayer_skipSky_skyLoss\",\n",
    "    \"resnet18_ours_oneLayer_bn_dw_pw_skipSky_skyLoss\",\n",
    "    \"resnet18_ours_scratch\",\n",
    "    \"resnet18_ours_bn_relu\",\n",
    "    \"resnet18_ours_relu\",\n",
    "    \"resnet18_ours_fastdw\",\n",
    "    \"resnet18_ours_oneLayer_fastdw_skipSky_skyLoss\",\n",
    "    \n",
    "    \n",
    "    #32###################\n",
    "    \n",
    "    \"resnet18_mono_trt32\", # have trained\n",
    "    \"resnet50_ys_trt32\", # have trained\n",
    "    \"mobilenetv2_mj_trt32\", # have trained\n",
    "    \n",
    "    #判斷 encoder 好壞\n",
    "    \"resnet18_ys_trt32\", # have trained\n",
    "    \"mobilenetv2_ys_trt32\", # have trained\n",
    "    \n",
    "    #判斷 decoder 好壞\n",
    "    \"resnet18_mj_trt32\", # have trained\n",
    "    \n",
    "    #改進天空訓練\n",
    "    \"resnet18_ours_skipSky_trt32\", #have trained\n",
    "    \"resnet18_ours_skyLoss_trt32\",\n",
    "    \"resnet18_ours_skipSky_skyLoss_trt32\",\n",
    "    \n",
    "    #簡化 ours decoder\n",
    "    \"resnet18_ours_trt32\", #應該跟resnet18_momo差不多 have trained\n",
    "    \"resnet18_ours_bn_trt32\", # have trained\n",
    "    \"resnet18_ours_oneLayer_trt32\", # have trained\n",
    "    \"resnet18_ours_dw_trt32\", # have trained\n",
    "    \"resnet18_ours_pw_trt32\", # have trained\n",
    "    \"resnet18_ours_skipAdd_trt32\", # have trained\n",
    "    \n",
    "    #其他訓練\n",
    "    \"resnet18_ours_oneLayer_pw_trt32\", #have trained\n",
    "    \"resnet18_ours_oneLayer_dw_pw_trt32\", #have trained\n",
    "    \"resnet18_ours_bn_dw_trt32\", #看dw有沒有做bn的影響 #have trained\n",
    "    \"mobilenet_ours_trt32\", #have trained\n",
    "    \"mobilenetv3_ours_trt32\", #have trained\n",
    "    \n",
    "    \n",
    "    #針對 ours 改進\n",
    "    \"resnet18_ours_oneLayer_bn_dw_pw_trt32\",\n",
    "    \"resnet18_ours_oneLayer_skipSky_skyLoss_trt32\",\n",
    "    \"resnet18_ours_oneLayer_bn_dw_pw_skipSky_skyLoss_trt32\",\n",
    "    \"resnet18_ours_scratch_trt32\",\n",
    "    \"resnet18_ours_bn_relu_trt32\",\n",
    "    \"resnet18_ours_relu_trt32\",\n",
    "    \"resnet18_ours_fastdw_trt32\",\n",
    "    \"resnet18_ours_oneLayer_fastdw_skipSky_skyLoss_trt32\",\n",
    "    \n",
    "    \n",
    "    #16###################\n",
    "    #3種 model\n",
    "    \"resnet18_mono_trt16\", # have trained\n",
    "    \"resnet50_ys_trt16\", # have trained\n",
    "    \"mobilenetv2_mj_trt16\", # have trained\n",
    "    \n",
    "    #判斷 encoder 好壞\n",
    "    \"resnet18_ys_trt16\", # have trained\n",
    "    \"mobilenetv2_ys_trt16\", # have trained\n",
    "    \n",
    "    #判斷 decoder 好壞\n",
    "    \"resnet18_mj_trt16\", # have trained\n",
    "    \n",
    "    #改進天空訓練\n",
    "    \"resnet18_ours_skipSky_trt16\", #have trained\n",
    "    \"resnet18_ours_skyLoss_trt16\",\n",
    "    \"resnet18_ours_skipSky_skyLoss_trt16\",\n",
    "    \n",
    "    #簡化 ours decoder\n",
    "    \"resnet18_ours_trt16\", #應該跟resnet18_momo差不多 have trained\n",
    "    \"resnet18_ours_bn_trt16\", # have trained\n",
    "    \"resnet18_ours_oneLayer_trt16\", # have trained\n",
    "    \"resnet18_ours_dw_trt16\", # have trained\n",
    "    \"resnet18_ours_pw_trt16\", # have trained\n",
    "    \"resnet18_ours_skipAdd_trt16\", # have trained\n",
    "    \n",
    "\n",
    "    #其他訓練\n",
    "    \"resnet18_ours_oneLayer_pw_trt16\", #have trained\n",
    "    \"resnet18_ours_oneLayer_dw_pw_trt16\", #have trained\n",
    "    \"resnet18_ours_bn_dw_trt16\", #看dw有沒有做bn的影響 #have trained\n",
    "    \"mobilenet_ours_trt16\", #have trained\n",
    "    \"mobilenetv3_ours_trt16\", #have trained\n",
    "    \n",
    "    #針對 ours 改進\n",
    "    \"resnet18_ours_oneLayer_bn_dw_pw_trt16\",\n",
    "    \"resnet18_ours_oneLayer_skipSky_skyLoss_trt16\",\n",
    "    \"resnet18_ours_oneLayer_bn_dw_pw_skipSky_skyLoss_trt16\",\n",
    "    \"resnet18_ours_scratch_trt16\",\n",
    "    \"resnet18_ours_bn_relu_trt16\",\n",
    "    \"resnet18_ours_relu_trt16\",\n",
    "    \"resnet18_ours_fastdw_trt16\",\n",
    "    \"resnet18_ours_oneLayer_fastdw_skipSky_skyLoss_trt16\"\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = Model(model_name)\n",
    "cv2.setNumThreads(0)  # This speeds up evaluation 5x on our unix systems (OpenCV 3.3.1)\n",
    "\n",
    "\n",
    "# Models.save_ONNX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Deal with resnet18_mono model\n",
      "\n",
      "[info] Loading weights from /work/garin0115/models/resnet18_mono_256x832/models\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'encoder_pth' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-09d3156aaa31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_evaluate_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_CSV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch2trt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e05536d94210>\u001b[0m in \u001b[0;36mbatch_evaluate_depth\u001b[0;34m(self, save_CSV, is_torch2trt)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set_Net\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set_Net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_Net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e05536d94210>\u001b[0m in \u001b[0;36mset_Net\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mtrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e05536d94210>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, name, epoch, size, trt)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ch_enc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_ch_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoder_pth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_pth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'encoder_pth' referenced before assignment"
     ]
    }
   ],
   "source": [
    "Models.batch_evaluate_depth(save_CSV=True, is_torch2trt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.inference_depth(column=2, is_torch2trt=False, is_onnx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.inference_segment_sky()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX2TRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_mode = True\n",
    "print(\"Model Name           FPS\")\n",
    "for name in model_name:\n",
    "    if name == \"resnet18_oneLayer\":\n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                          \"garin0115\", \n",
    "                          \"models\", \n",
    "                          name+\"_256x832\", \n",
    "                          \"models\", \n",
    "                          \"weights_18\", \n",
    "                          name+\".onnx\")\n",
    "    else:\n",
    "        onnx_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\", \n",
    "                                  name+\"_256x832\", \n",
    "                                  \"models\", \n",
    "                                  \"weights_19\", \n",
    "                                  name+\".onnx\")\n",
    "    if fp16_mode:\n",
    "        engine_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\",\n",
    "                                  \"trt16_models\",\n",
    "                                  name+\".trt\")\n",
    "    else:\n",
    "        engine_path = os.path.join(\"/work\", \n",
    "                                  \"garin0115\", \n",
    "                                  \"models\",\n",
    "                                  \"trt_models\",\n",
    "                                  name+\".trt\")\n",
    "\n",
    "    engine = get_engine(fp16_mode=fp16_mode, onnx_file_path=onnx_path, engine_file_path=engine_path, save_engine=True)\n",
    "\n",
    "    # Create the context for this engine\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Allocate buffers for input and output\n",
    "    inputs, outputs, bindings, stream = allocate_buffers(engine) # input, output: host # bindings\n",
    "\n",
    "\n",
    "    # Load data to the buffer\n",
    "    image_path = \"assets/test_image.jpg\"\n",
    "    input_image = pil.open(image_path).convert('RGB').resize((832, 256), pil.LANCZOS)\n",
    "    input_image = np.array(input_image).transpose((2, 0, 1)).astype(np.float32) / 255.\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    inputs[0].host = input_image.reshape(-1)\n",
    "\n",
    "    # inputs[1].host = ... for multiple input\n",
    "    t1 = time.time()\n",
    "    for i in range(100):\n",
    "        trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) # numpy data\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print(\"{}       {}\".format(name, 100/(t2-t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇要建立 video 的 data [TODO]\n",
    "# file_name = '2011_10_03/2011_10_03_drive_0047_sync' #837\n",
    "# file_name = '2011_09_30/2011_09_30_drive_0016_sync' #279\n",
    "# file_name = '2011_09_29/2011_09_29_drive_0026_sync' #158\n",
    "# file_name = '2011_09_28/2011_09_28_drive_0037_sync' #89\n",
    "file_name = '2011_09_26/2011_09_26_drive_0036_sync' #803\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0023_sync' #474\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0020_sync' #86\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0013_sync' #144\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0002_sync' #77\n",
    "\n",
    "# 選擇影片輸出資料夾 [TODO]\n",
    "video_output_folder = os.path.join(os.path.expanduser(\"~\"), \n",
    "                                                      \"depth\",\n",
    "                                                      \"monodepth2\",\n",
    "                                                      \"video_result\")\n",
    "\n",
    "# 取得資料夾中所有影像檔案路徑\n",
    "kitti_depth_folder = '/work/garin0115/datasets/kitti_data/'+file_name+'/image_02'\n",
    "filenames = glob.glob(kitti_depth_folder+'/*/*.jpg')\n",
    "\n",
    "# 將檔案路徑排序\n",
    "filenames.sort()\n",
    "num_images = len(filenames)\n",
    "print(\"Total images: {}\".format(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 思考看看要不要實作Analyze的class去產生圖示化的結果\n",
    "2. 或是直接實作在Depth上面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyze():\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = networks.ResnetEncoder(18, False).cuda()\n",
    "summary(resnet18, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = networks.ResnetEncoder(50, False).cuda()\n",
    "summary(resnet50, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = networks.MobileNet().cuda()\n",
    "summary(mobilenet, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv2 = networks.MobileNetV2().cuda()\n",
    "summary(mobilenetv2, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv3 = networks.MobileNetV3().cuda()\n",
    "summary(mobilenetv3, (3, 256, 832), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 轉 tensorRT 並儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch2trt import torch2trt\n",
    "import torch.nn as nn\n",
    "class Depth(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output2list=True): #trt必須將結果轉成list(dict會錯誤)\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output2list = output2list\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        feature = self.encoder(inputs)\n",
    "        output = self.decoder(feature)\n",
    "        output_list = []\n",
    "        if self.output2list:\n",
    "            for i in range(4):\n",
    "                output_list.append(output[(\"disp\", 3-i)])\n",
    "            output = output_list\n",
    "        return output\n",
    "    \n",
    "encoder_dict = {\n",
    "    \"resnet18\":networks.ResnetEncoder(18, False),\n",
    "    \"resnet50\":networks.ResnetEncoder(50, False),\n",
    "    \"mobilenet\":networks.MobileNet(),\n",
    "    \"mobilenetv2\":networks.MobileNetV2(),\n",
    "    \"mobilenetv3\":networks.MobileNetV3(),\n",
    "    \"shufflenetv2\":networks.ShuffleNetV2(),\n",
    "    \"peleenet\":networks.PeleeNet(),\n",
    "    \"mnasnet\":networks.MnasNet()\n",
    "}\n",
    "decoder_dict = {\n",
    "    \"mj\":networks.MJDecoder,\n",
    "    \"ys\":networks.YSDecoder,\n",
    "    \"mono\":networks.MonoDecoder,\n",
    "    \"ours\":networks.OursDecoder\n",
    "}\n",
    "\n",
    "for name in model_name:\n",
    "    print(f\"[info] Deal with {name}\")\n",
    "    name_split = name.split(\"_\")     \n",
    "    encoder = encoder_dict[name_split[0]]\n",
    "    if \"ours\" in name_split:\n",
    "        ablation = {\"bn\":False, \n",
    "                    \"oneLayer\":False, \n",
    "                    \"dw\":False, \n",
    "                    \"pw\":False,\n",
    "                    \"skipAdd\":False, \n",
    "                    \"skipInput\":False,\n",
    "                    \"fastdw\":False,\n",
    "                    \"relu\":False}\n",
    "        for key, abl in ablation.items():\n",
    "            if key in name_split:\n",
    "                ablation[key] = True\n",
    "        decoder = decoder_dict[name_split[1]](num_ch_enc=encoder.num_ch_enc,\n",
    "                                                           bn=ablation[\"bn\"], \n",
    "                                                           dw=ablation[\"dw\"], \n",
    "                                                           pw=ablation[\"pw\"], \n",
    "                                                           oneLayer=ablation[\"oneLayer\"], \n",
    "                                                           skipAdd=ablation[\"skipAdd\"],\n",
    "                                                           fastdw=ablation[\"fastdw\"],\n",
    "                                                           relu=ablation[\"relu\"])\n",
    "    else:\n",
    "        decoder = decoder_dict[name_split[1]](num_ch_enc=encoder.num_ch_enc)\n",
    "    \n",
    "\n",
    "    path = os.path.join(os.path.expanduser(\"~\"), \"depth\", \"monodepth2\",\"models\", name+\"_256x832\", \"models\", \"weights_19\")\n",
    "    if not os.path.isdir(path):\n",
    "        path = os.path.join(\"/work\", \"garin0115\", \"models\", f\"{name}_256x832\", \"models\", \"weights_19\")\n",
    "    encoder_pth = torch.load(path+\"/encoder.pth\")\n",
    "    decoder_pth = torch.load(path+\"/depth.pth\")\n",
    "    encoder.load_state_dict({k: v for k, v in encoder_pth.items() if k in encoder.state_dict()})\n",
    "    decoder.load_state_dict(decoder_pth)\n",
    "    model = Depth(encoder, decoder, True).eval().cuda()\n",
    "    # create example data\n",
    "    x = torch.ones((1, 3, 256, 832)).cuda()\n",
    "\n",
    "    # convert to TensorRT feeding sample data as input\n",
    "    print(\"  Convert trt32...\")\n",
    "    model_trt32 = torch2trt(model, [x])\n",
    "    print(\"  Convert trt16...\")\n",
    "    model_trt16 = torch2trt(model, [x], fp16_mode=True)\n",
    "    \n",
    "    path32 = \"/work/garin0115/models/trt32_models/\"\n",
    "    path16 = \"/work/garin0115/models/trt16_models/\"\n",
    "    pathModel = \"/work/garin0115/models/models/\"\n",
    "    torch.save(model.state_dict(), pathModel+f'{name}.pth')\n",
    "    torch.save(model_trt32.state_dict(), path32+f'{name}_trt32.pth')\n",
    "    torch.save(model_trt16.state_dict(), path16+f'{name}_trt16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
