{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import glob \n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import networks\n",
    "from utils import readlines\n",
    "from collections import OrderedDict\n",
    "\n",
    "from thop import profile, clever_format\n",
    "import copy\n",
    "import cv2\n",
    "from random import shuffle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "no_cuda = False\n",
    "data_path = \"/work/garin0115/datasets/kitti_data/\"\n",
    "split = \"eigen\"\n",
    "image_path = \"\"\n",
    "ext = \"jpg\"\n",
    "split_folder = os.path.join(os.path.expanduser(\"~\"), \"depth\", \"monodepth2\", \"splits\", split)\n",
    "CMAP = 'plasma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and not no_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_dict = {\"shufflenetv2\":networks.ShuffleNetV2(), \n",
    "# #                 \"shufflenetv2_more\":networks.ShuffleNetV2(),\n",
    "#                 \"mobilenetv2\":networks.MobileNetV2(), \n",
    "# #                 \"mobilenetv3_more\":networks.MobileNetV3(),\n",
    "#                 \"mobilenetv3\":networks.MobileNetV3(),\n",
    "#                 \"mnasnet\":networks.MnasNet(),\n",
    "#                 \"peleenet\":networks.PeleeNet(),\n",
    "#                 \"resnet18\":networks.ResnetEncoder(18, False),\n",
    "#                 \"resnet50\":networks.ResnetEncoder(50, False),\n",
    "# #                 \"peleenet_share\":networks.PeleeNet()\n",
    "#                 }\n",
    "# encoder_dict = {\n",
    "#                 \"resnet50\":networks.ResnetEncoder(50, False),\n",
    "#                 \"mnasnet\":networks.MnasNet(),\n",
    "#                 \"mnasnet_share\":networks.MnasNet(),\n",
    "#                 \"shufflenetv2\":networks.ShuffleNetV2(), \n",
    "#                 \"shufflenetv2_share\":networks.ShuffleNetV2(),\n",
    "#                 \"mobilenetv3\":networks.MobileNetV3(), \n",
    "#                 \"mobilenetv3_share\":networks.MobileNetV3(),\n",
    "#                 \"mobilenetv2\":networks.MobileNetV2(),\n",
    "#                 \"mobilenetv2_share\":networks.MobileNetV2(),\n",
    "#                 \"peleenet\":networks.PeleeNet(),\n",
    "#                 \"peleenet_MJ\":networks.PeleeNet(),\n",
    "#                 \"peleenet_share\":networks.PeleeNet(),\n",
    "#                 \"peleenet_1x1_MJ\":networks.PeleeNet(),\n",
    "#                 \"peleenet_posecnn\":networks.PeleeNet(),\n",
    "#                 \"peleenet_share_1x1_MJ\":networks.PeleeNet(),\n",
    "#                 }\n",
    "    \n",
    "encoder_dict = {\"resnet18\":networks.ResnetEncoder(18, False)}\n",
    "# decoder_dict = {\"resnet18\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc)}\n",
    "# decoder_dict = {\"resnet18\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#                 \"resnet18_nn3\":networks.NNDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=3, dw=False),\n",
    "#                 \"resnet18_nn3_dw\":networks.NNDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=3, dw=True),\n",
    "#                 \"resnet18_nn5\":networks.NNDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=5, dw=False),\n",
    "#                 \"resnet18_nn5_dw\":networks.NNDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=5, dw=True),\n",
    "#                 \"resnet18_nn35\":networks.NNDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=35, dw=False),\n",
    "#                 \"resnet18_nn53\":networks.NNDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=53, dw=False),\n",
    "#                 \"resnet18_nn3_3x3\":networks.NNDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=3, dw=False, pw=3),\n",
    "#                 \"resnet18_nn3_upconv\":networks.NNDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=3, dw=False, upconv=True)}\n",
    "\n",
    "# decoder_dict = {\"resnet18\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "# #                 \"resnet18_nn3\":networks.NNDecoder(\n",
    "# #                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=3, dw=False),\n",
    "#                 \"resnet18_my3_dw\":networks.MYDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=3, dw=True),\n",
    "# #                 \"resnet18_nn5\":networks.NNDecoder(\n",
    "# #                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=5, dw=False),\n",
    "# #                 \"resnet18_nn5_dw\":networks.NNDecoder(\n",
    "# #                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=5, dw=True),\n",
    "#                 \"resnet18_my35\":networks.MYDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=35, dw=True),\n",
    "# #                 \"resnet18_nn53\":networks.NNDecoder(\n",
    "# #                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=53, dw=False),\n",
    "#                 \"resnet18_my3_3x3\":networks.MYDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=3, dw=False, pw=3),\n",
    "#                 \"resnet18_my3_3x3_dw\":networks.MYDecoder(\n",
    "#                     encoder_dict[\"resnet18\"].num_ch_enc, kernel_size=3, dw=True, pw=3)}\n",
    "\n",
    "# decoder_dict = {\"resnet18\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#             \"resnet18_my3\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "                \n",
    "#             \"resnet18_my35_moreFeature\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, moreFeature=True, kernel_size=35),\n",
    "#             \"resnet18_my3_more1\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, moreConv=1),\n",
    "#             \"resnet18_my3_more2\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, moreConv=2),\n",
    "                \n",
    "#             \"resnet18_my3_doubleConv1\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, doubleConv=1),\n",
    "#             \"resnet18_my3_doubleConv3\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, doubleConv=3),\n",
    "#             \"resnet18_my3_doubleConv3_3x3\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, doubleConv=3, pw=False),\n",
    "                \n",
    "#             \"resnet18_my3_firstConv\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True),\n",
    "#             \"resnet18_my3_skipSky\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#             \"resnet18_my3_firstConv_skipSky\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True),\n",
    "                \n",
    "#             \"resnet18_my3_firstConv_skipSky_conv11\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True, conv11=True),\n",
    "#             \"resnet18_my3_smooth\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "#             \"resnet18_my3_concatDepth\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, concatDepth=True),\n",
    "#             }\n",
    "\n",
    "decoder_dict = {\"resnet18\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "            \"resnet18_my3\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "                \n",
    "            \"resnet18_my3_firstConv\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True),\n",
    "            \"resnet18_my3_skipSky\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "            \"resnet18_my3_firstConv_skipSky\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True),\n",
    "                \n",
    "            \"resnet18_my3_firstConv_skipSky_conv11\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True, conv11=True),\n",
    "            \"resnet18_my3_smooth\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc),\n",
    "            \"resnet18_my3_concatDepth\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, concatDepth=True),\n",
    "            \n",
    "            \"resnet18_skipFirstConv\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, skipFirstConv=True),\n",
    "            \"resnet18_skipFirstConv_skipSky\":networks.DepthDecoder(encoder_dict[\"resnet18\"].num_ch_enc, skipFirstConv=True),\n",
    "            \"resnet18_my3_firstConv_skipSky\":networks.MYDecoder(encoder_dict[\"resnet18\"].num_ch_enc, firstConv=True),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "lines = readlines(os.path.join(split_folder, \"test_files.txt\"))\n",
    "side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "\n",
    "inWork = [\"resnet18\", \"resnet18_my3\", \"resnet18_my35_moreFeature\", \"resnet18_my3_more1\", \"resnet18_my3_more2\", \"resnet18_my3_concatDepth\", \"resnet18_my3_doubleConv1\", \"resnet18_my3_doubleConv3\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in decoder_dict:\n",
    "        print(\"Deal with \", name)\n",
    "        if name in inWork:\n",
    "            weight_path = os.path.join(\"/work\",\n",
    "                                      \"garin0115\",\n",
    "                                      \"models\",\n",
    "                                      name+\"_256x832\",\n",
    "                                      \"models\",\n",
    "                                      \"weights_19\")\n",
    "        else:\n",
    "            weight_path = os.path.join(os.path.expanduser(\"~\"), \n",
    "                                       \"depth\",\n",
    "                                       \"monodepth2\",\n",
    "                                       \"models\", \n",
    "                                       name+\"_256x832\", \n",
    "                                       \"models\", \n",
    "                                       \"weights_19\")\n",
    "            \n",
    "        encoder_path = os.path.join(weight_path , \"encoder.pth\")\n",
    "        depth_decoder_path = os.path.join(weight_path, \"depth.pth\")\n",
    "\n",
    "\n",
    "        encoder = encoder_dict[\"resnet18\"]\n",
    "        depth_decoder = decoder_dict[name]\n",
    "\n",
    "        loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
    "        filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "        loaded_dict = torch.load(depth_decoder_path, map_location='cpu')\n",
    "\n",
    "        models[name+\"_encoder\"] = copy.deepcopy(encoder)\n",
    "        models[name+\"_decoder\"] = copy.deepcopy(depth_decoder)\n",
    "        models[name+\"_height\"] = loaded_dict_enc[\"height\"]\n",
    "        models[name+\"_width\"] = loaded_dict_enc['width']\n",
    "\n",
    "        models[name+\"_encoder\"].load_state_dict(filtered_dict_enc)\n",
    "        models[name+\"_decoder\"].load_state_dict(loaded_dict)\n",
    "\n",
    "        models[name+\"_encoder\"].to(device)\n",
    "        models[name+\"_decoder\"].to(device)\n",
    "        models[name+\"_encoder\"].eval()\n",
    "        models[name+\"_decoder\"].eval()\n",
    "\n",
    "#             input_image_resized = input_image.resize((models[name+\"_width\"], models[name+\"_height\"]), pil.LANCZOS)\n",
    "#             input_image_torch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "#             input_image_torch = input_image_torch.to(device)\n",
    "\n",
    "#             features = models[name+\"_encoder\"](input_image_torch)\n",
    "#             outputs = models[name+\"_decoder\"](features)\n",
    "\n",
    "#             disp = outputs[(\"disp\", 0)]\n",
    "#             disp_resized = torch.nn.functional.interpolate(\n",
    "#                     disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "#             disp_resized_np = disp_resized.squeeze().cpu().detach().numpy()\n",
    "\n",
    "#             show[\"Input\"] = input_image\n",
    "#             show[\"{}_disp\".format(name)] = disp_resized_np\n",
    "\n",
    "\n",
    "#         plt.figure(figsize=(20, 18)) \n",
    "\n",
    "#         for idx, key in enumerate(show):\n",
    "#             if idx == 0:\n",
    "#                 plt.subplot(621)\n",
    "#                 plt.imshow(show[key])\n",
    "#                 plt.title(\"Input\", fontsize=22)\n",
    "#                 plt.axis('off')\n",
    "#             else:\n",
    "#                 plt.subplot(6, 2, idx+1)\n",
    "#                 vmax = np.percentile(show[key], 95)\n",
    "#                 plt.imshow(show[key], cmap=CMAP, vmax=vmax)\n",
    "#                 plt.title(key, fontsize=22)\n",
    "#                 plt.axis('off')\n",
    "#         plt.tight_layout(pad=0.5, w_pad=0.1, h_pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For test_files.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = readlines(os.path.join(split_folder, \"test_files.txt\"))\n",
    "side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "with torch.no_grad():\n",
    "    for i in np.random.choice(len(lines), 10, replace=False):\n",
    "        folder, frame_id, side = lines[i].split()\n",
    "        frame_id = int(frame_id)        \n",
    "        \n",
    "\n",
    "        image_path = os.path.join(data_path, folder, \"image_0{}\".format(side_map[side]), \"data\", \"{:010d}.jpg\".format(frame_id))\n",
    "#         image_path = \"assets/test_image.jpg\"\n",
    "        input_image = pil.open(image_path).convert('RGB')\n",
    "        original_width, original_height = input_image.size\n",
    "        \n",
    "        show = OrderedDict()\n",
    "        show[\"Input\"] = input_image\n",
    "\n",
    "        \n",
    "        for name in decoder_dict:\n",
    "            input_image_resized = input_image.resize((models[name+\"_width\"], models[name+\"_height\"]), pil.LANCZOS)\n",
    "            input_image_torch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "            input_image_torch = input_image_torch.to(device)\n",
    "            \n",
    "            features = models[name+\"_encoder\"](input_image_torch)\n",
    "            outputs = models[name+\"_decoder\"](features)\n",
    "\n",
    "            disp = outputs[(\"disp\", 0)]\n",
    "            disp_resized = torch.nn.functional.interpolate(\n",
    "                    disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "            disp_resized_np = disp_resized.squeeze().cpu().detach().numpy()\n",
    "            show[\"{}_disp\".format(name)] = disp_resized_np\n",
    "            \n",
    "            \n",
    "        plt.figure(figsize=(25, 15)) \n",
    "        for idx, key in enumerate(show):\n",
    "            if idx == 0:\n",
    "                plt.subplot(431)\n",
    "                plt.imshow(show[key])\n",
    "                plt.title(\"Input\", fontsize=22)\n",
    "                plt.axis('off')\n",
    "            else:\n",
    "                plt.subplot(4, 3, idx+1)\n",
    "                vmax = np.percentile(show[key], 95)\n",
    "                plt.imshow(show[key], cmap=CMAP, vmax=vmax)\n",
    "                plt.title(key, fontsize=22)\n",
    "                plt.axis('off')\n",
    "        plt.tight_layout(pad=0.5, w_pad=0.1, h_pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇要建立 video 的 data [TODO]\n",
    "# file_name = '2011_10_03/2011_10_03_drive_0047_sync' #837\n",
    "# file_name = '2011_09_30/2011_09_30_drive_0016_sync' #279\n",
    "# file_name = '2011_09_29/2011_09_29_drive_0026_sync' #158\n",
    "# file_name = '2011_09_28/2011_09_28_drive_0037_sync' #89\n",
    "file_name = '2011_09_26/2011_09_26_drive_0036_sync' #803\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0023_sync' #474\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0020_sync' #86\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0013_sync' #144\n",
    "# file_name = '2011_09_26/2011_09_26_drive_0002_sync' #77\n",
    "\n",
    "# 選擇影片輸出資料夾 [TODO]\n",
    "video_output_folder = os.path.join(os.path.expanduser(\"~\"), \n",
    "                                                      \"depth\",\n",
    "                                                      \"monodepth2\",\n",
    "                                                      \"video_result\")\n",
    "\n",
    "# 取得資料夾中所有影像檔案路徑\n",
    "kitti_depth_folder = '/work/garin0115/datasets/kitti_data/'+file_name+'/image_02'\n",
    "filenames = glob.glob(kitti_depth_folder+'/*/*.jpg')\n",
    "\n",
    "# 將檔案路徑排序\n",
    "filenames.sort()\n",
    "num_images = len(filenames)\n",
    "print(\"Total images: {}\".format(num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = OrderedDict()\n",
    "show[\"Input\"] = []\n",
    "for img in filenames:\n",
    "    input_image = pil.open(img).convert('RGB')\n",
    "    show[\"Input\"].append(input_image)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in decoder_dict:\n",
    "        print(\"Deal with {}\".format(name))\n",
    "        show[\"{}_disp\".format(name)] = []\n",
    "        show[\"{}_FPS\".format(name)] = []\n",
    "        for input_image in show[\"Input\"]:\n",
    "            \n",
    "            original_width, original_height = input_image.size\n",
    "\n",
    "            input_image_resized = input_image.resize((models[name+\"_width\"], models[name+\"_height\"]), pil.LANCZOS)\n",
    "            input_image_torch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "            input_image_torch = input_image_torch.to(device)\n",
    "            \n",
    "            \n",
    "            start_time = time.time()\n",
    "            features = models[name+\"_encoder\"](input_image_torch)\n",
    "            outputs = models[name+\"_decoder\"](features)\n",
    "            end_time = time.time()\n",
    "\n",
    "            disp = outputs[(\"disp\", 0)]\n",
    "            disp_resized = torch.nn.functional.interpolate(\n",
    "                    disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "            disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "            \n",
    "            show[\"{}_disp\".format(name)].append(disp_resized_np)\n",
    "            show[\"{}_FPS\".format(name)].append(\"{:.2f}\".format(1/(end_time-start_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(video_output_folder+'/disp_{}.avi'.format(file_name.split('/')[-1]), fourcc, 15.0, (original_width*3, original_height*5))\n",
    "num_model = len(decoder_dict)\n",
    "\n",
    "for idx in range(num_images):\n",
    "    res = []\n",
    "    input = np.array(show[\"Input\"][idx])\n",
    "    cv2.putText(input, \"Input\", (10, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    res.append(input[:, :, ::-1])\n",
    "    \n",
    "    for name in decoder_dict:\n",
    "        disp = show[\"{}_disp\".format(name)][idx]\n",
    "        vmax = np.percentile(disp, 95)\n",
    "        normalizer = mpl.colors.Normalize(vmin=disp.min(), vmax=vmax)\n",
    "        mapper = cm.ScalarMappable(norm=normalizer, cmap=CMAP)\n",
    "        colormapped_im = (mapper.to_rgba(disp)[:, :, :3] * 255).astype(np.uint8)\n",
    "        cv2.putText(colormapped_im, name, (10, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(colormapped_im, \"FPS \"+show[\"{}_FPS\".format(name)][idx], (10, 100), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        im = pil.fromarray(colormapped_im[:, :, ::-1])\n",
    "        res.append(im)\n",
    "\n",
    "    result = np.vstack([np.hstack(res[:3]), np.hstack(res[3:6]), np.hstack(res[6:9]), \n",
    "                        np.hstack(res[9:12]), np.hstack(res[12:])])\n",
    "\n",
    "        \n",
    "    out.write(result)\n",
    "out.release()\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Pose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_img = None\n",
    "orig = np.array([0, 0, 0, 1])\n",
    "pos_gt = [orig.copy()]\n",
    "pos_pred = [orig.copy()]\n",
    "\n",
    "# seg = 'road04_seg'\n",
    "# record = 'Record021'\n",
    "# record_dir = '/home/ubuntu/hdd/data/Apolloscape/Segmentation/%s/ColorImage/%s/Camera 5'%(seg, record)\n",
    "# vid_name = '%s_%s.avi'%(seg, record)\n",
    "\n",
    "# record_dir = '/home/ubuntu/hdd/data/kitti/RawData/2011_09_30/2011_09_30_drive_0033_sync/image_02/data/'\n",
    "# vid_name = '2011_09_30_drive_0033_2.avi'\n",
    "\n",
    "record_dir = '/home/ubuntu/hdd/data/Cityscapes/leftImg8bit_demoVideo/leftImg8bit/demoVideo/stuttgart_02/'\n",
    "vid_name = 'stuttgart_02.avi'\n",
    "\n",
    "# apollo pose\n",
    "# pose_gt = pd.read_csv('/home/ubuntu/hdd/data/Apolloscape/Segmentation/%s/Pose/%s/Camera 5/pose.txt'%(seg, record), sep=' ', header=None)\n",
    "# pose_gt.sort_values(16, inplace=True)\n",
    "# pose_gt.drop(pose_gt.columns[16], axis=1, inplace=True)\n",
    "# pose_gt = np.array(pose_gt, dtype=np.float32).reshape(-1, 4, 4)\n",
    "\n",
    "# kitti pose\n",
    "# pose_gt = pd.read_csv('/home/ubuntu/hdd/data/kitti/odometry/poses/09.txt', sep=' ', header=None)\n",
    "# pose_gt[12] = 0\n",
    "# pose_gt[13] = 0\n",
    "# pose_gt[14] = 0\n",
    "# pose_gt[15] = 1\n",
    "# pose_gt = np.array(pose_gt, dtype=np.float32).reshape(-1, 4, 4)\n",
    "\n",
    "out = cv2.VideoWriter(vid_name, cv2.VideoWriter_fourcc(*'XVID'), 10.0, (feed_width*2+320,feed_height))\n",
    "T_pred = []\n",
    "T_gt = np.eye(4)\n",
    "T_pr = np.eye(4)\n",
    "for i, filename in enumerate(sorted(os.listdir(record_dir))):\n",
    "    image_path = os.path.join(record_dir, filename)\n",
    "    input_image = pil.open(image_path).convert('RGB')\n",
    "    input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = encoder(input_image_pytorch)\n",
    "        outputs = depth_decoder(features)\n",
    "        disp = outputs[(\"disp\", 0)][0]\n",
    "        if prev_img is not None:\n",
    "            features_pose = [pose_encoder(torch.cat([prev_img, input_image_pytorch], 1))]\n",
    "            axisangle, translation = pose_decoder(features_pose)\n",
    "            T_ = transformation_from_parameters(axisangle[:, 0], translation[:, 0], True).cpu().numpy()[0]\n",
    "            T_pr = T_pr.dot(T_)\n",
    "            T_pred += [T_]\n",
    "#             T_gt = T_gt.dot(np.linalg.inv(pose_gt[i-1]).dot(pose_gt[i]))\n",
    "#             pos_gt += [T_gt[:,-1]]\n",
    "            pos_pred += [T_pr[:,-1]]\n",
    "    \n",
    "    disp_np = disp.squeeze().cpu().numpy()\n",
    "    vmax = np.percentile(disp_np, 95)\n",
    "    output = np.zeros((feed_height, feed_width*2+320, 3), dtype=np.uint8)\n",
    "    buf = io.BytesIO()\n",
    "    plt.imsave(buf, disp_np, cmap='plasma', vmax=vmax)\n",
    "    buf.seek(0)\n",
    "    disp_ = np.array(pil.open(buf))[:,:,:3]\n",
    "    buf.close()\n",
    "    \n",
    "    pos_gt_ = np.array(pos_gt)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.title(\"Visual odometry\", fontsize=15)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal', adjustable='datalim')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "#     plt.plot(pos_gt_[:, 0], pos_gt_[:, 2], 'o-', label='gt')\n",
    "    scale = 29.5\n",
    "    pos_pred_ = np.array(pos_pred)*scale\n",
    "    plt.plot(pos_pred_[:, 0], pos_pred_[:, 2], 'o-', label='pred')\n",
    "    plt.scatter(pos_pred_[-1, 0], pos_pred_[-1, 2], color='r', s=100, zorder=10)\n",
    "    plt.text(pos_pred_[-1, 0], pos_pred_[-1, 2], 'NOW', fontsize=15, zorder=20)\n",
    "    plt.legend(loc=4, fontsize=15)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, dpi=64)\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    vo = np.array(pil.open(buf))[:,:,:3]\n",
    "    buf.close()\n",
    "    \n",
    "    output[:,:feed_width] = np.array(input_image_resized)\n",
    "    output[:,feed_width:-320] = disp_\n",
    "    output[:,-320:] = vo\n",
    "    out.write(output[:,:,::-1])\n",
    "    \n",
    "    prev_img = input_image_pytorch\n",
    "    print('\\r', i+1, len(os.listdir(record_dir)), end=' '*10)\n",
    "    \n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_gt = [orig.copy()]\n",
    "pos_pred = [orig.copy()]\n",
    "\n",
    "T_gt = np.eye(4)\n",
    "T_pr = np.eye(4)\n",
    "for i in range(1, len(pose_gt)):\n",
    "    T_pr = T_pr.dot(T_pred[i-1])\n",
    "    T_gt = T_gt.dot(np.linalg.inv(pose_gt[i-1]).dot(pose_gt[i]))\n",
    "    pos_gt += [T_gt[:,-1]]\n",
    "    pos_pred += [T_pr[:,-1]]\n",
    "    \n",
    "pos_gt = np.array(pos_gt)\n",
    "plt.figure(figsize=(5, 8))\n",
    "plt.title(\"Visual odometry\", fontsize=15)\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='datalim')\n",
    "plt.plot(pos_gt[:, 0], pos_gt[:, 2], '-o', label='gt')\n",
    "scale = 34.169\n",
    "pos_pred = np.array(pos_pred)*scale\n",
    "plt.plot(pos_pred[:, 0], pos_pred[:, 2], '-o', label='pred')\n",
    "plt.scatter(pos_pred[-1, 0], pos_pred[-1, 2], color='r', s=100, zorder=10)\n",
    "plt.text(pos_pred[-1, 0], pos_pred[-1, 2], 'NOW', fontsize=15, zorder=20)\n",
    "plt.legend(loc=4, fontsize=15)\n",
    "\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, dpi=64)\n",
    "# plt.close()\n",
    "buf.seek(0)\n",
    "x = np.array(pil.open(buf))[:,:,:3]\n",
    "buf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
